{"./":{"url":"./","title":"序言","keywords":"","body":"Introduction 随着云计算、容器、微服务、DevOps 等理念、技术的成熟，以及企业对架构的稳定、高效、敏捷、智能、安全、成本控制等方面的更高要求，云原生成为了企业主流架构。企业架构从上云化演变成了在云中。 在这样的背景下，技术人员需要掌握更多的云原生知识，迎接其带来的挑战及红利。 为进一步推进云原生技术的普及，帮助广大技术爱好者快速掌握云原生相关技能，我把我的知识及经验整理成《云原生运维指南》这本书，并通过 https://cloudnative.aiops.red 这个网站，分享给大家。 本书内容包括... 当然，一个人的能力和知识是有局限的，书中难免有不足或不恰当的地方。希望我只是抛砖引玉，请诸位看官多多指正，共同提高文章的质量。 By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-26 17:39:18 "},"cloudnative/jieshao.html":{"url":"cloudnative/jieshao.html","title":"课程介绍","keywords":"","body":"课程介绍 By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-26 17:39:18 "},"helm/jieshao.html":{"url":"helm/jieshao.html","title":"Kubernetes and Helm 介绍","keywords":"","body":"Kubernetes 与 Helm 介绍 应用程序对可扩展性、高可用性以及能够承受各种程度负载的要求，是压在技术人员头上的三座大山。在容器化流行之前，我们接触更多的应用程序是 monolithic(单体)架构。Docker 的诞生，推动了容器化的发展，monolithic 架构也逐渐转向了微服务架构，并以容器化方式部署，直到演变为使用 Kubernetes 之类的工具编排和扩展容器化微服务。 虽然 Kubernetes 是高效的容器编排工具，但它学习曲线陡峭，对于使用 Kubernetes 的团队来说，这是一个不小的挑战。Helm 是一个简化在 Kubernetes 上部署应用程序的工具，用户通过 Helm 可以简单、方便地在 Kubernetes 集群中部署和管理应用程序的生命周期。Helm 抽象了配置应用程序的复杂性，可以有效提高团队在 Kubernetes 平台上的工作效率。 本书会详细介绍 Helm 的各种功能，并探索 Helm 如何使 Kubernetes 上的应用程序部署更加简单。一开始，我们仅作为使用者，使用社区编写好的 Helm Charts 实践程序包管理器。随着内容深入，我们会作为 Helm Charts 开发者角色，并学习如何高效的打出易用的 Kubernetes 应用程序包。最后，介绍 Helm 插件、应用程序管理及安全性这些高级功能。 首先，我们先从了解微服务、容器、Kubernetes，以及使用它们部署应用程序带来的挑战开始。随后，我们将讨论 Helm 的主要功能和优点。 从 monolithic 到微服务 你应该听说过摩尔定律，它由英特尔创始人之一的戈登摩尔提出，内如是：“当价格不变时，集成电路上可容纳的元器件数目，约每隔18-24个月便会增加一倍，性能也将提升一倍”。虽然这种趋势近几年稍显平息，但在处理器存在的头 30 年中，这种趋势的持久性对技术的进步至关重要。 编程人员充分利用了这一特性，在开发软件时集成了越来越多的功能和组件。这就导致，一个应用程序可以由多个较小的组件组成，每个组件可以单独开发为独立的服务。这就是 monolithic 架构，这种架构的优势之一，就是简化了部署过程。但随着行业趋势发生变化，企业更加关注快速交付的能力。在这种情况下，monolithic 架构的应用程序面临了许多挑战：每当对软件进行更改时，都需要验证整个应用程序及其所有基础组件，以确保修改后没有异常。这个过程可能需要多个团队协作完成，大大降低了软件交付的速度及机动性。 企业希望能够更快完成软件交付，尤其是跨组织或部门的快速交付能力，为了实现这一点，很多企业引入了 DevOps 文化。DevOps 提倡以最小的代价帮助企业应用开发进入高效的协作模式和快速交付的迭代过程。为了在这种新模式中保持可持续性，软件架构也从大型 monolithic 应用程序逐渐拆分为可以更快速交付的多个较小的应用程序，这些独立的多个小应用被命名为微服务。微服务应用程序固有的特性带来了一些理想的功能，包括并行开发、独立扩展（增加实例数量）服务等。 从 monolithic 服务到微服务的架构变化，也改变了软件交付时的打包和部署方式。monolithic 服务部署时，整台机器专用于一个或两个应用程序。在微服务时代，单个应用所需的资源整体减少，因此将整台机器专用于一个或两个微服务已不再可行。 幸运的是，容器技术完美贴合了创建微服务运行时所需的功能。容器技术虽然在计算机方面有着悠久历史，但真正“飞入寻常百姓家”，还要从 Docker 的诞生开始，它简单便捷的打包方式、易于分发容器镜像的能力，将容器化推向了大众。 容器和微服务已成了天作之合。结合容器技术，应用程序有了便捷的打包和分发机制、共享计算资源的同时又彼此隔离。然而，随着越来越多的容器化微服务被部署，对容器的管理成为了一个难题。如何确保每个运行中的容器的运行状态？如果一个容器发生了故障怎么办？如果宿主机的计算能力不足，会发生什么？Kubernetes 很好地解决了这些容器编排的需求。 什么是 Kubernetes Kubernetes 常被缩写为 k8s(8 代表 k s 之间的8个字符)，是一个开放源代码的容器编排平台，源自 Google 的内部编排工具 Borg 系统，于 2015 年开源。在 2015 年 7 月 21 日发布 v1.0 之后，Google 和 Linux Foundation 合作成立了 Cloud Native Computing Foundation(CNCF)，是 Kubernete 项目的当前维护者。 Kubernetes 是希腊语，意为“舵手”或“飞行员”。舵手负责操纵船舶，并与船员紧密合作，确保航行安全稳定以及船员的整体安全。Kubernetes 在容器和微服务方面也负有类似的责任。Kubernetes 负责容器的编排和调度，将容器部署到可以处理其工作负载的适当 Worker 节点。Kubernetes 还可以通过提供高可用性和健康检查来确保这些微服务的安全性。 让我们一起看看 Kubernetes 是如何简化管理容器化微服务，以及帮助技术人员卸掉头顶三座大山的： 容器编排 容器编排是 Kubernetes 最显著的特性，通过计算资源池，把容器调度到满足其运行的主机上。我用一个简单的示例来说明这一点。下图中的应用程序请求 2Gi 内存和 1 核 CPU，这意味着容器被调度到的 Worker 节点至少拥有满足该应用程序请求的空闲资源。Kubernetes 负责跟踪哪些节点拥有所需的资源，并将容器调度到该节点。没有足够资源的节点，是不会被调度的；如果集群中的所有节点都没有足够的可用资源，该容器不会被部署，直到某个节点具备足够的资源： 图1.1-Kubernetes 编排和调度 容器编排使我们无需时时关注主机可用资源，Kubernetes 提供了对这些指标的监控功能。因此开发人员只需声明容器期望使用的资源，而无需担心资源是否可用，Kubernetes 会处理好这些工作。 高可用 Kubernetes 的另一个显著特性是为应用程序提供了冗余和高可用。高可用是为了防止应用程序停止服务，由负载均衡器在应用程序的多个实例间分配流量。高可用的前提是，如果应用程序的一个实例宕机了，其他实例仍然可以处理请求，这就避免了停服的可能，并且不论其中的一个或多个实例宕机，对客户端来说完全无感知。Kubernetes 提供了 Service 这种网络机制，实现在多个实例间的负载均衡。在 1.3 章节中，我会详细介绍 Service。 可扩展 考虑到容器和微服务的轻量级特性，运维人员可以使用 Kubernetes 快速伸缩应用的负载能力，无论是水平还是垂直。 水平扩展是部署更多的容器实例副本数，如果你评估到运行在 Kubernetes 上的某个应用程序最近负载会增大，可以部署更多的副本。这个过程很简单，无需关注应用程序会被部署到哪些节点上，Kubernetes 是一个容器编排工具，它会根据资源需求自动完成这一过程，并将新部署的实例加入到负载均衡池中，保持其高可用。 垂直扩展是为应用程序分配更大的内存和 CPU 资源，运维人员可以在应用程序运行时修改其资源需求。Kubernetes 会重新部署正在运行的实例，并把它们调度到满足新资源需求的节点上。根据不同的配置，Kubernetes 可以在不停服的情况下重新部署每个实例。 社区活跃度 Kubernetes 拥有活跃的开源社区，经常收到补丁和新特性。社区还对文档做出了很多贡献，包括官方文档以及专业或业余爱好者的博客网站、公众号。除了文档，社区还通过积极参与计划和参加世界各地的聚会和会议，进行技术分享和创新。 Kubernetes 大型社区还构建了大量的工具来增强 Kubernetes 的功能，Helm 就是其中之一。正如我们将在本章以及整本书中看到的：Helm——Kubernetes 社区成员构建的工具，简化了 Kubernetes 中应用程序部署和生命周期管理，极大改善了开发、运维人员的体验。 了解了 Kubernetes 为技术人员带来的帮助之后，我们来看看如何在 Kubernetes 中部署应用程序。 Kubernetes 部署应用程序 不论应用程序部署在 Kubernetes 上，还是部署在非 Kubernetes 环境，根本上是相似的，都需要以下要素： 网络 持久化存储和文件挂载 可用性和冗余 应用配置 安全 可以与 Kubernetes API(Application Programming Interface 应用程序编程接口)交互，在 Kubernetes 集群中配置这些要素。Kubernetes API 是一组端点(Endpoint)，通过这些端点可以查看、修改、删除 Kubernetes 资源，很多 Kubernetes 资源用来配置应用程序的部署细节。 接下来介绍一些基本的 API 端点，以便在 Kubernetes 上部署和配置应用程序。 Deployment Deployment 定义了在 Kubernetes 集群中部署应用程序所需的基本细节，如使用哪个镜像。容器镜像可以使用 docker 在本地构建，也可以使用 kaniko 在 Kubernetes 上构建。 除了指定容器镜像外，Deployment 还定义了部署应用程序的副本数。创建 Deployment 时，会生成名为 ReplicaSet 的中间资源。ReplicaSet 部署的应用程序副本数由replicas字段定义。应用程序部署在容器中，容器被封装在 Pod 里。Pod 是 Kubernetes 中的最小单元，它包含至少一个容器。 Deployment 还可以配置应用程序的资源限制、运行状况检查和卷挂载。Deployment 创建后的架构如下图： 1.2dtcreate 图1.2- Deployment 创建一组 Pods Service Deployment 部署了一组 Pods(一个或多个)，如果我们直接访问这些 Pods，会有很多问题，如不能进行负载均衡、更新 Pods 后地址会改变。这时就需要引入另一个资源 Service 了。Service 的 IP 是固定的，功能类似负载均衡器，把流量分发到后端的 Pods 上，提供负载均衡和高可用的作用。 下图是一个 Service 的示例结构，Service 位于客户端和 Pod 之间，提供负载均衡和高可用的作用： 1.3Service 图1.3-Service 负载均衡器 PersistentVolumeClaim 微服务风格的应用程序生命周期比较短暂，不具备持久化存储。然而在实际环境中，许多应用程序产生的数据，是要在容器生命周期之外，要持久化存储的。Kubernetes 通过一个子系统封装了如何提供存储以及如何使用存储的底层细节，从而解决了这一问题，用户可以通过 PersistentVolumeClaim 端点为应用程序分配持久化存储。Kubernetes 运维人员负责静态分配存储(PersistentVolume)、或使用 StorageClass 动态分配存储(它分配PersistentVolume以响应 PersistentVolumeClaim 端点)。PersistentVolume 捕获所有必需的存储细节，包括类型(例如NFS、Ceph)、存储的大小。从用户的角度无论在集群中使用哪种 PersistentVolume，都不需管理底层存储服务。利用 Kubernetes 的持久化存储功能，可以增加 Kubernetes 平台上可部署的应用程序数量。 下图是 StorageClass 动态分配存储示例： 1.4-storageclass 图1.4-PersistentVolume created by PersistentVolumeClaim 除了以上这些，Kubernetes 还有很多资源类型，但这些已满足我们本书的使用了(更详细的 Kubernetes 知识，可以参考我的Kubernetes 从入门到实战系列文章)。 下一节，我们来学习如何创建资源。 资源管理 在 Kubernetes 集群中部署应用程序时，我们首先要和 Kubernetes API 进行交互。在命令行界面，kubectl 是主流的 Kubernetes API 交互工具。 我们来看看 kubectl 是如何管理 Kubernetes 资源的。 命令式和声明式 kubectl 包含很多子命令，以命令式的方式创建和修改资源。下面是几个常用命令： create describe edit delete kubectl 命令格式如下： kubectl verb 是 kubectl 的子命令，noun 指特定的 Kubernetes 资源。以下是创建 Deployment 资源的命令示例： kubectl create deployment my-deployment --image=busybox 这条命令会指示 Kubernetes API 创建一个名为 “my-deployment” 的 Deployment 资源，并使用来自 Docker Hub 的 busybox 镜像。 可以通过 kubectl 的 describe 子命令查看 Deployment 的详细信息： kubectl describe deployment my-deployment 这条命令会格式化的输出 Deployment 的相关信息，我们可以通过这条命令查看 Deployment 的状态。 如果需要对 Deployment 进行修改，可以通过 edit 子命令： kubectl edit deployment my-deployment 这条命令将打开一个文本编辑器，你可以像使用 vim 一样修改 Deployment。 需要删除某个资源时，可以使用 delete 子命令： kubectl delete deployment my-deployment 执行此命令，会删除名为 “my-deployment” 的 Deployment 资源。 Kubernetes 资源一旦创建，将以 JSON 文件的格式存在于集群中，可以将其导出为 YAML 文件，以提高可读性。YAML 格式的资源示例如下： apiVersion: apps/v1 kind: Deployment metadata: name: busybox spec: replicas: 1 selector: matchLabels: app: busybox template: metadata: labels: app: busybox spec: containers: - name: main image: busybox args: - sleep - infinity 这是一个基础的 yaml 示例，使用来自 Docker Hub 的 busybox 镜像，并无限期地运行 sleep 命令以保持 Pod 运行。 kubectl 是以命令式的方式创建资源的，看上去更简单一些，但在实际使用中，我们更多的是使用 yaml 文件。kubectl 子命令不能配置所有的资源选项，但通过 yaml 文件可以，并且也更灵活。更重要的一点是，yaml 文件是以声明式的方式创建资源的。 在声明式创建资源时，用户首先以 yaml 格式写出要创建的资源，再使用 kubectl 工具将资源应用于 Kubernetes API。在命令式配置中，开发人员使用 kubectl 诸多子命令来管理资源，而声明式配置主要依赖于一个 apply 子命令。 声明式配置通常使用以下命令实现： kubectl apply -f my-deployment.yaml 使用此命令，Kubernetes 会根据资源是否存在来推断要对资源执行的操作（创建或修改）。 配置声明式应用程序的步骤： 创建名为 my-deployment.yaml 的文件，并将以下 yaml 格式的内容写入该文件： apiVersion: apps/v1 kind: Deployment metadata: name: busybox spec: replicas: 1 selector: matchLabels: app: busybox template: metadata: labels: app: busybox spec: containers: - name: main image: busybox args: - sleep - infinity 使用以下命令创建 Deployment： kubectl apply -f my-deployment.yaml 执行此命令后，Kubernetes 将按照 yaml 内容创建 Deployment。 如果要修改 Deployment，例如将副本数修改为 2，可以通过修改 my-deployment.yaml 文件实现： apiVersion: apps/v1 kind: Deployment metadata: name: busybox spec: replicas: 2 # 修改此处的数值 selector: matchLabels: app: busybox template: metadata: labels: app: busybox spec: containers: - name: main image: busybox args: - sleep - infinity 然后执行 kubectl apply 命令应用修改： kubectl apply -f my-deployment.yaml 执行此命令，Kubernetes 会基于原有部署声明 Deployment，副本数将由 1 扩展为 2。 在删除应用程序时，同样使用 delete 子命令： kubectl delete -f my-deployment.yaml 可以通过 delete -f 标志指定文件，该文件提供了要删除的资源名称，并且该 YAML 文件可以反复使用。 了解了Kubernetes资源是如何创建的，现在我们讨论一下资源配置中涉及的一些挑战。 资源配置中的挑战 上一节介绍了 Kubernetes 资源的两种配置方式(命令式和声明式)，接着我们聊聊使用这两种方式会遇到的挑战。 Kubernetes 资源类型多 Kubernetes有很多很多不同的资源类型，以下是工作中常用的资源类型： Deployment StatefulSet Service Ingress ConfigMap Secret StorageClass PersistentVolumeClaim ServiceAccount Role RoleBinding Namespace 这么多资源类型构成了 Kubernetes 的强大功能，同时也提高了 Kubernetes 的使用复杂度。在 Kubernetes 上部署应用程序时，首先需要确定使用哪些资源，只有深入了解了这些资源，才能进行正确的配置。这需要丰富的 Kubernetes 知识，对使用者来说还是有一定难度的，但这仅仅是更多挑战的开始。 保持运行时和本地状态一致 为方便团队间协同工作，可以将配置 Kubernetes 资源的 YAML 文件存储在类似 GitLab 的代码仓库中，然后根据代码仓库中的 YAML 文件部署应用程序。这一过程看似简单，但需要调整资源时，会发生什么呢？正确的做法是修改本地文件，并 apply 修改后的文件，以确保本地状态与运行时状态一致。但也会有例外发生，为了方便，有人可能通过 kubectl patch 或者 kubectl edit 命令跳过本地文件，直接修改运行时。这就会使本地状态和运行时状态不一致，导致后期维护与扩展变得更困难。 应用程序生命周期难以管理 姑且将应用程序的安装、升级、回滚定义为应用程序的生命周期吧。在 Kubernetes 中安装应用程序，会创建用于部署和配置应用程序所需的资源。第一次安装应用程序，将创建我们这里所说的该应用程序的第一个版本 version1。 因此，升级应用程序，可以被视为是对这些资源的一次或多次编辑。每一次编辑都可以看作是一次升级。运维人员可以修改 Service 资源，版本号就会变为 version2。之后，运维人员又修改了 Deployment、ConfigMap 和 Service，版本号将变为 version3。 随着新版本持续迭代，跟踪版本号愈发困难。Kubernetes 没有提供固定保存版本变化历史的方法，这不利于版本回滚。假设运维人员之前对某个资源做了不正确的更改，团队如何知道回滚到哪个版本？“n - 1” 的情况比较容易确定回滚到的版本，但最近的正确版本如果是最近五个之前的版本呢？团队就只能把生产当测试，在生产环境匆忙处理故障，因为无法识别哪个是最近的正确版本，从而通过版本回滚来快速解决生产环境问题。 资源文件是静态的 把它也列为挑战之一，是因为它影响 YAML 声明式配置的应用。在使用声明式时，Kubernetes 资源文件本身不支持参数化。有些资源文件可能很长，包含许多可定制字段，而且完整地编写、配置 YAML 文件非常麻烦。 可以把静态文件做成模板。模板就是框架，在不同但相似的上下文中保持一致的文本或代码，我们只需填入自己需要的内容。运维人员管理不同的应用程序，需要维护不同的 Deployment、Service 等资源，这个工作量会很大，并随着应用程序的数量成倍增加，因为一个应用程序，需要引入多个 Kubernetes 资源。但我们比较不同的应用程序资源文件时，可以发现它们之间有大量类似的 YAML 配置。 下图是两个资源的 YAML 文件对比示例，它们之间有大量的相同配置，这些相同的部分就是模板。蓝色文本表示模板行，红色文本表示可定制的行： 图1.5 - 两个有模板的资源配置 在这个示例中，两个文件几乎完全相同。对于管理声明式应用程序的团队来说，模板文件会成为一个让人头痛的问题。 Helm 脚踏七彩祥云而来 Kubernetes 社区也意识到了通过创建、维护 Kubernetes 资源部署应用程序的复杂度，为了帮助使用者客服这些挑战，一个简单、强大、开源、能够在 Kubernetes 上部署应用程序的工具诞生了。这个工具就是 Helm，用于在 Kubernetes 上打包和部署应用程序。因为它与操作系统上的包管理器有很多相似之处，它也被称为 Kubernetes 的包管理器。Helm 在 Kubernetes 社区被广泛使用，是 CNCF 的毕业项目。 鉴于 Helm 与操作系统包管理器的相似之处，并且操作系统包管理器又是我们或多或少使用过的。为了便于大家理解，我们先以操作系统包管理器为切入点，作为学习 Helm 的开始。 包管理器 包管理器用于简化操作系统中应用程序的安装、升级、恢复和删除过程。这些应用程序以包为单元定义，包包含目标软件及其依赖项的元数据。 包管理器的工作过程很简单。首先用户将需要安装的软件包名称作为参数传递，包管理器在包存储库中查找，如果找到，包管理器将包定义的应用程序及依赖项安装到操作系统的指定位置。 包管理器让管理软件变得非常容易，举个例子，我们在 Rocky Linux 8 系统上安装 htop 包(htop 命令可以查看主机资源使用情况)，只需执行以下命令： dnf -y install htop 执行此命令，dnf 会在 Rocky Linux 包存储库中查找并安装 htop，同时也会安装 htop 的依赖包(如果有依赖的话)，所以你无需担心依赖项。 如果出现了 htop 的新版本，可以通过 upgrade 子命令安装更新： dnf upgrade htop -y 这条命令会升级 htop 及其依赖包到最新的版本。 包管理器还支持将包回滚到以前版本： dnf downgrade htop -y 在更新的版本存在漏洞或错误时，回滚功能就显得尤为重要了。 删除安装的包： dnf remove htop -y 注意： Rocky Linux 是 CentOS 项目的创始人 Gregory Kurtzer 领导开发的，用以替换 CentOS。红帽宣称 CentOS 8 将在2021年底停止维护。 在 CentOS 7 及其之前的版本中，使用 yum 作为包管理，在 CentOS 8 及 Rocky Linux 8 中，包管理器为 dnf，两者使用方式相似。 对学习 Rocky Linux 系统感兴趣的读者，可以阅读我的《Rocky Linux最佳实践》。 Helm 作为 Kubernetes 的软件包管理器，作用与 dnf 相似。dnf 用来管理 Rocky Linux 上的应用程序，Helm 用来管理 Kubernetes 上的应用程序。我将在下文中对此进行更详细的介绍。 Kubernetes 的包管理器 虽然 Helm 和 dnf 同为包管理器，但实现的细节还是有些差异的。dnf 操作的是 RPM 包，Helm 处理的 Charts。Helm 的 Charts 可以认为是 Kubernetes 的 RPM 包。Charts 包含部署应用所需的声明性资源文件，与 RPM 类似，它也可以包含应用程序的依赖项。 Helm 通过存储库获取 Charts。运维人员在创建完成声明性的 YAML 文件后，打包为 Charts，然后将 Charts 包存放在存储库中，用户使用 Helm 搜索要部署到 Kubernetes 上的 Charts，这类似用户使用 dnf 搜索要部署到 Rocky Linux 的 RPM 软件包。 以部署 Redis 为例，我们使用已发布到存储库中的 Redis Chart，通过 Helm 将其部署到 Kubernetes 上： helm install redis bitnami/redis --namespace=redis 这条命令把 bitnami chart 仓库中的 redis chart 安装到 Kubernetes 的 redis 命名空间中。这是初始安装。 如果有新版的 Redis Chart 可用，可以使用如下命令升级到新版本： helm upgrade redis bitnami/redis --namespace=redis 如果发现新版本存在 bug，用户会比较关心如何回滚，Helm提供了 rollback 命令来处理回滚操作： helm rollback redis 1 --namespace=redis 这个命令会将 Redis 回滚到第一个版本。 当你不再需要 Redis 的时候，可以通过 uninstall 命令删除： helm uninstall redis --namespace=redis 了解了 Helm 作为包管理器的功能后，我们更详细的讨论下 Helm 给 Kubernetes 带来的好处。在上一节，我们讨论了创建 Kubernetes 资源时的挑战，现在来看看 Helm 是如何解决这些挑战的。 Kubernetes 资源的抽象复杂性 假设我们接到了在 Kubernetes 上部署 MySQL 的任务，我们需要配置创建容器、网络、存储所需的资源。从头配置这样的应用程序，有很高的 Kubernetes 相关技术要求，对于新手或者经验不丰富的用户来说，会是一个不小的挑战。 有了 Helm，我们可以从 Charts 库中搜索 MySQL Charts。这些 Charts 由社区中的 Charts 开发人员编写，包含了部署 MySQL 所需的声明性配置。所以我们可以直接使用 Helm 安装MySQL，非常简单。 历史版本 Helm 有一个概念叫做发布历史。当第一次安装 Helm chart 时，Helm 会记录一个初始版本。随着版本更新，历史记录也随着修改，从而保存了应用程序不同版本的快照。 下图是版本更新的历史记录，蓝色方块表示更新的版本： 1.6-Upgrade 图1.6 - 版本历史 记录每次版本更新便于回滚操作。Helm 中的版本回滚非常简单，用户只需指向先前的版本，Helm 就会将运行状态恢复到所选版本。有了 Helm，n-1 备份的日子已经一去不复返了。Helm 可以回滚到任意的版本，甚至可以回滚到应用第一次部署时的状态。 配置动态声明式资源 Kubernetes 的声明式资源文件是静态的，无法参数化，哪怕相似度很高的两个资源文件，都得独立配置，增加了很多工作量和维护成本。Helm 引入了 values 和 template 解决了这个问题。 values 是 Helm Charts 的参数，template 是基于 values 动态生成的资源文件，有了它们，Helm 管理应用程序就灵活多了，并且更易于维护。 values 和 template 允许用户执行以下操作： 参数化公共字段，例如 Deployment 中的镜像名称和 Service 中的端口 根据用户输入生成较长的 YAML 配置片段，例如 Deployment 中的卷挂载或 ConfigMap 中的数据 包含或排除资源 动态生成声明式资源文件使创建基于 YAML 格式的资源变得更简单，又容易复制。 本地状态和运行时一致性 包管理器有效避免了用户手动修改应用程序及依赖，所有对应用程序包及依赖的修改，都可以通过包管理器完成。这个优势同样适用于 Helm，因为 Helm charts 包含了 Kubernetes 资源的各种配置，所以用户无需直接修改 Kubernetes 的资源。如果用户需要修改应用程序，可以通过向 Helm charts 提供新的 values，并对版本进行更新来实现。这有效保证了本地状态和运行时状态在修改过程中的一致性。 智能部署 Helm 能够识别要创建的 Kubernetes 资源的先后顺序，简化应用程序部署过程。Helm 分析 Charts 包含的资源，并根据资源类型排序，避免因依赖关系而导致资源部署失败。例如 Secret 和 Configmap 应在 Deployment 之前创建，因为 Deployment 可能会以卷的形式挂载这些资源。Helm 无需用户干预就能按先后顺序创建它们，这种复杂性被抽象出来，用户无需关心这些资源的创建顺序。 生命周期 hooks 管理 与其他包管理器类似，Helm 提供了定义生命周期钩子(hooks)的能力。生命周期钩子可以在应用程序生命周期的不同阶段自动触发事先定义好的操作。它们可以做以下事情： 升级时执行备份 回滚时恢复数据 在安装应用程序前验证 Kubernetes 环境 生命周期钩子很有用，因为它们将任务的复杂性抽象出来，而这些任务可能不是 kubernetes 特有的。例如，Kubernetes 用户可能不熟悉数据库备份的最佳实践，或者不清楚何时该执行这样的任务。生命周期钩子可以执行专业人员编写的最佳实践的自动化程序，这样用户就可以继续高效地专注于自己的工作，而不必担心其他细节。 By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-10 18:21:46 "},"helm/bushu.html":{"url":"helm/bushu.html","title":"Helm 部署","keywords":"","body":"部署 Kubernetes 和 Helm 环境 Helm 能为我们提供很多帮助，但在使用 Helm 前，我们要有 Kubernetes 环境。搭建高可用的 Kubernetes 集群需要的主机资源比较多，搭建过程也比较复杂，所幸使用 Minikube 可以快速搭建单机版的 Kubernetes 集群，又不失真实性。 如果你对安装高可用的 Kubernetes 集群感兴趣，或者想要学习 Kubernetes 相关知识，可以阅读我的 《Kubernetes 最佳实践》。 使用 Minikube 部署 Kubernetes 没有 Kubernetes 集群，Helm 全无用武之地。因此，我们先使用 Minikube 搭建单机版的 Kubernetes 集群。 Minikube 是社区驱动的工具，使用它可以很方便的部署一个单节点的 Kubernetes 集群。我们可以创建一台虚拟机，用来作为部署 Kubernetes 的主机。 安装 Minikube Minikube 支持 Windows、MacOS 和 Linux 操作系统，我们可以在 GitHub 页面上下载对应的最新版本的二进制包。本书的示例都是在 Rcoky Linux 上进行的，所以我下载安装 Linux 版本的 Minikube。 下载地址：https://github.com/kubernetes/minikube/releases/。在该页面 “Latest release” 的 “Assets” 部分，查看并下载对应版本。如图： 2.1versions 图2.1 - GitHub 页面中的 Minikube 二进制文件，该图仅展示了 “Assets” 的部分内容 下载与操作系统对应版本的二进制文件： [root@aiops0113 ~]# wget https://github.com/kubernetes/minikube/releases/download/v1.20.0/minikube-linux-amd64 下载完成后，将二进制文件重命名为 minikube。例如，我的是 Linux 系统，需要执行以下命令： [root@aiops0113 ~]# mv minikube-linux-amd64 minikube 要执行 minikube，Linux 和 macOS 用户需要为该文件添加可执行权限： [root@aiops0113 ~]# chmod a+x 将 minikube 放置 PATH 变量管理的路径中，以便在命令行执。PATH 变量的内容取决于操作系统。Linux 和 macOS 用户，可以在终端执行以下命令查看路径信息： [root@aiops0113 ~]# echo $PATH Windows 用户可以在命令提示符或者 PowerShell 中执行以下命令： env: PATH 将 minikube 移动到 /usr/local/bin/ 目录： [root@aiops0113 ~]# mv minikube /usr/local/bin/ 验证命令是否能正常使用： [root@aiops0113 ~]# minikube version # 返回类似以下内容 minikube version: v1.20.0 commit: c61663e942ec43b20e8e70839dcca52e44cd85ae Minikube 已经安装完成，我们还需要 hypervisor 工具管理本地 Kubernetes 集群，推荐使用 VirtualBox。 Minikube 驱动 Minikube 依赖驱动安装 Kubernetes 集群。操作系统不同，Minikube 支持的驱动也不同。下图是 Minikube 在不同操作系统上支持的驱动： 2.2minikubeDrivers 针对不同的操作系统，Minikube 都有推荐的驱动器。在本书示例中，我使用 VirtualBox 驱动，你可以尝试其它驱动，作为技术拓展。 安装 VirtualBox Minikube 依赖 hypervisors (虚拟机管理器) 将 Kubernetes 集群安装到 VM (虚拟机)上。在本书我们使用 VirtualBox，因为它同时支持 Windows、macOS 和 Linux 操作系统。 Windows 上安装 VirtualBox： > choco install virtualbox macOS 上安装 VirtualBox： $ brew cask install virtualbox Debian Linux 上安装 VirtualBox： $ apt-get install virtualbox 基于 RHEL 的 Linux 上安装 VirtualBox，Rocky Linux 和 CentOS 都是基于 RHEL 的操作系统： [root@aiops0113 ~]# dnf config-manager --add-repo=https://download.virtualbox.org/virtualbox/rpm/el/virtualbox.repo root@aiops0113 ~]# dnf search virtualbox VirtualBox-5.2.x86_64 : Oracle VM VirtualBox VirtualBox-6.0.x86_64 : Oracle VM VirtualBox VirtualBox-6.1.x86_64 : Oracle VM VirtualBox [root@aiops0113 ~]# dnf -y install VirtualBox-6.1 更多安装方法可以在 VirtualBox 官网查看：https://www.virtualbox.org/wiki/Downloads。 安装 VirtualBox 后，须将 Minikube 的默认 hypervisor 设置为 VirtualBox。 设置 VirtualBox 为默认 hypervisor 设置 minikube 的 vm-driver 为 VirtualBox： [root@aiops0113 ~]# minikube config set vm-driver virtualbox 执行此命令，可能会有打印出以下警告： ! These changes will take effect upon a minikube delete and then a minikube start 如果你还没有通过 Minikube 创建集群，可以忽略此消息。这句话的意思是：在执行此命令前创建的 Kubernetes 集群都不会以 VirtualBox 作为 hypervisor。 查看 vm-driver 的值： [root@aiops0113 ~]# minikube config get vm-drivervirtualbox 如果一切正常，会输出 VirtualBox。 除了配置默认 hypervisor 外，还可以配置分配给 Minikube 集群的资源。 设置 Minikube 资源 默认情况下，Minikube 会为虚拟机分配 2 核 CPU 和 2GB 内存的资源。这些资源足以运行本书中大部分示例。如果你的主机资源充足，可以将内存改为 4GB，CPU不变。 执行以下命令，可以将 Minikube 默认分配的内存大小改为 4GB： [root@aiops0113 ~]# minikube config set memory 4000 运行 minikube config get memory 命令验证对内存的设置，类似于前面验证 vm-driver 的更改： [root@aiops0113 ~]# minikube config get memory Minikube 基本用法 Minikube 是一个入门简单的工具，下面是它的三个常用命令： start stop delete start 子命令用于创建单节点的 Kubernetes 集群。执行此命令，会创建一台虚拟机，并在其中安装 Kubernetes 集群，集群安装完成，命令会结束执行。 在执行 start 子命令前，我们需要创建一个普通账户，并切换到普通账户执行，因为 VirtualBox 驱动不能以 root 用户，使用 root 用户会报错。如果没有特别说明，以后的示例都会使用该普通账号。 [root@aiops0113 ~]# useradd aiops [root@aiops0113 ~]# su - aiops [aiops@aiops0113 ~]$ minikube start 由于网络因素，中国用户在启动 minikube 时，需要设置几个选项： [aiops@aiops0113 ~]$ minikube start --image-mirror-country='cn' \\ --iso-url=https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/iso/minikube-v1.20.0.iso # 以下是安装时的正常输出 * minikube v1.20.0 on Rocky 8.3 * Automatically selected the virtualbox driver * Using image repository registry.cn-hangzhou.aliyuncs.com/google_containers * Starting control plane node minikube in cluster minikube * Creating virtualbox VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... > kubelet.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s > kubelet: 108.73 MiB / 108.73 MiB [----------] 100.00% 611.48 KiB p/s 3m2s - Generating certificates and keys ... - Booting up control plane ... - Configuring RBAC rules ... * Verifying Kubernetes components... - Using image registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5 (global image repository) * Enabled addons: storage-provisioner, default-storageclass * kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A' * Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default 注意： 在执行 minikube start 时，可能会遇到以下问题： ! Startup with virtualbox driver failed, trying with alternate driver ssh: Failed to start host: creating host: create: precreate: We support Virtualbox starting with version 5. Your VirtualBox install is \"WARNING: The vboxdrv kernel module is not loaded. Either there is no module\\n 这是因为 VirtualBox 的内核组件未加载，执行以下步骤可以解决此问题： [root@aiops0113 ~]# dnf -y install gcc make perl kernel-devel elfutils-libelf-devel [root@aiops0113 ~]# rcvboxdrv setup vboxdrv.sh: Stopping VirtualBox services. vboxdrv.sh: Starting VirtualBox services. vboxdrv.sh: Building VirtualBox kernel modules. Failed to start host: creating host: create: precreate: This computer doesn't have VT-X/AMD-v enabled. Enabling it in the BIOS is mandatory 需要开启 VT-X/AMD。如果你直接在个人电脑的系统上安装的，可以设置 BIOS 的 Intel Virtual Technology(这个是 Intel CPU的设置) 的值是否为 Enable；如果你是在 VMware Workstation 或者 VirtualBox 中嵌套的虚拟机，需要在处理器虚拟化设置中勾选 VT-X/AMD-v 。 stop 子命令用于关闭集群和虚拟机。集群和虚拟机的运行状态会保存在磁盘上，下次可以通过 start 子命令快速启动，而不是重新创建一个新的虚拟机。 $ minikube stop delete 子命令用于删除集群和虚拟机。此命令会删除集群和虚拟机的运行状态，释放分配的磁盘空间。下次执行 minikube start 时，将重新创建新集群和虚拟机。 $ minikube delete Minikube 还有其他子命令，不过在本书中使用甚少，你可以自己研究下。 当 minikube start 命令执行完毕，Kubernetes 集群就安装完成了，但安装时打印的内容显示：kubectl not found，接下来我们安装 kubectl 命令行工具。 安装 kubectl 有了 kubectl，用户就无需直接通过 Kubernetes API 端点执行各种操作了(如 创建、查询、删除资源)，大大提高了操作 Kubernetes 集群便捷性。 编写此文档时，最新的 kubectl 版本是 xxx，在整本书中，我们就以此版本为例。 可以使用 Minikube 安装 kubectl，也可以通过包管理或者直接下载二进制文件的方式安装。 使用 Minikube 安装 Minikube 提供了 kubectl 的子命令，执行子命令会下载 kubectl 二进制文件： $ minikube kubectl version 这个命令会把 kubectl 安装到 $HOME/.minikube/cache/linux/v1.20.2/ 目录下，kubectl 版本取决于 Minikube 版本，你的路径中的 linux/v1.20.2/ 部分，可能和我的不同。 minikube kubectl 的语法如下： $ minikube kubectl 示例： $ minikube kubectl version client 虽然 minikube kubectl 实现了 kubectl 的所有功能，但它语法比直接使用 kubectl 繁琐。可以把 kubectl 可执行文件从 Minikube 缓存目录拷贝到 $PATH 管理的路径。在 Linux 服务器上执行： $ sudo cp ~/.minikube/cache/linux/v1.20.2/kubectl /usr/local/bin/ 现在，kubectl 可以作为一个独立的二进制文件使用了，如下所示： $ kubectl version client 包管理器安装 不同操作系统使用的包管理不同，安装方式也略有差异： 在 Windows 上安装 kubectl > choco install kubernetes-cli 在 macOS 上安装 kubectl $ brew install kubernetes-cli 在基于 Debian 的 Linux 上安装 kubectl： $ sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2 $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - $ echo 'deb https://apt.kubernetes.io/ kubernetes-xenial main' | sudo tee -a /etc/apt/sources.list.d/kubernetes.list $ sudo apt-get update $ sudo apt-get install -y kubectl 在基于 RHEL 的 Linux 上安装 kubectl： [aiops@aiops0113 ~]$ sudo cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF [aiops@aiops0113 ~]$ sudo dnf -y install kubectl 二进制安装 这是最常用的方式，和介绍的第一种方式相似，只是以不同方式下载二进制文件。 下载最新稳定版 kubectl 的二进制文件： [aiops@aiops0113 ~]$ curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" 添加执行权限 [aiops@aiops0113 ~]$ chmod +x kubectl 移动到 /usr/local/bin/ 目录 [aiops@aiops0113 ~]$ sudo mv kubectl /usr/local/bin/ 验证： $ kubectl version client 安装 Helm Windows 的 Chocolatey 和 macOS 的 Homebrew 包管理器中已经有了 Helm 包，可以直接安装： Windows 安装 Helm： > choco install kubernetes-helm macOS 安装 Helm： $ brew install helm Linux 用户可以通过下载页面，下载 Helm 的二进制文件进行安装： 在 https://github.com/helm/helm/releases 页面下载对应操作系统的二进制文件； 使用 tar 命令，解压压缩包： $ tar zxf helm-v3.5.4-linux-amd64.tar.gz 将解压后的二进制文件移动到 PATH 路径： $ sudo mv linux-amd64/helm /usr/local/bin/ 配置 Helm Helm 提供了大量默认配置，实现了开箱即用。但为了使用上更便利，我们可以修改下面几个参数。 添加存储库 在“包管理器安装”小节中介绍包管理器安装 kubectl 时，我们为包管理器添加了软件包仓库，用来安装 kubectl。Helm 作为 Kubernetes 的软件包管理器，也可以添加各种可靠的图表存储库，用以在 Kubernetes 集群中安装应用程序。 Helm 提供了 repo 子命令管理图表存储库。repo 子命令又包含子命令，用于操作指定的存储库。以下是 repo 的五个子命令： add：添加图表存储库 list：列出已添加的图表存储库 remove：删除已添加的图表存储库 update：图表安装后，它的元数据被缓存到本地，update 子命令从图表库中更新数据到本地 index：在打包的图表目录下生成索引文件 repo add 子命令示例： $ helm repo add $REPO_NAME $REPO_URL repo list 子命令示例： $ helm repo list repo update 子命令示例： $ helm repo update repo remove 子命令示例： $ helm repo remove $REPO_NAME helm repo index 命令会在开发图表的时候介绍。 添加插件 Helm 插件由其社区维护，用以增强 Helm 的功能。插件列表地址： https://helm.sh/docs/community/related/ Helm 提供了 plugin 子命令管理插件，plugin 包含的子命令如下表： plugin 子命令 描述 语法 install 安装一个或多个 Helm 插件 helm plugin install $URL list 查询已安装的 插件 helm plugin list uninstall 卸载一个或多个已安装的插件 helm plugin uninstall $PLUGIN update 更新一个或多个已安装的插件 helm plugin update $PLUGIN 插件增强了 Helm 功能，以下是几个插件的例子： helm diff：对比已部署的应用程序不同版本的差异 helm secrets：Used to help conceal secrets from Helm charts 加密 helm charts helm monitor：监控发布过程，并在出现某些事件时回滚 helm unittest：用于在 Helm 图表上执行单元测试 在本书的后面，会有专门一个章节介绍插件。 环境变量 Helm 的一些选项可以通过系统的环境变量配置，以下是六个主要环境变量： XDG_CACHE_HOME：设置缓存文件的存储目录 XDG_CONFIG_HOME：设置 helm 的配置文件 XDG_DATA_HOME：设置数据存储目录 HELM_DRIVER：设置后端存储驱动程序 HELM_NO_PLUGINS：禁用插件 KUBECONFIG：设置 Kubernetes 配置文件 Helm 遵循 XDG 目录规范，该规范定义了一套指向应用程序的环境变量，变量指明了这些应用程序应该存储的基准目录。根据 XDG 规范，Helm 会根据需要，在操作系统上自动创建三个默认目录： 操作系统 缓存目录 配置目录 数据目录 Windows %TEMP%\\helm %APPDATA%\\helm %APPDATA%\\helm macOS $HOME/Library/Caches/helm $HOME/Library/Preferences/helm $HOME/Library/helm Linux $HOME/.cache/helm $HOME/.config/helm %HOME/.local/share/helm 换成目录缓存 helm 从 Charts 仓库中下载的charts，已安装的 charts 会被缓存到该目录，以便下次能够快速安装。如需更新缓存，可以使用 helm repo update 命令，它会下载最新的可用charts，更新本地缓存。 配置目录保存通过 helm repo add 命令添加的存储库信息。安装尚未缓存到本地的 Charts 时，Helm 会根据配置目录的路径查询 Charts 存储库的 RUL，通过 URL 找到所需的 Charts。 数据目录用来存储插件，使用 helm plugin install 命令安装的插件数据会存储在这个位置。 HELM_DRIVER 定义发布状态如何存储在 Kubernetes 中，默认值是 secret，也是推荐值，它将状态存储在 Kubernetes Secret中，Secret 是 Base64 编码；该值也可以设置为 configmap，它将状态以纯文本的形式存储在 Kubernetes ConfigMap 中；如果该值设置为 memory，状态信息将存储在本地内存中，存储在内存中的方式，不适用于生产环境。 HELM_NO_PLUGINS 变量用来禁用插件，插件默认为启用状态，其值为 0。如需禁用插件，需要将值设置为 1。 KUBECONFIG 变量设置认证 Kubernetes 集群的文件。如果未设置，默认为 ~/.kube/config。大多数情况下，用户无需修改此值。 命令补全 Bash 和 Z Shell 用户可以启用 Tab键 补全命令，既能简化命令输入，又能有效避免输多手误。当按下 Tab键 时，终端会通过输入命令的状态猜测下一个参数，例如：cd /usr/local/b，输入 Tab键，会自动补全为 cd /usr/local/bin，同样的，执行 helm upgrade hello-，按下 Tab键，会自动补全为 helm upgrade hello-world。 启用 Tab键 补全功能： $ source $SHELL 必须是 bash 或 zsh。自动补全仅在执行了此命令的窗口中有效，其他窗口则无效。 身份验证 Helm 需要通过 Kubernetes 集群的验证，才能部署和管理应用程序。它们之间通过 kubeconfig 文件进行身份验证，该文件定义了一个或多个 Kubernetes 集群如何进行身份验证。 使用 Minikube 安装的 Kubernetes 集群不需要配置身份验证，因为每次创建集群时，Minikube 都会自动生成一个 kubeconfig 文件。但那些没有运行 Minikube 的用户需要创建(提供)一个 kubeconfig 文件，这取决于你的 Kubernetes 集群。 通过以下三个 kubectl 命令创建 kubeconfig 文件： 第一个命令是 set-cluster： kubectl config set-cluster set-cluster 命令在 kubeconfig 文件中定义一个集群条目，确定 Kubernetes 集群的主机名或 IP 地址，以及它的证书颁发机构。 第二个命令是 set-credentials： kubectl config set-credentials set-credentials 命令定义用户名及其身份验证方法和详细信息。该命令可以配置用户名和密码对、客户端证书、承载令牌或身份验证提供者，用户和管理员可以指定不同的身份验证方法。 第三个命令是 set-context： kubectl config set-context set-context 命令将凭证关联到集群。一旦凭证和集群建立了关联，用户就可以使用凭证的身份验证方法对指定的集群进行身份验证。 kubectl config view 命令可以查看 kubeconfig 文件。 一旦有了 kubeconfig 文件，Kubectl 和 Helm 就可以与 Kubernetes 集群交互了。 在下一节，我们将介绍如何针对 Kubernetes 集群进行授权。 Authorization/RBAC 身份验证是确认身份的一种方式，而授权定义了允许经过身份验证的用户执行的操作。Kubernetes 使用基于角色访问控制（RBAC）执行授权的。RBAC 是一个设计角色和权限的系统，可以将角色和权限分配给指定的用户或用户组。用户被允许在 Kubernetes 上执行的操作取决于用户被分配的角色。 Kubernetes 提供了许多不同角色，这里列出了三种常见的角色： cluster-admin：管理员角色，允许用户对整个集群中的任何资源执行任何操作。 edit：允许用户读写 namespace 或逻辑分组中的大多数 Kubernetes 的资源。 view：禁止用户修改资源，仅允许用户读取 namespace 内的资源。 由于 Helm 使用 kubeconfig 文件中定义的凭据对 Kubernetes 进行身份验证，因此 Helm 被赋予了与文件中定义的用户相同的访问级别。如果设置的 edit 权限，Helm 在大多数情况下拥有安装应用程序的足够权限。如果是 view 权限，Helm 将不能安装应用程序，因为这个角色是只读的。 Minikube 默认提供的是 cluster-admin 权限，虽然这不是生产环境中的最佳实践，但对于学习和试验来说是可以接受的。使用其他方式安装 Kubernetes 集群的读者，注意至少要分配 edit 角色，以便 Helm 能够在大多数情况下部署应用程序： $ kubectl create clusterrolebinding $USER -edit --clusterrole=edit --user=$USER 在“Helm 安全”一章，我们会详细讨论 RBAC，以及如何分配合适的角色，以防止错误或恶意意图。 By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-10 18:19:14 "},"helm/anzhuangcharts.html":{"url":"helm/anzhuangcharts.html","title":"安装 Helm 图表","keywords":"","body":"安装 Helm 图表 在本书的第一章，我把 Helm 定义为 Kubernetes 的包管理器，并与操作系统的包管理器进行了比较。用户使用包管理器可以便捷地安装应用程序、管理它们可能需要的依赖。这也是 Helm 的作用。 在 Kubernetes 上安装应用程序时，用户只需输入对应的图表名称，Helm 会完成剩余工作。Helm 图表包含安装应用程序所需的逻辑和组件，用户无需知道具体的细节即可执行安装。用户还可以将值(values)作为参数传递给 Helm 图表，用来配置应用程序，实现自定义安装。在本章中，我们通过使用 Helm 在 Kubernetes 集群中安装 WordPress 博客系统，来学习 Helm 包管理器的特性。 WordPress 介绍 WordPress 是一个开源的内容管理系统(CMS)，可以创建网站和博客。它有两个版本：SaaS 版本和自维护版本。SaaS 版需要用户花钱购买服务，自维护就是用户下载软件包自行部署维护。笔者的 https://www.aiops.red 就是使用 WordPress 的自维护版本搭建的。 WordPress 将网站数据存储在数据库中，数据库要求 MySQL 或者 MariaDB。在 Kubernetes 集群中部署一套 WordPress 服务需要创建的资源如下： Secret：管理数据库和控制台的凭证 ConfigMap：配置数据库 Service：负载均衡 PersistentVolumeClaim：持久化存储数据 StatefulSet：以有状态的方式部署数据库 Deployment：部署 WordPress 前端服务 创建这些资源需要对 WordPress 和 Kubernetes 都有深入的了解，要知道如何配置 WordPress，以及如何将 WordPress 的需求作为 Kubernetes 的资源表述出来。考虑到所需资源的复杂性以及需要的资源类型较多，在 Kubernetes 上部署 WordPress 可能是一项令人生畏的任务。 完成这一挑战是 Helm 的完美使用案例。用户不必专注于创建和配置每个 Kubernetes 资源，也无需 WordPress 的专业知识，只需使用 Helm 包管理器，在 Kubernetes 上部署和配置 WordPress。首先，我们在 Helm Hub 平台上找到 WordPress 的 Helm 图表，之后使用 Helm 将 WordPress 部署到 Kubernetes 集群中， 并在部署的过程中，探索 Helm 的基本特性。 搜索 WordPress 图表 Helm 图表可以存储在图表存储库中，供其他人使用。图表存储库是一个 HTTP 服务，如 GitHub、NGINX Web 服务器，可以存储和共享图表包。 在使用 Helm 图表安装 WordPress 前，我们需要配置 Helm 能够从哪些存储库中获取图表。添加存储库的命令就是之前介绍过的 helm repo add。为了方便查找图表，Helm 社区创建了 Helm Hub 平台。 Helm Hub 旨在聚合所有已知的公共图表，并提供了搜索功能。在本章中，我们使用 Helm Hub 平台搜索所需的 WordPress Helm 图表。在找到合适的图表后，添加该图表的存储库，以便执行安装。 可以通过命令或 Web 浏览器与 Helm Hub 交互。当使用命令行搜索 Helm 图表时，返回的结果包括了 Helm Hub 的 URL（ URL 可用于查找有关 chart 的其他信息）、版本及说明。 从命令行搜索 WordPress 图表 Helm 提供了两个不同的搜索命令帮助我们查找 Helm 图表： 在 Helm Hub 中搜索图表 $ helm search hub 通过图表的关键字在存储库中搜索： $ helm search repo 如果没有添加过图表存储库，用户应使用 helm search hub 命令从所有公共图表存储库中搜索可用的 helm 图表。在添加存储库后，用户可以使用 helm search repo 命令在已添加的存储库中搜索图表。 我们从 Helm Hub 中搜索所有的 WordPress 图表。Helm Hub 中的每个图表都包括一组可以搜索的关键字。执行以下命令搜索包含 wordpress 关键字的图表： [aiops@aiops0113 ~]$ helm search hub wordpress 执行此命令会显示以下输出： 3.1 helm search hub wordpress outputs 该命令返回的每一行都来自 Helm Hub 的一个图表 。输出内容包括图表 Helm Hub 页面的 URL，Helm 图表的最新版本，图表部署的应用程序版本，以及描述信息。 你可能注意到有些信息被截断了，这是因为 helm search hub 以表格形式返回结果，默认超过 50 个字符的列将被截断。可以通过 --max-col-width=0 参数，显示完整信息： $ [aiops@aiops0113 ~]$ helm search hub wordpress --max-col-width=0 这次以表格的形式完整输出了字段信息。 还可以通过 --output 参数指定 yaml 或 json 格式输出： [aiops@aiops0113 ~]$ helm search hub wordpress --output yaml 结果将以 YAML 格式显示，类似下图： 3.2 helm search hub wordpress yaml outputs 我们选择返回的第一个图表进行安装。要了解更多该图表的信息，可以访问 https://artifacthub.io/packages/helm/bitnami/wordpress 页面。 从浏览器搜索 WordPress 图表 使用 helm search hub 命令是在 helm hub 中搜索图表的最快方式，但它并不能提供完整的信息。用户需要添加图表存储库的 URL，以便安装图表。图表的Helm Hub 页面提供了此 URL，以及其他内容。 把 WordPress 图表的 URL 粘贴到浏览器中： 3.3 wordpress page details Helm Hub 的 WordPress 图表页面提供了很多内容，包括图表简介、维护者、安装信息等。 点击右侧栏的 INSTALL 按钮，可以看到添加 Bitnami 图表库的命令： 在命令行中执行以下命令添加图表库： [aiops@aiops0113 ~]$ helm repo add bitnami https://charts.bitnami.com/bitnami 查看添加的图表库列表： [aiops@aiops0113 ~]$ helm repo list 搜索 bitnami 存储库中或其它存储库中包含 bitnami 关键字的图表，并以 YAML 格式显示： [aiops@aiops0113 ~]$ helm search repo bitnami --output yaml 与 helm search hub 命令类似，helm search repo 命令也使用图表关键字作为参数。将 bitnami 作为关键字将返回 bitnami 存储库中所有的图表，以及该存储库之外包含 bitnami 关键字的图表。 为了确保我们搜到的是 WordPress 相关的图表，可以将 wordpress 作为 helm search repo 命令的参数： [aiops@aiops0113 ~]$ helm search repo wordpress 返回的是我们在浏览器上看到的 Helm Hub 上的 WordPress chart： 3.4 bitnami wordpress NAME 列中，/ 前的字段表示 Helm 图表存储库的名称，写作此文时最新的图表版本是 11.0.5，也是我们要安装的版本。可以使用 version 参数，查看历史版本。 从命令行查看图表信息 在 Helm Hub 页面，我们看到了很多有用的图表信息。将图表存储库添加到本地，就可以使用 helm show 的四个子命令从命令行查看这些信息了： 显示图表的元数据： $ helm show chart 显示图表的自述(README)文件 $ helm show readme 显示图表的值(values)： $ helm show values 显示图表的定义、自述文件和值： $ helm show all 我们通过查看 Bitnami WordPress 的图表信息来熟悉一下这 4 个子命令，图表名称为 bitnami/wordpress。如果要查看特定版本的信息，需要使用 --version 标志，后跟版本号。如果省略此标志，将返回最新版本的图表信息。 运行 helm show chart 命令查看图表的元数据： [aiops@aiops0113 ~]$ helm show chart bitnami/wordpress --version 11.0.5 这个命令输出了图表的定义信息，包括图表版本、依赖关系、关键字和维护人员信息等： 3.5 show chart 运行 helm show readme 命令查看图表自述文件中的内容： [aiops@aiops0113 ~]$ helm show readme bitnami/wordpress --version 11.0.5 运行 helm show values 命令查看图表的值。用户通过提供值，实现图表的自定义配置。 运行 helm show all 命令，将前面三个命令中的信息聚合在一起。如果你想一次性查看图表的所有信息，请使用此命令。 我们已经搜索到了 WordPress 图表，并检查了它的内容，现在我们来配置 Kubernetes 环境，然后安装图表。 启动 Kubernetes 集群 在上一章中，我们使用 Minikube 创建了 Kubernetes 集群，如果你没有保持环境的运行，可以通过以下步骤重新启动 Kubernetes 集群： 启动 Kubernetes 集群： [aiops@aiops0113 ~]$ minikube start 在集群启动之后，我们创建一个名为 website 的 namespace，专门用来安装 WordPress： [aiops@aiops0113 ~]$ kubectl create namespace website 安装 WordPress 图表 Helm 图表的安装非常简单，只需要一条命令。但在安装前，需要我们确认图表包含的值，以及自定义哪些值，才能达到我们期望的安装效果。 创建值文件 可以通过创建 YAML 格式的值文件覆盖图表的默认值。为了创建正确的值文件，我们首先使用 helm show values 子命令查看图表支持的值： [aiops@aiops0113 ~]$ helm show values bitnami/wordpress --version 11.0.5 ## @section Global parameters ...... image: registry: docker.io repository: bitnami/wordpress tag: 5.7.2-debian-10-r2 ## Specify a imagePullPolicy ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent' ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images ## pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## e.g: ## pullSecrets: ## - myRegistryKeySecretName ## pullSecrets: [] ## Enable debug mode ## debug: false ## @section WordPress Configuration parameters ## WordPress settings based on environment variables ## ref: https://github.com/bitnami/bitnami-docker-wordpress#environment-variables ## @param wordpressUsername WordPress username ## wordpressUsername: user ## @param wordpressPassword WordPress user password ## Defaults to a random 10-character alphanumeric string if not set ## wordpressPassword: \"\" 该命令输出了 wordpress 图表能够设置的所有值，其中有很多已经提供了默认值。在安装图表时，如果我们没有提供值，图表将使用这些默认值。例如，在我们提供的值文件中没有覆盖 image 的值，WordPress 图表将使用默认的镜像配置：docker hub 镜像仓库中的 bitnami/wordpress 镜像，镜像 tag 是 5.7.2-debian-10-r2。 图表值文件中以 # 开头的是注释，注释可以作为某个值或块的说明，便于用户理解；也可以用来注释某个值，使其不再生效。 继续浏览输出内容，会看到设置 WordPress 元数据的值。这些值对于配置 WordPress 博客很重要，我们创建一个值文件覆盖它们。在你的机器上创建一个名为 wordpress-values.yaml 的文件，并将以下内容写入该文件： wordpressUsername: weiwendi # WordPress 控制台登录用户 wordpressPassword: Z5tOfyh # WordPress 控制台登录密码 wordpressEmail: weiwendi@aiops.red wordpressFirstName: AiOps wordpressBlogName: Learn Helm! 除了这些，还有一个重要的值我们要修改： service: type: LoadBalancer 这个值在 minikube 安装的 Kubernetes 集群中，需要改为 NodePort： service: type: NodePort 最终，完整的值文件内容为： wordpressUsername: weiwendi wordpressPassword: Z5tOfyh wordpressEmail: weiwendi@aiops.red wordpressFirstName: AiOps wordpressBlogName: Learn Helm! service: type: NodePort 创建完值文件，我们开始安装 WordPress 图表。 安装图表 helm install 命令用来安装 Helm 图表。语法如下： helm install [NAME] [CHART] [flags] NAME ：使用 Helm 安装的应用程序，在 Helm 中被称为 release，NAME 参数是我们为 release 起的名称。Release 会把 Kubernetes 资源与图表一起安装，并跟踪应用程序的生命周期。本章会介绍 release 是如何工作的。 CHART： Helm 图表的名称，对于图表存储库中的图表，遵循 \\/\\。 flags：代指 helm install 命令支持的一系列参数，方便用户定制安装图表。flags 允许用户设置、覆盖值，指定命名空间(namespace) 等。可以通过 helm install --help 命令查看 flags 列表。--help 参数也可以传递给其它命令，查看它们的用法和支持的选项。 我们已经正确掌握了 helm install 命令的用法，现在执行以下命令安装 WordPress 图表： [aiops@aiops0113 valuesFile]$ helm install wordpress bitnami/wordpress --values=wordpress-values.yaml --namespace website --version 11.0.5 在该命令中，我们使用 bitnami/wordpress 图表安装了一个名为 wordpress 的 release，并把自定义的值通过 wordpress-values.yaml 文件传递给了图表，以覆盖默认值，图表被安装到了 website 命名空间中。--version 标志指定了要部署的版本是 11.0.5。在不指定版本号的情况下，将安装最新版本的 Helm 图表。 如果图表安装成功，你会看到类似以下的输出内容： NAME: wordpress LAST DEPLOYED: Mon May 17 11:28:51 2021 NAMESPACE: website STATUS: deployed REVISION: 1 NOTES: ** Please be patient while the chart is being deployed ** Your WordPress site can be accessed through the following DNS name from within your cluster: wordpress.website.svc.cluster.local (port 80) To access your WordPress site from outside the cluster follow the steps below: 1. Get the WordPress URL by running these commands: export NODE_PORT=$(kubectl get --namespace website -o jsonpath=\"{.spec.ports[0].nodePort}\" services wordpress) export NODE_IP=$(kubectl get nodes --namespace website -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo \"WordPress URL: http://$NODE_IP:$NODE_PORT/\" echo \"WordPress Admin URL: http://$NODE_IP:$NODE_PORT/admin\" 2. Open a browser and access WordPress using the obtained URL. 3. Login with the following credentials below to see your blog: echo Username: weiwendi echo Password: $(kubectl get secret --namespace website wordpress -o jsonpath=\"{.data.wordpress-password}\" | base64 --decode) 输出内容显示了安装信息，包括 release 名称、部署时间、部署的命名空间、部署状态(已部署)和修订号(Revision)，由于该 release 是初次安装，所以修订号为 1。 输出内容还包括 Notes，Notes 用于向用户提供图表安装的附加信息。在 WordPress 图表的例子中，Notes 提示了如何访问和验证 WordPress 应用程序。可以通过 helm get notes 命令随时查看 Notes 信息。 Helm 图表安装完成后，我们可以通过 release 查看应用程序的资源清单和配置信息。 查看 release 可以在 Kubernetes 命名空间中查看所有已安装的 release： [aiops@aiops0113 valuesFile]$ helm list --namespace website NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION wordpress website 1 2021-05-17 10:50:12.321181442 +0800 CST deployed wordpress-11.0.5 5.7.2 list 子命令字段的含义： NAME：release 名称 NAMESPACE：release 所在的命名空间 REVISION：release 的最新修订号 UPDATED：release 发布的最新时间戳 STATUS：release 的状态 CHART：图表名称及版本 APP VERSION：图表中应用的版本 除了 list 子命令可以查看 release 的信息，Helm 还提供了 get 子命令，用来获取 release 的更多信息。下面的命令列表用来查看指定 release 的更详细信息： 或取 release 的所有钩子： helm get hooks 获取 release 的资源清单： helm get manifest 获取 release 的 notes： helm get notes 获取 release 的值： helm get values 获取 release 的所有信息，涵盖了以上 4 个命令： hlem get all 通过 helm get manifest 命令，可以查看到安装 release 时创建的 Kubernetes 资源清单： [aiops@aiops0113 valuesFile]$ helm get manifest wordpress --namespace website 执行此命令，可以看到以下 Kubernetes 资源清单： 两个 Secret 两个 Service 一个 ConfigMap 一个 ServiceAccount 一个 Deployment 一个 StatefulSet 一个 PersistentVolumeClaim 并且从输出中，还可以看到我们自定义的值，如：NodePort。 图表提供的大多数默认值都保持不变，这些值已经被应用到了 Kubernetes 资源上，可以通过 helm get manifest 命令查看到。如果修改了这些值，那么 Kubernetes 资源的配置也会改变。 helm get notes 命令显示了 release 安装完成后的说明信息。你可能还记得，在安装完成 WordPress 图表时显示过 release notes。这些 notes 引导用户如何访问应用程序，我们通过命令再次查看 notes： [aiops@aiops0113 valuesFile]$ helm get notes wordpress --namespace website helm get values 命令用来查看安装 release 时提供的值，也就是我们通过 wordpress-values.yaml 文件传递的值： [aiops@aiops0113 valuesFile]$ helm get values wordpress --namespace website 加上 --all 参数，可以显示所有值，包括默认值： [aiops@aiops0113 valuesFile]$ helm get values wordpress --all --namespace website helm get all 命令，可以打印所有来自各个 helm get 命令的信息： [aiops@aiops0113 valuesFile]$ helm get all wordpress --namespace website 除了 Helm 的命令外，kubectl 命令可以更方便的查看安装的资源。kubectl 可以精确查询某种类型的资源，如 Deployment，而不是获取所有的资源。为了确保返回的资源是 Helm 部署 release 时创建的，可以给 kubectl 提供部署时定义的标签，Helm 图表通常会为 Kubernetes 资源设置标签。通过以下命令，查看带有 app.kubernetes.io/instance=wordpress 标签的 Kubernetes 资源： [aiops@aiops0113 valuesFile]$ kubectl get all -l app.kubernetes.io/instance=wordpress --namespace website install 的更多用法 -n 标志 -n 标志可以替代 --namespace，以减少输入命令时的工作量，如下示例： [aiops@aiops0113 valuesFile]$ helm list --namespace website # 可以写成 [aiops@aiops0113 valuesFile]$ helm list -n website HELM_NAMESPACE 环境变量 如果不指定命名空间，Helm 的操作都会在 default 命名空间进行。可以修改 HELM_NAMESPACE 变量的值，设置 Helm 默认交互的命名空间。在不同操作系统上设置环境变量的方法略有不同： macOS 和 Linux： $ export HELM_NAMESPACE=website Windows 用户使用 PowerShell： > $env:HELM_NAMESPACE = 'website' 修改后，使用 helm env 命令验证： [aiops@aiops0113 valuesFile]$ helm env 在本书中，我们不依赖于 HELM_NAMESPACE 变量，而是在每个命令上使用 -n 标志，这样能更清楚地知道我们使用的命名空间。使用 -n 标志也是为 Helm 指定命名空间的最佳方式，因为它便于我们确认操作的命名空间是否正确。 --set 和 --values 对于 install、upgrade、rollback 命令，可以使用两种方式将值传递给图表： --set：从命令行传入值。 --values：从 YAML 值文件或 URL 中传入值 --values 标志是本书中配置图表值的首选方法，用这种方式配置多个值会更方便，还可以将值文件保存在源码管理(SCM)系统(如git)中。但对于密码等敏感值，建议使用 --set 标志。 --set 直接从命令行传递值，对于密码类型的、少量的值，这种方式可以接受，但 --set 不方便重复安装图表。 访问 WordPress WordPress 图表安装完成后，Notes 输出了四个命令，用于配置访问 WordPress 程序。 macOS 或者 Linux： $ export NODE_PORT=$(kubectl get --namespace website -o jsonpath=\"{.spec.ports[0].nodePort}\" services wordpress) $ export NODE_IP=$(kubectl get nodes --namespace website -o jsonpath=\"{.items[0].status.addresses[0].address}\") $ echo \"WordPress URL: http://$NODE_IP:$NODE_PORT/\" $ echo \"WordPress Admin URL: http://$NODE_IP:$NODE_PORT/admin\" Windows PowerShell： > $NODE_PORT = kubectl get --namespace website -o jsonpath=\"{.spec.ports[0].nodePort}\" services wordpress | Out-String > $NODE_IP = kubectl get nodes --namespace website -o jsonpath=\"{.items[0].status.addresses[0].address}\" | Out-String > echo \"WordPress URL: http://$NODE_IP:$NODE_PORT/\" > echo \"WordPress Admin URL: http://$NODE_IP:$NODE_PORT/admin\" 把 kubectl 的执行结果传递给两个变量，echo 命令打印拼接在一起的两个变量，输出访问 WordPress 的 URL。第一个 RUL 是网站主页，用户通过该页面查看网站内容；第二个 URL 是网站的管理控制台，网站管理人员用它来配置和管理网站内容。 将第一个 URL 粘贴到浏览器中，你应该会看到类似下图显示的页面： 3.6 wordpress 首页 这个页面中有些内容看起来很熟悉，在居中位置最上方，博客的标题叫 Learn Helm! ，这不仅是本书的主题，也是你在安装过程中设置的 wordpressBlogName 的值。 影响页面的另一个自定义值是 wordpressUsername。请注意，“Hello world！” 的作者是 weiwendi。这是提供给 wordpressUsername 的值，如果该值是其他用户名，则会显示不同的名称。 将第二个echo命令中的链接粘贴到浏览器中，访问控制台，页面内容如下： 3.7 admin pages 要登录管理控制台，需要输入安装时提供的 wordpressUsername 和 wordpressPassword 的值。这些值可以在 wordpress-values.yaml 文件中查看，也可以运行以下命令查看，这两条命令也是 WordPress 图表 notes 的内容： echo Username: weiwendi echo Password: $(kubectl get secret --namespace website wordpress -o jsonpath=\"{.data.wordpress-password}\" | base64 --decode) 通过身份验证后，将显示管理控制台仪表板，如下所示： 3.8 dashboard 图3-8 管理控制台仪表盘 更新 WordPress 版本 修改值或升级图表到最新版本，都需要更新 WordPress 版本实现。在本节中，我们通过修改 WordPress 副本数及资源请求的值，来演示版本更新。 修改值 通过 Helm 图表的值配置应用程序的实例副本数及资源请求大小是很常见的情况，下面的内容是 helm show values 命令的部分输出，这是当前运行的副本数： [aiops@aiops0113 ~]$ helm show values bitnami/wordpress |grep replicaCount replicaCount: 1 我们需要把 replicaCount 的值修改成 2。 需要修改的第二部分的值是 YAML 文件的 “WordPress container” resources 块的值，该块中 cpu 和 memory 的值如下： [aiops@aiops0113 ~]$ helm show values bitnami/wordpress |grep resources -C 4 ## WordPress containers' resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## @param resources.limits The resources limits for the WordPress container ## @param resources.requests [object] The requested resources for the WordPress container ## resources: limits: {} requests: memory: 512Mi cpu: 300m 值通过缩进进行逻辑分组，如 resources 部分的 requests 块，定义了 WordPress 应用程序在启动时， Kubernetes 为其分配的 memory 和 cpu 的值。我们将 requests.memory 的值修改为 256Mi (256 mebibytes)，将 requests.cpu 的修改为100m(100毫秒)。将这些修改添加到 wordpress-values.yaml 文件中，如下所示： resources: requests: memory: 256Mi cpu: 100m 修改完这两部分，新的 wordpress-values.yaml 内容如下： wordpressUsername: weiwendi wordpressPassword: Z5tOfyh wordpressEmail: weiwendi@aiops.red wordpressFirstName: AiOps wordpressBlogName: Learn Helm! service: type: NodePort replicaCount: 2 resources: requests: memory: 256Mi cpu: 100m 更新了 wordpress-values.yaml 文件的内容，在下一节中，我们使用 helm upgrade 命令升级 release。 执行 upgrade helm upgrade 命令的语法与 helm install 命令几乎相同，如下例所示： helm upgrade [RELEASE] [CHART] [flags] 执行 helm install 命令时，需要我们提供一个在该命名空间中不存在的 release 名称，而执行 helm upgrade 命令时，需要我们指定更新(已存在)的 release 名称。 与 helm install 命令相同，helm upgrade 也是通过 --values 标志传递值文件。使用我们修改后的值文件更新 WordPress 版本。 但如果直接使用我们修改后的 wordpress-values.yaml 文件更新 WordPress 会收到包含以下内容的错误： Error: UPGRADE FAILED: template: wordpress/templates/NOTES.txt:83:4: executing \"wordpress/templates/NOTES.txt\" at : error calling include: template: wordpress/charts/common/templates/_errors.tpl:21:48: executing \"common.errors.upgrade.passwords.empty\" at : error calling fail: PASSWORDS ERROR: You must provide your current passwords when upgrading the release. Note that even after reinstallation, old credentials may be needed as they may be kept in persistent volume claims. Further information can be obtained at https://docs.bitnami.com/general/how-to/troubleshoot-helm-chart-issues/#credential-errors-while-upgrading-chart-releases 'mariadb.auth.rootPassword' must not be empty, please add '--set mariadb.auth.rootPassword=$MARIADB_ROOT_PASSWORD' to the command. To get the current value: export MARIADB_ROOT_PASSWORD=$(kubectl get secret --namespace \"website\" wordpress-mariadb -o jsonpath=\"{.data.mariadb-root-password}\" | base64 --decode) 'mariadb.auth.password' must not be empty, please add '--set mariadb.auth.password=$MARIADB_PASSWORD' to the command. To get the current value: export MARIADB_PASSWORD=$(kubectl get secret --namespace \"website\" wordpress-mariadb -o jsonpath=\"{.data.mariadb-password}\" | base64 --decode) 图表要求在更新 release 时，必须提供数据库当前的密码，我们根据提示获取密码： [aiops@aiops0113 valuesFile]$ kubectl get secret --namespace \"website\" wordpress-mariadb -o jsonpath=\"{.data.mariadb-root-password}\" | base64 --decode [aiops@aiops0113 valuesFile]$ kubectl get secret --namespace \"website\" wordpress-mariadb -o jsonpath=\"{.data.mariadb-password}\" | base64 --decode 将 Mariadb 的两个密码也添加到 wordpress-values.yaml 文件中，最终内容如下： wordpressUsername: weiwendi wordpressPassword: Z5tOfyh wordpressEmail: weiwendi@aiops.red wordpressFirstName: AiOps wordpressBlogName: Learn Helm! service: type: NodePort mariadb: auth: rootPassword: szopyrAlqR # Mariadb root 账号密码 password: ePMtyJarxb # Mariadb 普通账号密码 replicaCount: 2 resources: requests: memory: 256Mi cpu: 100m 修改完 wordpress-values.yaml 文件后，执行 upgrade 命令： [aiops@aiops0113 valuesFile]$ helm upgrade wordpress bitnami/wordpress --values wordpress-values.yaml -n website --version 11.0.5 一旦命令执行完成，你会看到类似 3.4小节中 helm install 时的 Notes 输出。 使用 kubectl 命令查看 WordPress Pod 的信息： [aiops@aiops0113 valuesFile]$ kubectl get pods -n website 在 Kubernetes 中，当 Deployment 被修改时会创建新的 Pod。同样的行为也能在 Helm 中观察到。升级过程中添加的值更改了 WordPress 配置，因此，Helm 使用更新后的配置创建了新的 WordPress Pod。这些更改可以通过 helm get manifest 和 kubectl get deployment 命令来查看。 在下一小节中，我们将再执行几次升级，演示在升级过程中值的设置方式。 upgrade 时值的重用与重置 helm upgrade 命令有两个标志，用于操作值。 这两个标志是： --reuse-values：升级时，重用上一个版本的值。 --reset-values：升级时，将值重置为 chart 默认值。 如果执行 upgrade 时没有使用 --set 或 --values 提供值，则会默认添加 --reuse-values 标志。换句话说，如果在更新时没有提供值，将使用与之前版本相同的值： 重新执行升级命令，不指定任何参数： [aiops@aiops0113 valuesFile]$ helm upgrade wordpress bitnami/wordpress -n website --version 11.0.5 使用 helm get values 命令查看升级过程中使用的值： [aiops@aiops0113 valuesFile]$ helm get values wordpress -n website USER-SUPPLIED VALUES: mariadb: auth: password: ePMtyJarxb rootPassword: szopyrAlqR primary: persistence: enabled: false persistence: enabled: false replicaCount: 2 resources: requests: cpu: 100m memory: 256Mi service: type: NodePort wordpressBlogName: Learn Helm! wordpressEmail: weiwendi@aiops.red wordpressFirstName: AiOps wordpressPassword: Z5tOfyh wordpressUsername: weiwendi 显示的值与前一个版本相同。 当升级过程中从命令行提供值时，可以观察到不同的结果。如果值是通过 --set 或 --values 传递的，那么所有没有提供的图表值都将重置为默认值。 使用 --set 提供单个值再次执行升级。当然，别忘记获取并传递 Mariadb 账号密码，不然更新会执行失败： [aiops@aiops0113 valuesFile]$ export MARIADB_ROOT_PASSWORD=$(kubectl get secret --namespace \"website\" wordpress-mariadb -o jsonpath=\"{.data.mariadb-root-password}\" | base64 --decode) [aiops@aiops0113 valuesFile]$ export MARIADB_PASSWORD=$(kubectl get secret --namespace \"website\" wordpress-mariadb -o jsonpath=\"{.data.mariadb-password}\" | base64 --decode) [aiops@aiops0113 valuesFile]$ export WORDPRESS_PASSWORD=$(kubectl get secret --namespace \"website\" wordpress -o jsonpath=\"{.data.wordpress-password}\" | base64 --decode) [aiops@aiops0113 valuesFile]$ helm upgrade wordpress bitnami/wordpress --set replicaCount=1 --set mariadb.auth.rootPassword=$MARIADB_ROOT_PASSWORD --set mariadb.auth.password=$MARIADB_PASSWORD --set wordpressPassword=$WORDPRESS_PASSWORD -n website --version 11.0.5 升级完成后，执行 helm get values 命令： [aiops@aiops0113 valuesFile]$ helm get values wordpress -n website 在升级过程中提供了值时，Helm 会自动使用 --reset-values 标志，把除了提供的值之外的其它值置为默认值。 用户可以使用 --reset-values 或 --reuse-values，在升级过程中显式地定义值的行为。如果在升级时希望除了从命令行覆盖的值之外，其他值都被重置为默认值，可以使用 --reset-values；如果希望重用 release 上一版本的值，同时又想重新设置部分值，可以使用 --reuse-values。把值保存在值文件中，能有效简化值的管理，并在更新时以声明性的方式设置值。 按照本章的操作，现在 release 应该有了 4 个版本。第 4 个修订版并不符合我们的期望，因为它只配置了 replicaCount(除了图表要求的必须传入的密码相关值)，其它都被置为了默认值。在下一节中，我会介绍如何将 wordpress 回退到包含所需值的 release 版本。 回退 WordPress 版本 有时后退是为了更好的前进。在某些情况下，回退到应用程序的前一个版本非常有必要。helm rollback 命令就为满足这种场景。接下来我们把 WordPress 版本回退到之前的状态。 查看 WordPress 历史版本 Helm 的每个 release 都有历史发布记录，用于统计不同版本中使用的 values、Kubernetes 资源、图表版本。在安装、升级或回退图表时，会创建新的修订记录。默认情况下，修订数据存储在 Kubernetes 的 Secret 中。 使用 kubectl 命令查看 website 命名空间中的 Secret： [aiops@aiops0113 valuesFile]$ kubectl get secret -n website 这条命令会返回 website 命名空间中所有的 Secret，你至少会看到以下 4 个 Secret： sh.helm.release.v1.wordpress.v1 sh.helm.release.v1.wordpress.v2 sh.helm.release.v1.wordpress.v3 sh.helm.release.v1.wordpress.v4 这些 Secret 都对应一个 release 的历史修订条目，可以通过 helm history 命令查看： [aiops@aiops0113 valuesFile]$ helm history wordpress -n website 该命令以表格的方式输出每个 release 的条目，以下内容只是截取了部分字段的信息： REVISION STATUS DESCRIPTION 1 superseded Install complete 2 superseded Upgrade complete 3 superseded Upgrade complete 4 deployed Upgrade complete 完整的列表字段如下： REVISION：修订版本号，1 为最初版(install 时的版本)，更新或回退一次则加 1 UPDATED：更新日期及时间 STATUS：状态 CHART：图表版本 APP VERSION：图表部署的应用程序版本 DESCRIPTION：描述信息 在这个输出中，STATUS 为 “superseded” 表示已升级的版本，为 “deployed” 表示当前运行的版本。STATUS 的其他类型还包括 pending、pending_upgrade(这两个表示安装、升级正在进行中)、failed(安装或升级失败，DESCRIPTION 会显示详细的失败信息)、unknown(状态未知，很难遇到这种状态)。 前面介绍过，helm get 命令通过 --revision 标志可以操作指定的修订版本。在执行回退前，我们使用此命令确认指定的修订版本是否包含了我们需要的值。你可能还记得，修订号为 4 的版本仅包含了 replicacount 值，而修订号为 3 的版本包含了我们需要的所有值，可以使用以下命令验证： [aiops@aiops0113 valuesFile]$ helm get values wordpress --revision 3 -n website 执行回退 helm rollback 是 Helm 提供的回退命令，语法如下： helm rollback [REVISION] [flags] 在执行回退时，用户需要提供回退的 release 名称、回退到的修订版本号。执行以下命令可以将 wordpress 回退到修订号为 3 的版本： [aiops@aiops0113 valuesFile]$ helm rollback wordpress 3 -n website Rollback was a success! Happy Helming! 执行 helm history 命令，可以看到 release 回退到了修订号为 3 的版本： [aiops@aiops0113 valuesFile]$ helm history wordpress -n website 从输出内容可以看到，release 记录中多了一条修订号为 5 的记录，STATUS 是 deployed，DESCRIPTION 中的内容是 “Rollback to 3”。当回退应用程序时，会添加一条新的 release 历史修订记录。不要将此与升级混淆，最高修订版本号仅表示当前部署的版本，从 DESCRIPTION 中才能确定这条记录是由升级还是回退创建的。 再次查看当前版本使用的值，确认回退的版本是我们希望的： [aiops@aiops0113 valuesFile]$ helm get values wordpress -n website 在 rollback 子命令中，我们没有显式指定图表版本和它的值，这是因为 rollback 子命令无须接收这些信息；它的目的是将图表回退到之前的修订版，并使用之前修订版的值和图表版本。rollback 子命令不是 Helm 日常实践的一部分，它仅适用于在当前应用程序的状态不稳定、且必须恢复到之前稳定版的紧急情况。 在成功演示了版本回退功能后，本章的内容也即将结束。最后一步是使用 uninstall 子命令从 Kubernetes 集群中删除 WordPress 应用程序。我将在下一小节介绍 uninstall 子命令。 卸载 WordPress 卸载 Helm release，意味着要删除它管理的 Kubernetes 资源。默认情况下，uninstall 命令会同时删除 release 的历史记录，这是合理操作，如果想保留这些记录，也可以使用 --keep-history 标志。 uninstall 的语法非常简单： helm uninstall RELEASE_NAME [...] [flags] 执行 helm uninstall 命令卸载 WordPress 版本： [aiops@aiops0113 valuesFile]$ helm uninstall wordpress -n website release 'wordpress' uninstalled 通过 list 子命令查看 website 命名空间中的 release，wordpress 已经不存在了： [aiops@aiops0113 valuesFile]$ helm list -n website 输出一个空表。也可以使用 kubectl 获取 WordPress 的 Deployment，来确认 wordpress 已经不存在了： [aiops@aiops0113 valuesFile]$ kubectl get deployments -l app.kubernetes.io/instance=wordpress --namespace website No resources found in website namespace. 正如预期的那样，已没有 WordPress 了。 [aiops@aiops0113 valuesFile]$ kubectl get pvc -n website 然而，你会看到在 website 命名空间中仍然有两个 PersistentVolumeClaim 可用。 这个 PersistentVolumeClaim 资源没有被删除，因为它是由 StatefulSet 在后台创建的。在 Kubernetes 中，如果 StatefulSet 被删除，那么由 StatefulSet 创建的 PersistentVolumeClaim 资源不会被自动删除。在 Helm 卸载过程中，StatefulSet 被删除，但相关的 PersistentVolumeClaim 没有被删除。这是我们所期望的。可以通过以下命令手动删除 PersistentVolumeClaim 资源： [aiops@aiops0113 valuesFile]$ kubectl delete pvc -l app.kubernetes.io/instance=wordpress -n website 现在我们已经安装和卸载了 Wordpress，让我们清理你的 Kubernetes 环境，这样我们就有了一个干净的设置，我们将在本书后面的章节中进行练习。 我们已经完成了 WordPress 的安装和卸载，本章的所有试验都已经完成，接下来我们清理 Kubernetes 环境，打扫战场。 清扫战场 我们所有操作都是在 website namespace 中完成的，要清理 Kubernetes 环境，直接删除该 namespace 即可： [aiops@aiops0113 valuesFile]$ kubectl delete ns website namespace \"website\" deleted 在删除 wordpress 命名空间后，也可以关闭 Minikube 虚拟机： $ minikube stop VM 被关闭，但会保留 Minikube 状态，以便在下一个练习中快速启动。 Minikube 的一个小 Bug 本书写作时，最新版本也是本书演示示例所用的版本：minikube version: v1.20.0，在使用了 --image-mirror-country=cn 标志时，会从错误的镜像地址下载 storage-provisioner 镜像，如果你够仔细的话，会留意到执行完 minikube start 命令后，输出内容包含以下信息： ... - Using image registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5 (global image repository) * Enabled addons: storage-provisioner, default-storageclass registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5 这个地址是错误的，在阿里云镜像仓库中，正确的镜像地址是 registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5，执行以下命令可以验证这一点： [aiops@aiops0113 valuesFile]$ kubectl get pod -n kube-system 从输出中查看 storage-provisioner Pod 的状态为 ImagePullBackOff。编辑 storage-provisioner Pod 的 YAML 文件： [aiops@aiops0113 valuesFile]$ kubectl edit pod/storage-provisioner -n kube-system 把 image 的值修改为正确的地址，保存退出即可。 关于这个问题，也有人提了 PR：https://github.com/kubernetes/minikube/pull/10770。 By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-18 18:27:18 "},"helm/chartsjinjie.html":{"url":"helm/chartsjinjie.html","title":"Helm 图表进阶","keywords":"","body":"Helm 图表进阶 在上一章中，我从使用者角度讨论了 Helm 的用法，并将其作为包管理器在 Kubernetes 集群中安装了 WordPress 应用程序。安装过程非常简单，不需要任何 Kubernetes 专业知识，也无需对安装的应用程序有所了解，因为所需资源和逻辑都封装在了 Helm 图表中。我们只需要了解图表中的值，以便自定义配置一些选项。 知其然，只能勉强成为合格的执行者；知其所以然才能有更好的发展。在这一章中，我会深入介绍图表的工作原理，以及如何创建图表。 准备好与上一章相同的 Kubernetes 集群和 Helm 环境，我们开始一探图表究竟。 YAML 语法介绍 YAML(YAML Ain't Markup Language)一种易于人类阅读的文件格式，是配置 Kubernetes 资源最常用的文件格式，也是 Helm 图表中大多数文件的格式。 YAML 遵循键值对(key-value)格式声明配置，我们来看一下 YAML 的键值对结构。 定义键值对 下面是 YAML 最基本的键值对示例： blog: \"cloudnative.aiops.red\" 在这个示例中，定义了 blog 键，它的值是 cloudnative.aiops.red。在 YAML 中键和值之前用冒号 : 分隔。冒号左边的字符是键，冒号右边的字符是值。 在 YAML 格式中，空格非常重要。下面是一个错误的 YAML 键值对示例： blog:\"cloudnative.aiops.red\" 冒号和 cloudnative.aiops.red 字符串之间缺少空格，会导致解析错误。YAML 格式要求冒号和值之间必须要有空格。 YAML 也支持嵌套元素或块定义更复杂的键值对，示例： resources: limits: cpu: 100m memory: 512Mi 示例中的 resources 对象映射了两个键值对，如下表： Key Value resources.limits.cpu 100m resources.limits.memory 512Mi 键是通过 YAML 块下的缩进来确定的，每个缩进都向键名添加一个点 . 分隔符。当 YAML 块中不再有任何缩进的时候，就是值。按照使用规范，YAML 的缩进使用两个空格表示，但用户可以使用任意数量的空格，只要在整个文件中空格数是一致的即可。 YAML 不支持制表符(Tab 键)，使用制表符将会导致解析错误。 值类型 YAML 文件中的值可以是不同类型，常见的类型是字符串，它是文本值。可以通过引号将值引起来，声明字符串类型(如第一个示例中的 \"cloudnative.aiops.red\")。但这不是必须的，只要值中包含字母或特殊字符，不论是否有引号，就会被视为字符串。可以使用管道符(|)设置多行字符串，示例： configuration: | server.port=8443 logging.file.path=/var/log 值也可以是整数。当值是没有用引号括起来的数字时，这个值就是整数类型。这个 YAML 示例声明了一个整数类型的值： replicas: 1 将其与下面的 YAML 进行比较，后者为 replicas 分配给一个字符串类型的值： replicas: '1' 布尔值也经常被使用，可以用 true 或 false 声明： ingress: enable: true 这个 YAML 示例设置了 ingress.enable 为 true 的布尔值类型。其他布尔值包括：yes、no、on、off、y、n、Y、N。 值也可以是更复杂的类型，比如列表(list)。YAML 中的列表项由 - (减号)标识。下面是一个值为列表类型的示例： servicePorts: - 8080 - 8443 servicePorts 的值是一个整数列表 8080 和 8443。这种语法也可以用在对象上： deployment: env: - name: MY_VAR value: MY_VALUE - name: SERVICE_NAME value: MY_SERVICE 在本例中，env 是一个包含 name 和 value 字段的对象列表。在配置 Kubernetes 和 Helm 的 YAML 文件时，我们会经常用到列表，所以要记住这种格式的写法。 YAML 便于人类阅读的特性，使它在 Kubernetes 和 Helm 的配置中比较流行，但也可以使用 JSON 格式替代 YAML。JSON 也遵循键值对格式，类似 YAML，主要的区别在于 YAML 依赖空格缩进配置键值对，而 JSON 依赖于大括号和方括号。下面是将 YAML 格式转换为 JSON 格式的示例： { 'deployment': { 'env': [ { 'name': 'MY_VAR', 'value': 'MY_VALUE' }, { 'name': 'SERVICE_NAME', 'value': 'MY_SERVICE' } ] } } JSON 中的所有键都用引号括起来，放在冒号前面： 使用花括号({)表示块，这与 YAML 中以缩进表示块的方式类似。 使用方括号([)表示列表，这与 YAML 中以减号表示列表的方式类似。 YAML 和 JSON 格式还有很多结构，但本文介绍的知识点已足够我们在 Helm 中使用了。 在下一节中，我将讨论 Helm 图表的文件结构，你可能会注意到它包含了 YAML 和 JSON 文件。 Helm 图表结构 图表作为 Kubernetes 的包，必须要遵循一定的文件结构。 $ ls my-chart/ chart 文件和目录... 把 Helm 图表的顶级目录作为 Helm 图表的名称，这是最佳命名规则。虽然 Helm 不严格要求这么做，但这样命名能增加图表的辨识度。在上个示例中，my-chart 是图表的顶级目录，也是这个图表的名称。 顶级目录里包含了组成图表的文件和目录。下表列出了可能包括的文件和目录： 文件/目录 定义 是否必须 Chart.yaml 该文件中包含了图表的元数据 是 templates/ 存放图表的资源模板文件，这些文件是 YAML 格式 是(除非依赖项在 Chart.yaml 文件中声明) templates/NOTES.txt 图表的说明文件。安装、升级或执行 notes 命令会输出此文件内容 否 values.yaml 图表的默认值文件 否，但作为最佳实践，最好包含此文件 .helmignore 指定忽略 Helm 包中哪些文件和目录 否 charts/ 存放图表依赖的其他图表 无需明确提供，Helm 的依赖管理系统会自动创建此目录 Chart.lock 保存先前版本的依赖 无需明确提供，Helm 的依赖管理系统会自动创建此文件 crds/ 存储自定义(CRD)资源的目录。YAML 格式。这些资源会在 templates/ 中的资源之前安装。 否 README.md 图表安装和使用说明的自述文件 否，但作为最佳实践，最好包含此文件 LICENSE 图表 license 文件 否 values.schema.json JSON 格式的图表值文件 否 我会逐个介绍这些文件或目录，以便大家了解如何创建 Helm 图表。首先从 templates/ 目录开始，它是动态生成 Kubernetes 资源的关键。 图表模板 Helm 图表之所以能够创建和管理在 Kubernetes 集群中部署应用程序所需的资源，主要是通过模板(templates)实现的。用户将自定义的值传递给模板，就可以生成所需的 Kubernetes 资源。在本节中，我会详细介绍模板和值是如何结合的。 Helm 图表中必须包含 templates/ 目录，存放生成 Kubernetes 资源的 YAML 模板文件，文件列表大致如下： templates/ configmap.yaml deployment.yaml service.yaml configmap.yaml 文件的内容大致如下： {% raw %} apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }} data: configuration.txt: |- {{ .Values.configurationData }} {% endraw %} 你可能觉得这段代码不是有效的 YAML 文件，这是因为 configmap.yaml 文件仅是一个 Helm 模板文件，它可以根据特定的值修改资源配置，然后生成有效的 YAML 资源文件。两个花括号 {{ ... }} 中的内容是 Go 语言模板的输入文本，会在图表安装或更新期间被替换成对应的值。 Go 模板是什么？如何使用它生成 Kubernetes 资源文件呢？带着这两个问题，我们进入下一小节。 Go 模板 Go 是谷歌开发的编程语言，Kubernetes、Helm、Istio 以及云原生的许多生态系统都是使用 Go 开发的，对 Go 感兴趣的朋友，欢迎阅读我的《Go 入门与实战》系列文章。模板是 Go 语言的核心组件，可以生成不同格式的文件。对于 Helm，Go 模板用于在 Helm 图表的 templates/ 目录下生成 Kubernetes YAML 资源文件。 Go 模板以两个左花括号 {{ 开始、两个右花括号 }} 结束，这些花括号虽然在 YAML 文件中可见，但在图表安装或升级的时候，会被自动删除。 在第五章，我会详细介绍 Go 模板的使用，并带你从零到一完成图表开发。 使用值和内置对象参数化字段 Helm 图表中包含一个名为 values.yaml 的文件，提供了图表所需的默认值。这些值会被 Go 模板引用，并交给 Helm 处理，动态生成 Kubernetes 资源。 values.yaml 文件的格式大致如下： ## weChatPublicId is my WeChat public number weChatPublicId: sretech ## weChatPublicName is my WeChat public name weChatPublicName: 魏文弟 以井号(#)开头的行是注释行，对键值对进行说明，在执行时会被忽略。我们应该为每个值都提供注释，以便用户理解如何使用这些值。文件中的其他行表示键值对。 以 .Values 开头的 Go 模板引用 values.yaml 文件中定义的值，或者在安装、升级过程中，使用 --set、--values 参数传入的值。 下面的示例是模板文件中的内容： env: - name: WECHATPUBLIC_ID value: {% raw %}{{ .Values.weChatPublicId }}{% endraw %} - name: WECHATPUBLIC_NAME values: {% raw %}{{ .Values.weChatPublicName }}{% endraw %} 模板处理之后的 YAML 资源如下： env: - name: WECHATPUBLIC_ID value: sretech - name: WECHATPUBLIC_NAME values: 魏文弟 .Values 是一个可用于参数化的内置对象(Built-in Objects)，用来引用图表值。Helm 的所有内置对象可以在 Helm 文档页面(https://helm.sh/docs/chart_template_guide/builtin_objects/)查看。下表是我整理的常用内置对象： 内置对象 定义 .Release.Name release 名称 .Release.Namespace release 指定的命名空间 .Release.Revision 安装或升级时的修订号。安装时是 1，每次升级或回退都会自增。 .Values 用于引用 values.yaml 或用户提供的值 .Chart.Name, .Chart.Version, .Chart.AppVersion 用于引用 Chart.yaml 文件，引用字段包括 Chart.$Field .Files.Get 获取图表目录中的文件 .Files.AsSecrets 以Base 64编码字符串的形式返回文件内容 .Files.AsConfig 使用 YAML 格式返回文件内容 .Capabilities.APIVersions 返回 kubernetes 集群中可用的 API 版本的列表 .Template.Name 返回此对象使用的模板的相对文件路径 对象前面的点(.)表示对象作用域。点号后跟对象名，表示将作用域限制为该对象。例如 .Values 仅作用于图表的值；.Release 作用域仅使 Release 对象下的字段可见；.scope 表示全局范围，使所有对象可见。 VALUES.SCHEMA.JSON 在学习值和参数化前，我们先了解下 values.schema.json 文件，它并不是图表目录中必须存在的文件。values.schema.json 强制在值文件中执行特定的模式，这种模式可用于在图表安装或更新期间验证用户所提供的值。 values.schema.json 文件的内容片段： { '$schema': 'https://json-schema.org/draft-07/schema#', 'properties': { 'replicas': { 'description': 'number of application instances to deploy', 'minimum': 0 'type' 'integer' }, . . . 'title': 'values', 'type': 'object' } 有了 values.schema.json 模式文件，replicas 的值最小只能设置为 0，这就完成了对值的限制。该文件确保了用户只能提供图表模板支持的值。 Go 模板不仅支持图表开发人员参数化 Helm 图表，也支持开发人员在 YAML 文件中使用条件逻辑。接下来我们介绍这个特性。 模板中的流程控制 模板的参数化功能允许使用值替换模板中的字段，而流程控制语句，进一步增加了配置灵活性。以下是在模板中定义流程控制的几个关键词： 动作 定义 if/else 有条件地包含或排除某些资源 with 用于修改值引用的范围 range 用于循环遍历列表中的值 在图表模板的开发过程中，有时可能需要包含或排除某些 Kubernetes 资源或资源的某些部分。if…else 动作可满足此需求。以下是 Deployment 模板中包含条件块的代码内容： readinessProbe: {{- if .Values.probeType.httpGet }} httpGet: path: /healthz port: 8080 scheme: HTTP {{- else }} tcpSocket: port: 8080 {{- end }} initialDelaySeconds: 30 periodSeconds: 10 if 语句可以根据条件判断设置 readinessProbe，如果 probeType.httpGet 的值为 true 或非 null，httpGet readinessProbe 将被模板化。否则，readinessProbe 将为 tcpSocket 类型。花括号中的减号表示在处理后应该删除空格。左花括号后的减号可以删除花括号前面的空格，右花括号前面的减号可以删除花括号后面的空格。 图表开发人员还可以使用 with 动作修改值的作用域。当引用深度嵌套的值时，非常适合使用 with。通过减少引用深度嵌套值所需的字符数量，提高模板文件的可读性和可维护性。 下面的代码是值文件的一部分，包含了深度嵌套的值： application: resources: limits: cpu: 100m memory: 512Mi 如果没有 with 动作，这些值在模板文件中被引用的方式如下： cpu: {% raw %}{{ .Values.application.resources.limits.cpu }}{% endraw %} memory: {% raw %}{{ .Values.application.resources.limits.memory }}{% endraw %} with 动作允许开发人员修改这些值的作用域，使用简短的语法引用它们： {% raw %}{{- with .Values.application.resources.limits }}{% endraw %} cpu: {% raw %}{{ .cpu }}{% endraw %} memory: {% raw %}{{ .memory }}{% endraw %} {{- end }} 开发人员还可以使用 range 动作执行重复操作，遍历循环列表类型的值。假设某个图表包含以下值： servicePorts: - name: http port: 8080 - name: https port: 8443 - name: monitor port: 8881 上面的代码提供了一个 servicePort 列表，它可以被循环使用，如下所示： spec: ports: {{- range .Values.servicePorts }} - name: {% raw %}{{ - name }}{% endraw %} port: {% raw %}{{ .port }}{% endraw %} {{- end }} with 和 range 的作用域是所提供的对象。在 range 示例中，range 作用于 .Values.servicePorts 对象，点 .号将作用域限制在此对象下定义的值。要引用所有值或是设置全局作用域，开发人员应该在引用前加上美元 $ 符号，示例： {{- range .Values.servicePorts }} - name: {% raw %}{{ $.Release.Name }}{% endraw %}-{% raw %}{{ .name }}{% endraw %} port: {% raw %}{{ .port }}{% endraw %} {{- end }} 除了图表值外，还可以为模板传递变量。 模板变量 图表模板也支持变量，定义如下： {% raw %}{{ $weChatPublicId := 'sretech' }}{% endraw %} 设置 weChatPublicId 变量的值为 sretech(本人公众号 ID，欢迎关注☺) 。变量也可以赋值给对象，比如图表值： {% raw %}{{ $myvar := .Values.greeting }}{% endraw %} 在模板中引用变量的方式如下： data: greeting.txt: | {% raw %}{{ $myvar }}{% endraw %} 使用变量的最佳场景之一是在 range 块中，变量被设置为捕获列表迭代的索引和值： data: greetings.txt: | {{- range $index, $value := .Values.greetings }} Greeting {% raw %}{{ $index }}{% endraw %}: {% raw %}{{ $value }}{% endraw %} {{- end }} 结果如下： data: greetings.txt: | Greeting 0: Hello Greeting 1: Hola Greeting 2: Hallo 变量还可以简化对映射的迭代处理，如下所示： data: greetings.txt: | {{- range $key, $val := .Values.greetings }} Greeting in {% raw %}{{ $key }}{% endraw %}: {% raw %}{{ $val }}{% endraw %} {{- end }} 可能的结果如下： data: greetings.txt: | Greeting in English: Hello Greeting in Spanish: Hola Greeting in German: Hallo 也可以使用变量引用当前作用域之外的值。 考虑下面的 with 代码块： {{- with .Values.application.configuration }} My application is called {% raw %}{{ .Release.Name }}{% endraw %} {{- end }} 在这个模板中，.Release.Name 是不会被处理的，因为它不在 .Values.application.configuration 范围内。解决这个问题的一种方式是在 with 块上方，设置一个值为 .Release.Name 的变量： {% raw %}{{ $appName := .Release.Name }}{% endraw %} {{- with .Values.application.configuration }} My application is called {{ $appName }} {{- end }} 当然，这种方式虽然能解决问题，但不是最好的。可以使用 $ 符引用全局范围的资源，代码行数会更少、更易阅读。 流程控制和变量为动态生成资源提供了有力的支持，除此之外，开发人员还可以使用函数和管道来协助资源的渲染。 使用函数和管道处理复杂数据 Go 提供了函数和管道的概念，以支持在模板中处理复杂的数据。 Go 模板中的函数和其他结构或编程语言中的函数类似，旨在处理某些输入，并根据输入提供输出的逻辑。 Go 模板通过以下方法调用函数： functionName arg1 arg2 . . . indent 是常用的 Go 模板函数之一，该函数用于缩进字符串，以确保字符串被正确格式化，因为 YAML 对空格很敏感。indent 函数接收两个参数，第一个参数是要缩进的空格数，第二个参数要缩进的字符串。下面是 indent 函数的模板示例： data: application-config: |- {% raw %}{{ indent 4 .Values.config }}{% endraw %} 这个示例将 config 值中的字符串缩进 4 个空格，以确保字符串在 application-config YAML 键下正确缩进。 Helm 提供的另一种构造是管道。管道是从 UNIX 借来的概念：一个命令的输出被作为另一个命令的输入： cat file.txt | grep helm 上面的例子是一个 UNIX 管道。管道(|)的左边是第一个命令，右边是第二个命令。 第一个命令 cat file.txt 输出 file.txt 文件的内容，并将其作为第二个命令 grep helm 的输入，第二个命令用来在第一个命令的输出中过滤出包含 helm 单词的行。 Go 管道以类似的方式工作，再次以 indent 函数来演示： data: application-config: |- {% raw %}{{ .Values.config | indent 4 }}{% endraw %} 同样将 config 的值缩进 4 个空格，管道适合将多个命令连接在一起执行，阅读起来也连续自然。 有许多 Go 模板函数可以在 Helm 图表中使用。这些函数可以在 https://golang.org/pkg/text/template/#hdr-Functions 的 Go 文档和 http://masterminds.github.io/sprig/ 的 Sprig 模板库中找到。以下是一些常见的 Go 模板函数，你可能会在图表开发中用到： date：格式化日期 default：设置默认值 fail：模板渲染失败 include：执行 Go 模板并返回结果 nindent：将文本缩进指定数量的空格，并在缩进之前添加一个新行 indent：将文本缩进指定数量的空格 now：显示当前日期/时间 quote：用引号将字符串括起来 required：要求用户输入 splitList：将字符串拆分为字符串列表 toYaml：将字符串转换为 YAML 格式 Go 模板语言还包含布尔操作符，可以在 if 动作中使用它们来进一步控制 YAML 资源的生成： and or not eq（相等） ne（不等） lt（小于） le（小于或等于） gt（大于） ge（大于或等于） 除了生成 Kubernetes 资源，Go 模板还可以重用具有相同模板的 YAML 资源。这是通过命名模板实现的。 使用命名模板实现代码重用 在创建模板文件时，可能存在重复的 YAML 块。我们以标签资源作为示例： labels: 'app.kubernetes.io/instance': {% raw %}{{ .Release.Name }}{% endraw %} 'app.kubernetes.io/managed-by': {% raw %}{{ .Release.Service }}{% endraw %} 标签可以附加到任意资源对象上，一个资源对象上也可以添加任意数量的标签。将资源对象与标签捆绑，可以实现多维度的资源分组管理。如果图表包含许多不同的 Kubernetes 资源，那么在每个文件中定义所需的标签就会很麻烦，特别是在需要修改或者向资源添加新标签的时候。 Helm 提供了命名模板的概念，允许图表开发人员创建可重用的模板，从而减少重复的配置块。命名模板在 templates/ 目录下定义，这些文件以下划线开头，以 .tpl 作为文件扩展名。许多图表包含 _helpers.tpl 的命名模板文件，尽管该文件不需要被命名为 helpers。 可以使用 define 动作在 tpl 文件中创建命名模板。下面代码创建了一个封装资源标签的命名模板： {% raw %}{{- define 'mychart.labels' }}{% endraw %} labels: 'app.kubernetes.io/instance': {% raw %}{{ .Release.Name }}{% endraw %} 'app.kubernetes.io/managed-by': {% raw %}{{ .Release.Service }}{% endraw %} {{- end }} define 动作以模板名作为参数，在上面的示例中，模板名就是 mychart.labels。模板名的命名约定是 $CHART_NAME.$TEMPLATE_NAME，其中 $CHART_NAME 是 Helm 图表的名称，$TEMPLATE_NAME 是一个简短的描述性名称，用来描述模板的用途。 mychart.labels 这个名称说明了模板来自于 mychart 图表，并且它会生成所需的标签资源。 要在 Kubernetes YAML 模板中使用命名模板，需要 include 函数，语法如下： include [TEMPLATE_NAME] [SCOPE] TEMPLATE_NAME 参数是命名模板的名称，SCOPE 参数定义了应用的值和内置对象的范围。大多数情况下，这个参数是一个点(.)，表示当前的顶级作用域，如果指定的模板引用了当前作用域之外的值，则应该使用 $ 符号。 下面的示例演示如何使用 include 函数处理命名模板： metadata: name: {% raw %}{{ .Release.Name }}{% endraw %} {% raw %}{{- include 'mychart.labels' . | indent 2 }}{% endraw %} 这个示例中，首先设置了模板名称为 release 名称。接着使用 include 函数处理标签(labels)，并使用管道将标签行缩进两个空格。当处理完成后，一个名为 template-demonstration 的 release 中的资源可能会如下所示： metadata: name: template-demonstration labels: 'app.kubernetes.io/instance': template-demonstration 'app.kubernetes.io/managed-by': Helm Helm 还提供了 template 动作，可以扩展命名模板。template 的用法与 include 相同，但不能在管道中使用。由于这个原因，开发人员应该尽量使用 include 而不是 template，因为 include 不仅具有与 template 相同的特性，还提供了管道的功能。 在下一节中，我们将讨论如何使用命名模板来减少多个不同图表中重复的模板块。 图表库 Helm 图表分为应用程序(application)和库(library)两种类型，可以通过 Chart.yaml 文件中的 type 字段设置。应用程序类型的图表用于在 Kubernetes 集群中部署应用程序，也是常见的、默认设置的图表类型。图表也可以定义为库类型，这种类型的图表不能部署应用程序，而是提供可跨多个图表使用的命名模板。我们依然使用上一个小结的标签示例，开发人员可能维护成百上千的图表，每个图表都具有相同的资源标签，单独维护每个图表的 _helpers.tpl 文件就变得复杂且多余了。在这种情况下，图表开发人员可以声明一个库类型的图表，用来提供生成资源标签的命名模板。 虽然 Helm 常用来创建 Kubernetes 的原生资源，但也可以用它创建自定义资源(CRs)，我将在下一节详细介绍。 CR 模板 CR 用于创建非原生的 Kubernetes API 资源，扩展 Kubernetes 提供的功能。CR 可以使用 Helm 模板创建，就像创建原生的 Kubernetes 资源一样，但创建前必须有对应的 Custom Resource Definition (CRD)，用来定义 CR。 如果在创建 CR 之前没有 CRD，CR 将会创建失败。 在 Helm 图表中创建 crds/ 目录，存放 CRD。crds/ 目录示例如下： crds/ my-custom-resource-crd.yaml my-custom-resource-crd.yaml 文件的内容大致如下： apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: my-custom-resources.learnhelm.io spec: group: cloudnative.aiops.red names: kind: MyCustomResource listKind: MyCustomResourceList plural: MyCustomResources singular: MyCustomResource scope: Namespaced version: v1 templates/ 目录中可以包含 MyCustomResource 资源的实例。 templates/ my-custom-resource.yaml 这样的结构确保了在安装自定义资源(CR)前已经定义了 MyCustomResource CRD。 创建 CRD 需要管理员权限，如果你不是管理员，需要让管理员帮你创建好 CRD。如果是这样， crds/ 目录就不需要包含在图表中了，因为 CRD 在集群中已经存在了。 到现在为止，我已经详细介绍了 Helm 模板，总而言之，它是图表的大脑，用于生成 Kubernetes 资源。 接下来，我会继续介绍 Helm 图表的其它基础知识，与模板同样重要的 Chart.yaml 文件。 图表定义 Chart.yaml 文件又被称为图表定义，用于配置图表元数据。该文件在图表中是必须存在的，否则会报错： Error: validation: chart.metadata is required 在第三章中，我们执行 helm show chart 命令查看了 Bitnami's WordPress 图表的定义，再次执行此命令，重新查看图表的定义： [aiops@aiops0113 ~]$ helm show chart bitnami/wordpress --version 11.0.5 annotations: category: CMS apiVersion: v2 appVersion: 5.7.2 dependencies: - condition: mariadb.enabled name: mariadb repository: https://charts.bitnami.com/bitnami version: 9.x.x - condition: memcached.enabled name: memcached repository: https://charts.bitnami.com/bitnami version: 5.x.x - name: common repository: https://charts.bitnami.com/bitnami tags: - bitnami-common version: 1.x.x description: Web publishing platform for building blogs and websites. home: https://github.com/bitnami/charts/tree/master/bitnami/wordpress icon: https://bitnami.com/assets/stacks/wordpress/img/wordpress-stack-220x234.png keywords: - application - blog - cms - http - php - web - wordpress maintainers: - email: containers@bitnami.com name: Bitnami name: wordpress sources: - https://github.com/bitnami/bitnami-docker-wordpress - https://wordpress.org/ version: 11.0. 从输出中可以看到，图表定义或者说 Chart.yaml 文件，包含许多字段。有些字段是必要的，而大部分字段是可选的，只需在使用时提供。 我们对 Chart.yaml 文件有了基本的了解，在下一节中我们讨论必要字段。 必要字段 Chart.yaml 文件必须包含以下字段，这些字段都是重要的图表元数据： 字段 描述 apiVersion 图表 API 版本 name 图表名 version 图表版本，示例中的版本是 11.0.5 版本 我们看看这些字段都代表了什么： apiVersion 字段有两个值可选： v1 v2 aipVersion v1 版本是 Helm3 发布前使用的 apiVersion 版本。如果设置成 v1，意味着图表将遵循老的图表结构，老结构支持 requirement.yaml 文件，但不支持 type 字段。虽然 Helm 3 向后兼容 apiVersion v1，但还是建议直接使用 v2 版本，以免使用了被弃用的特性。 name 字段定义 Helm 图表的名称，该值应该与 Helm 图表的顶级目录名相同。helm search 命令和 helm list 命令的搜索结果包含了 Helm 图表的名称。这个字段的值应该简单且具有描述性，用一个简单的名字描述图表安装的应用程序，比如 wordpress 或 redis-cluster。减号是连接名称中不同单词的常用方式。有时，名称也会被写成一个单词，如 rediscluster。 version 字段定义 Helm 图表的版本，版本必须遵循 Semantic Versioning (SemVer) 2.0.0 格式。SemVer 格式为 Major.Minor.Patch(主版本号.次版本号.补丁版本号)。当版本改动较大，不向后兼容时，应该修改主版本号；当发布向后兼容的功能时，应该修改次版本号；当修复 bug 时，应该修改补丁版本号。次版本增大时，需将补丁版本号置为 0；主版本增大时，次版本号和补丁版本号都需置为 0。图表开发人员在更改版本号时要注意这些规范，因为它们代表了新版本是何种程度的更新。 虽然这三个字段在 Chart.yaml 文件中是必要字段，但还有很多定义图表元数据的可选字段： 字段 描述 appVersion 图表部署的应用程序版本。 dependencies 图表依赖的其他图表列表。 deprecated 图表是否被弃用。 description 图表的简短描述。 home 项目主页的 URL。 icon SVG 或 PNG 格式的图标，用于表示 Helm 图表。显示在 Helm Hub 的图表页上。 keywords 关键字列表，可以通过 helm search 命令搜索到。 kubeVersion 在 SemVer 中兼容 Kubernetes 的一系列版本。 maintainers 图表的维护人员名单。 sources 链接到 Helm 图表或应用程序源代码的 URL 列表。 type 定义图表的类型，可以是 application 或 library。 一些字段仅仅提供描述信息，但有些字段可以修改 Helm 图表的行为。比如 type 字段，可以设置图表类型为 application 或 library。如果设置为 application，图表将用来部署应用程序；如果设置为 library，图表可通过 helper 模板的形式向其他图表提供功能。 第二个可以修改 Helm 图表行为的字段是 dependencies(依赖项) 字段，我们在下一节讨论。 管理图表依赖 Helm 可以自行安装依赖的图表，但具体依赖哪些图表需要图表开发人员在 Chart.yaml 文件中的 dependencies 字段设置。比如 wordpress 图表，它在依赖项中声明了 mariadb。通过配置依赖，免去了重头定义资源的麻烦。以下是 wordpress 图表中定义 mariadb 依赖的代码段： dependencies: - condition: mariadb.enabled name: mariadb repository: https://charts.bitnami.com/bitnami version: 9.x.x 在这里，我仅仅粘贴了 mariadb 的依赖。通过执行 helm show chart bitnami/wordpress --version 11.0.5 命令，可以看到 dependencies 块是一个可定义多个依赖项的列表。 dependencies 块包含多个字段，这些字段用来修改图表的依赖项管理行为。可以在下表中查看这些字段： 字段 定义 是否必须 name 依赖的图表名称 是 repository 依赖的图表位置(存储库) 是 version 依赖的图表版本 是 alias 依赖的图表别名 否 condition 布尔值，它判断是否包含依赖项 否 import-values 将依赖图表的值导入到父图表 否 tags 标签 否 dependencies 块中的必要字段只有 name、repository 和 version。我们在 wordpress 依赖项中看到，依赖的图表名称是 mariadb，存储库(repository)地址是 https://charts.bitnami.com/bitnami。它会从该存储库中搜索名称为 mariadb 的依赖。version 字段定义了依赖图表的版本，它的值可以是一个固定的版本号，比如 9.0.0；也可以是通配符形式，比如 9.x.x，如果定义的是通配符，会下载与通配符匹配的最新图表版本。 在了解了 dependencies 的必要字段后，我们来学习如何下载依赖项。 下载依赖项 可以使用 helm dependency 子命令下载依赖项，子命令列表如下： 命令 作用 helm dependency build 基于 Chart.lock 文件重建 chart/ 目录，如果 Chart.lock 文件不存在，则执行 \"helm dependency update\" 命令。 helm dependency list 列出图表的依赖项。 helm dependency update 基于 Chart.yaml 文件的内容更新 chart/ 目录，并生成 Chart.lock 文件。 第一次下载依赖时，可以使用 helm dependency update 命令，它会将每个依赖项下载到指定图表的 charts/ 目录下： $ helm dependency update $CHART_PATH helm dependency update 命令从图表存储库中下载 .tgz 扩展名的 GZip 格式压缩的依赖项。这个命令还会生成 Chart.lock 的文件。Chart.lock 文件类似 Chart.yaml 文件。Chart.yaml 文件定义是否需要依赖项，Chart.lock 文件定义依赖项的实际状态。 Chart.lock 文件内容如下： dependencies: - name: mariadb repository: https://charts.bitnami.com version: 9.3.11 digest: xxxxx generated: \"2021-xxxx\" 与其对应的 Chart.yaml 文件示例： apiVersion: v2 version: 0.0.1 name: dependencies-demonstration dependencies: - name: mariadb version: 9.x.x repository: https://charts.bitnami.com 在 Chart.yaml 文件中，指定依赖的 mariadb 版本是 9.x.x，但在 Chart.lock 文件中，版本是 9.3.11，这是因为 Chart.yaml 文件用了通配符定义版本，Helm 会下载 9.x.x 的最新版本，而当前最新版本正是 9.3.11。 根据 Chart.lock 文件，Helm 可以在 charts/ 目录被删除或者需要重新构建时，自动下载依赖项： $ helm dependency build $CHART_PATH 既然能够通过 helm dependency build 命令下载依赖项，就可以从我们的图表源码中删除 charts/ 目录，以减少代码仓库的大小。 随着功能迭代，总会产生新的版本。这时可以使用 helm dependency update 命令更新依赖项，它会下载最新的可用版，并重新生成 Chart.lock 文件。如果将来要下载 10.x.x 版本，或者希望将依赖项固定到特定版本，如 9.0.0，可以在 Chart.yaml 文件中定义，然后执行 helm dependency update 命令。 helm dependency list 命令可用于查看下载并保存在本地主机上的 Helm 图表依赖项： [aiops@aiops0113 testChart]$ helm dependency list wordpress NAME VERSION REPOSITORY STATUS mariadb 9.x.x https://charts.bitnami.com/bitnami ok memcached 5.x.x https://charts.bitnami.com/bitnami ok common 1.x.x https://charts.bitnami.com/bitnami ok STATUS 列表示依赖项是否已经成功下载到 charts/ 目录。如果状态显示为 ok，表示依赖项已下载。如果状态为missing，则表示该依赖项还没有被下载。 默认会下载 Chart.yaml 文件中定义的所有依赖项，可以通过 dependencies 块提供的 condition 或者 tags 字段选择下载哪些依赖。 选择依赖项 使用 condition 和 tags 字段，可以在安装或升级时，有条件地选择依赖项。以下是 Chart.yaml 文件的 dependencies 块示例： dependencies: - name: dependency1 repository: https://example.com version: 1.x.x condition: dependency1.enabled tags: - monitoring - name: dependency2 repository: https://example.com version: 2.x.x condition: dependency2.enabled tags: - monitoring 在这个示例中， dependencies 块增加了 condition 和 tags 两个字段。condition 字段的值由用户或图表的 values.yaml 文件提供。如果评估结果为 true，依赖项会被使用；如果为 false，则不包含该依赖项。可以通过逗号 (,) 添加多个条件，如下所示： condition: dependency1.enabled, global.dependency1.enabled condition 的设置应遵循 chartname.enabled 的命名规范，每个依赖项都应根据依赖的图表名称设置唯一条件。这可以让用户更直观的启用或禁用某个图表依赖。如果条件值未包含在 values.yaml 文件中、或者用户未提供，则将忽略 condition 字段。 condition 字段用于启用或禁用某个依赖项，而 tags 字段用于启用或禁用依赖项组。在上面的示例中，dependencies 块中的两个依赖项都配置了 monitoring 标签(tag)，这意味着如果启用了 monitoring 标签，两个依赖项都会包含在内。如果 monitoring 设置为 false，则忽略这两个依赖项。通过在父图表的 values.yaml 文件中的 tags 对象下，设置标签是启用还是禁用： tags: monitoring: true 在 Chart.yaml 文件中，遵循 YAML list 语法，依赖项可以定义多个标签。其中一个为 true，此依赖项就会被使用。如果一个依赖项的所有标签都被忽略，那么依赖项将被默认包含。 在本节中，我们讨论了如何有条件地声明依赖关系。接下来，我们将讨论如何覆盖和引用依赖项中的值。 覆盖和引用子图表中的值 默认情况下，可以引用或覆盖依赖项(子图表) 的值，方法是将这些值包装到与子图表同名的映射(map)中。假设一个名为 my-dep 的子图表包含以下值： ## 子图表中的配置 replicas: 1 servicePorts: - 8080 - 8443 当 my-dep 图表作为依赖项安装时，可以在父图表的 my-dep YAML 对象中设置这些值： ## 父图表的配置 my-dep: replicas: 3 servicePorts: - 8080 - 8443 - 8881 上面的示例覆盖了 my-dep 图表中定义的 replicas 和 servicePorts 的值，将 replicas 设置为 3，并向 servicePorts 添加了一个 8881 的端口。这些值可以在父图表的模板中引用，方式是使用点号(.)，例如 my-dep.replicas。除了覆盖和引用值外，还可以通过 import-values 字段直接导入依赖项的值。 使用 import-values 导入值 Chart.yaml 文件中的 dependencies 块支持 import-values 字段，用来在父图表中导入子图表的值。这个字段有几种使用方式，第一种方式是提供要从子图表导入的键的列表。要确保其能正常工作，子图表必须在 exports 块下声明了该值，如下所示： exports: image: registry: 'my-registry.io' name: learnhelm/my-image tag: latest 然后父图表可以在 Chart.yaml 文件中定义 import-values 字段： dependencies: - name: mariadb repository: https://charts.bitnami.com/bitnami version: 7.x.x import-values: - image 子图表中 exports.image 的默认值会在父图表中被引用，内容如下： registry: 'my-registry.io' name: learnhelm/my-image tag: latest 需要注意的是，这种导入方式，删除了 image 映射，只保留了它的键值对。如果要保留 image，可以在 import-values 字段里使用 child-parent 格式。它会从指定的子图表中导入值，并提供它们在父图表中应引用的名称： dependencies: - name: mariadb repository: https://charts.bitnami.com/bitnami version: 7.x.x import-values: - child: image parent: image 这个示例会获取子图表中 image 块下的所有值，并将其导入父图表的 image 块中。 使用 import-values 字段导入的值不能在父图表中被覆盖。如果子图表中的值需要覆盖，则不应该使用 import-values 字段，而是应该通过在每个子图表的名称前添加前缀来覆盖所需值。 在本节中，我们讨论了如何在 Chart.yaml 文件中管理依赖项，接着，我们来学习如何在 Helm 图表中定义 release 的生命周期管理钩子(hooks)。 Release 生命周期管理 Helm 图表的功能之一是能够管理 Kubernetes 上的复杂应用程序。每个应用程序的 release 都会经历生命周期的各个阶段。为了对 release 的生命周期进行管理，Helm 提供了钩子(hooks)机制，以便在 release 生命周期的不同阶段执行操作。在本节中，我会介绍应用程序生命周期的不同阶段，以及如何使用钩子与 release 及整个 Kubernetes 集群交互。 在第三章中，我们介绍了 Helm release 生命周期的几个阶段，包括安装、升级、删除和回退。有的 Helm 图表很复杂，它们管理将要部署到 Kubernetes 的一个或多个应用程序，除了部署资源之外，往往还有其它操作，如： 完成应用程序所需的先决条件，例如管理证书或者 secrets 在升级图表前，执行数据库的备份、恢复操作 在删除图表前，清理部署的资源 当然，还会有很多具体操作，因此我们必须要了解 Helm 钩子的基本知识以及何时执行钩子，这也是我们下一节的目标。 钩子基础知识 钩子在 release 生命周期的指定阶段作为一次性动作执行，与 Helm 中的大多数特性一样，钩子也是作为 Kubernetes 的资源实现的，更具体地说是在容器中实现的。虽然 Kubernetes 中大多数工作负载都是为长期存在的进程而设计的，例如提供 API 请求服务的应用程序，但工作负载也可以由单个任务或一组任务组成，这些任务使用脚本执行，并在执行完成后显示成功或失败。 在 Kubernetes 集群中创建短期任务通常有两种方式：裸 Pod 或者 Job。裸 Pod 是一种运行完成即终止的 Pod，缺陷是如果工作(Worker)节点发生故障，裸 Pod 不会被重新调度。由于这个原因，最好将生命周期钩子作为 Job 运行，这样工作节点故障时，钩子依然会被重新调度。 钩子被定义为 Kubernetes 的资源，存放在 templates/ 目录中，并用 helm.sh/hook 注解(Annotations)。此注解确保在标准处理过程中，钩子不会与 Kubernetes 集群中的其他资源一起渲染。helm.sh/hook 注解决定了钩子何时作为 release 生命周期的一部分执行。 将钩子定义为 Job 的示例： apiVersion: batch/v1 kind: Job metadata: name: helm-auditing annotations: 'helm.sh/hook': pre-install,post-install spec: template: metadata: name: helm-auditing spec: restartPolicy: Never containers: - name: helm-auditing command: [\"/bin/sh\", \"-c\", \"echo Hook Executed at $(date)\", \"sleep 10\"] image: alpine 这个简单的示例会打印容器当前的日期和时间，然后休眠 10 秒钟。Helm 在图表安装前(pre-install)和安装后(post-install)执行这个钩子。这类钩子可以绑定到一个审计系统，跟踪应用程序在 Kubernetes 集群中的安装进度，安装完成后，触发钩子，统计图表安装完成所耗费的时间。 介绍了钩子的基础知识，我们来看看如何在 Helm 中定义钩子。 执行钩子 正如你在上一节的“将钩子定义为 Job”的示例中看到的，helm.sh/hook 注解的值是 pre-install。pre-install 是 Helm 图表生命周期中可以执行钩子的一个点。 下表列出了 helm.sh/hook 注释的所有值，它们决定了何时执行钩子。这些信息来自 Helm 官方文档，可以在 https://helm.sh/docs/topics/charts_hooks/#the-available-hooks 页面查看： 注解值 描述 pre-install 在模板渲染后、Kubernetes 创建资源前执行 post-install 在 Kubernetes 创建资源后执行 pre-delete 在 Kubernetes 删除资源前执行 post-delete 在该 release 相关的资源被删除后执行 pre-upgrade 在模板渲染后、任何资源更新前执行 post-upgrade 在所有资源更新后执行 pre-rollback 在模板渲染后、任何资源回退前执行 post-rollback 在所有资源回退后执行 test 当运行 helm test 子命令时执行 helm.sh/hook 注解可以包含多个值，这些值定义了同一资源在图表发布周期内的不同时间点执行。例如，要在图表安装前后执行钩子，可以在 Pod 或 Job 上定义以下注解： annotations: 'helm.sh/hook': pre-install,post-install 了解如何执行钩子、以及在图表生命周期中的哪个阶段执行钩子非常重要。如上面示例，当指定在 pre-install 和 post-install 钩子之间执行 helm install 命令时，会发生以下操作： 用户安装一个 Helm 图表(例如，helm install bitnami/wordpress --version 11.0.5)。 Helm API 被调用。 crds/ 目录中的 CRDs 被加载到 Kubernetes 中。 执行图表模板资源验证，并渲染资源。 pre-install 钩子按权重排序，然后被渲染并加载到 Kubernetes 中。 Helm 等待 pre-install 钩子执行完成。 模板资源被渲染并应用到 Kubernetes 环境。 post-install 钩子被执行。 Helm 等待 post-install 钩子完成。 返回 helm install 命令的执行结果。 在了解了 Helm 钩子执行的基础知识后，我们来学习 Helm 钩子的高级知识。 钩子进阶 在 Helm 图表的生命周期内执行的钩子数量是没有限制的，并且在某些情况下，可能会为同一生命周期的某个阶段配置多个钩子。此时，钩子的默认执行顺序是按照钩子的首字母顺序。通过 helm.sh/weight 注解可以为每个钩子设置权重，这样就可以按权重顺序执行钩子。权重按升序排序，如果多个钩子的权重值相同，则继续按名称字母顺序执行。 钩子提供了管理 Helm 图表生命周期的机制，但与模板资源不同的是，钩子不会被 Helm 跟踪和管理，在执行 helm uninstall 命令时，钩子不会随着图表一起删除。有两种方案删除 release 生命周期中的钩子：配置删除策略和在 Job 上设置生存时间控制器(TTL,time to live controller)。 首先，可以在钩子关联的 Pod 或者 Job 上添加 helm.sh/hook-delete-policy 注解，该注解定义了 Helm 应何时从 Kubernetes 中删除资源。下表是具体的注解值(https://helm.sh/docs/topics/charts_hooks/#hook-deletion-policies)： 注释值 描述 before-hook-creation 在启动一个新钩子前删除之前的资源（默认） hook-succeeded 钩子执行成功后删除资源 hook-failed 如果钩子在执行过程中失败，则删除资源 此外，Kubernetes 的生存时间控制器还提供了生存时间(TTL)机制，限制已完成执行的资源对象的寿命。生存时间控制器目前只支持 Job，使用 Job 的 ttlSecondsAfterFinished 属性来控制资源完成后的保留时间，如下所示： apiVersion: batch/v1 kind: Job metadata: name: ttl-job annotations: 'helm.sh/hook': post-install spec: ttlSecondsAfterFinished: 60 在本例中，钩子资源在完成或失败后60秒内被删除。 删除应用程序是 release 生命周期管理的最后一个阶段，尽管 helm uninstall 命令可以删除已安装的图表，但你可能希望保留某些资源，比如在应用程序生命周期开始时，通过 PersistentVolumeClaim 创建的持久卷，保留着应用程序程序运行期间的数据，不应该一起被删除。可以使用 helm.sh/resource-policy 注解对此进行设置： 'helm.sh/resource-policy': keep 这时再执行 helm uninstall 命令，持久卷就不会被删除了。但需要注意的是，该资源会变成一个孤立资源。如果再次使用 helm install 命令，可能会出现老资源未删除而导致的资源名字冲突，进而部署失败的现象。删除孤立资源，可以使用 kubectl delete 命令，手动删除。 本节介绍了钩子的创建和自动化管理图表的生命周期，在下一节中，我会介绍编写 Helm 图表文档的软技能。描述准确的文档，有助于用户获得流畅的使用体验。 图表文档 完善的文档可以帮助用户更便捷的使用 Helm 图表。Helm 图表结构支持使用自述文件(README.md)记录图表的用法、LICENSE 文件授权使用、tepmlate/NOTES.txt 文件在图表安装完成后输出使用说明。 README.md 自述文件(README.md)是软件开发中常用的文件，描述应用程序的部署、使用及其他信息。Helm 图表的 README 文件通常包含以下信息： Prerequisites：安装图表(应用程序)的先决条件。常见示例是在 Kubernetes 集群中安装图表时，需要先创建 Secret。通过 README 文件，用户会注意到这一要求。 Values：图表通常包含不同的值，每个值都应在 README 文件中进行说明。包括值的名称、描述信息、默认值、在安装或升级过程中，是否需要提供该值等。 Application-specific information：使用 Helm 图表安装应用程序后，可能需要应用程序自身的信息，例如该如何访问这个应用。这些信息也可以在 README 文件中提供。 Helm README 文件使用 Markdown 格式语法编写，Markdown 通常在 GitHub 项目和开放源代码软件中使用，是一种轻松编写文本的方式，以优雅的格式显示文本。在 Markdown 指南网站上，可以学习更多 Markdown 的使用方法，网址为 https://www.markdownguide.org。 LICENSE 除了包含技术说明的 README 文件，有时图表中也会包含一个许可文件，说明图表的使用和分发权限。许可文件的名称是 LICENSE，在图表的顶级目录下。 LICENSE 文件是包含一个软件许可证的纯文本文件，许可证可以是自己编写的，也可以是开源软件中常用的许可证副本，例如 “Apache License 2.0” 或者 “MIT License”。 如果你想了解更多许可证的内容，可以访问 https://choosealicense.com 网站。 templates/NOTES.txt 类似 README.md 文件，templates/NOTES.txt 文件用于在使用 Helm 安装应用程序后提供使用说明。不同的是，README.md 文件是静态的，NOTES.txt 文件可以通过 Go 模板动态生成。 假设一个 Helm 图表的 values.yaml 文件包含如下配置： ## serviceType can be set to NodePort or LoadBalancer serviceType: NodePort 设置不同的 Service 类型，应用的访问方式会有所不同。如果设置的 Service 类型为 Nodeport，则可以使用 Kubernetes 集群中任一节点的 IP:NodePort 访问。如果 Service 的类型为 LoadBalancer，则将使用在创建 Service 时自动设置的负载均衡器的地址访问应用程序。对于经验欠缺的 Kubernetes 用户来说，理解如何基于 Service 类型访问应用程序可能很困难，因此图表维护人员应该在 templates/ 目录下提供 NOTES.txt 文件，来说明该如何访问应用程序。 示例： 请按照下述说明访问你的应用程序： {{- if eq .Values.serviceType 'NodePort' }} export NODE_PORT=$(kubectl get --namespace {% raw %}{{ .Release.Namespace }}{% endraw %} -o jsonpath='{.spec.ports[0].nodePort}' services {% raw %}{{.Release.Name }}{% endraw %}) export NODE_IP=$(kubectl get nodes --namespace {% raw %}{{ .Release.Namespace }}{% endraw %} -o jsonpath='{.items[0].status.addresses[0].address}') echo \"URL: http://$NODE_IP:$NODE_PORT\" {{- else }} export SERVICE_IP=$(kubectl get svc --namespace {% raw %}{{ .Release.Name }}{% endraw %} wordpress --template '{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}') echo \"URL: http://$SERVICE_IP\" {{- end }} 该内容会在应用程序安装、升级、回退时生成并输出到终端，也可以通过 helm get notes 命令获取。通过显示的内容，指导用户如何访问应用程序。 到目前为止，我已经介绍了 Helm 图表的大部分内容，除了将图表打包。包使图表易于分发，在下一节，我们讨论包的概念。 图表的压缩包 虽然 Helm 图表遵循通用的文件结构，但考虑到分发与传播，我们应该把图表打成 tgz 格式的压缩包。可以使用 bash 中的 tar 命令手动完成，不过 Helm 提供了 helm package 命令，简化了打包的过程。helm package 命令的语法如下： $ helm package [CHART_NAME] [...] [flags] 执行 helm package 命令时需要指定打包的本地图表目录。命令执行成功后，将生成一个 tgz 的压缩文件，文件名称格式如下： $ CHART_NAME-$CHART_VERSION.tgz 可以把压缩文件推送到图表存储库中，其他人就可以从库中获取并使用这个图表了，就像我们使用 WordPress 图表一样。这部分内容我会在第五章详细介绍。 helm package 命令会打包图表目录下的所有文件，但有时图表目录中包含了非必须打包的文件或目录，在打包时最好把它们过滤掉。比如 .git 目录，它存在于 Git SCM 管理的项目中，把它打包进 tgz 压缩文件中，没有任何意义，还会增加压缩文件的大小，造成资源浪费。可以通过 .helmignore 文件，在打包时忽略某些文件或目录。.helmignore 文件内容如下： # 忽略 .git/ 目录和 .gitignore 文件. .git/ .gitignore 包含在 .helmignore 文件中的目录或文件名，会被 helm package 命令忽略，不会被打包进 tgz 压缩文件中。以# 开头的行为注释行。如果你的图表目录中包含了与 helm 图表功能无关的文件或目录，请确保使用 .helmignore 文件将其过滤掉。 By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-26 18:03:49 "},"helm/kaifacharts.html":{"url":"helm/kaifacharts.html","title":"开发 Helm 图表","keywords":"","body":"5. 开发 Helm 图表 通过上一章的学习，你对 Helm 图表有了更深入的了解，现在是时候自己开发一个 Helm 图表来检验所学了。学会构建 Helm 图表，就可以更便捷的在 Kubernetes 集群中部署复杂的应用程序了。 为了便于大家学习开发 Helm 图表，我编写了 player-stats 应用程序 ，我们就以此程序为例，完成 Helm 图表开发。开发过程会遵循 Kubernetes 和 Helm 图表的最佳实践，以提供编写良好且易于维护的代码，在整个过程中，你将学到许多构建图表的技能。在本章的最后，会介绍如何打包图表并上传图表到图表存储库，以便让其他用户使用我们开发的图表。 5.1 player-stats 应用程序介绍 在本章中，你将创建一个部署 player-stats 应用程序的 Helm 图表。Player-stats 使用 Go 开发，后端存储是 MongoDB，感兴趣的读者可以在 https://github.com/weiwendi/player-stats 页面查看 player-stats 的源码及介绍。部署完成后，可以在页面录入球员的姓名及球衣号码，这些信息会存储到 MongoDB 数据库中，并展示在前端页面。这种架构很常见，很适合作为学习开发 Helm 图表的示例。 Payer-stats 的前端页面由对话框和提交按钮组成，用于将消息持久化到后端的 MongoDB。 要与此应用程序交互，用户可以遵循以下步骤： 在“球员姓名”和“球衣号码”对话框中输入球员信息。 点击“添加信息”按钮提交信息。 信息被保存到 MongoDB 数据库。 MongoDB 是一个基于分布式文件存储的数据库，在本章中，我们以单实例的方式部署 MongoDB。 在基本了解 player-stats 应用程序后，我们开始开发 Helm 图表。在此之前，要确保使用 Minikube 部署的 Kubernetes 集群是可用的。 5.2 环境准备 按照以下步骤创建 minikube 环境： 使用 minikube start 启动 minikube： [aiops@aiops0113 ~]$ minikube start 在 Kubernetes 集群中创建一个名为 website 的命名空间，用于部署留言板应用程序： [aiops@aiops0113 ~]$ kubectl create namespace website 现在环境已经准备好了，我们开始开发图表。 5.3 创建 player-stats 图表 在本节中，我们创建 player-stats 应用程序的 Helm 图表，以便在 Kubernetes 集群中部署该应用。图表的最终版本我已经发布到了 https://github.com/weiwendi/learn-helm/tree/main/helm-charts/charts/player-stats 图表存储库中，在跟随本章学习时，可以随时参考。 开发图表前，需要搭建图表初始文件结构。 5.3.1 搭建初始文件结构 上一章我们介绍了图表必须遵循一个特定的文件结构。也就是说，一个图表必须包含以下文件： Chart.yaml：定义图表元数据 values.yaml：定义图表的默认值 templates/：定义创建 Kubernetes 资源的图表模板 在上一章中，我们已经看到了一个图表可能包含的文件列表，前三个文件是开发图表所必需的。虽然这三个文件可以手动创建，但 Helm 提供了 helm create 命令，可以快速创建图表。 helm create 命令除了自动创建前面列出的三个文件外，还会生成许多不同的模板文件，开发人员可以利用这些模板更快的编写 helm 图表。现在，我们使用此命令创建一个名为 player-stats 的图表。 helm create 命令使用 Helm 图表的名称作为参数，此处的 Helm 图表名称是 player-stats。我们在命令行上创建这个图表： [aiops@aiops0113 ~]$ cd helm-charts/ [aiops@aiops0113 chart]$ helm create player-stats Creating player-stats 此命令执行后，会在当前目录生成一个名为 player-stats/ 的目录，这个就是 player-stats 应用程序的图表目录。在这个目录中，你能看到以下四个文件或目录： charts/ Chart.yaml templates/ values.yaml 如你所见，除了所需的 Chart.yaml、values.yaml、templates/，helm create 命令还创建了 charts/ 目录。charts/ 目前是一个空目录，但当我们声明图表依赖关系时，该目录下会生成对应的文件。你可能还注意到，前面提到的其他文件已经使用默认值进行了填充。在本章中，我们将在开发 player-stats 图表时使用部分默认值。 如果你查看 templates/ 目录下的内容，会发现默认包含了许多不同的模板资源，这些资源大大节省了我们的开发时间。虽然生成了许多有用的模板，但我们需要删除 templates/test/ 文件夹，这个文件夹用来存放 Helm 图表的测试文件，我会在下一章介绍如何对 Helm 图表进行测试。现在，我们先删除该目录： [aiops@aiops0113 chart]$ rm -rf player-stats/templates/tests 我们已经搭建好了 player-stats 图表，接着配置 Chart.yaml 文件。 5.3.2 配置图表定义 图表定义或者 Chart.yaml 文件，包含着 Helm 图表的元数据。在上一章中，我介绍了 Chart.yaml 文件中的每一个选项，现在我们一起回顾下图表定义的主要设置： apiVersion：设置为 v1 或 v2，使用 Helm3 时，v2 是首选项 version：Helm 图表的版本，应该遵循 Semver 规范 appVersion：Helm 图表部署的应用程序的版本 name：Helm 图表的名称 description：对 Helm 图表的描述信息 type：设置为 application 或 library。Application 图表用于部署应用程序；Library 图表包含一组辅助函数(也称为“命名模板”)，可以跨其他图表使用，以减少相同模板的编写。 dependencies：图表所依赖的图表列表 查看 player-stats 图表中默认生成的 Chart.yaml 文件，你会发现除了依赖项，其它字段已经设置好了。我们使用这些默认值即可。 图表定义中默认没有设置依赖项，在下一节，我们为 player-stats 图表添加 MongoDB 依赖项，来简化我们的开发工作。 5.3.3 添加 MongoDB 图表依赖 Player-stats 应用程序前端页面将数据存储到后端的 MongoDB 里，所以我们的 Helm 图表必须能够部署 MongoDB。如果从头创建 MongoDB 的 Helm 图表，除了所需的图表模板，还要对 MongoDB 有一定的了解，以及要知道如何将 MongoDB 部署到 Kubernetes 上。 但通过引入 MongoDB 的依赖，可以大大减少开发 player-stats 图表所涉及的工作量，因为它已经包含了所需的图表模板以及部署逻辑。我们修改 Chart.yaml 文件，添加 MongoDB 依赖。 添加 MongoDB 图表依赖的过程大致需要以下步骤： 在 Helm 图表存储库中搜索 MongoDB 图表： [aiops@aiops0113 player-stats]$ helm search hub mongodb 搜索结果中会出现 Bitnami 的 mongodb 图表，我们使用这个图表作为依赖。如果你没有添加 bitnami 图表库，可以使用 helm repo add 命令添加： $ helm repo add bitnami https://charts.bitnami.com/bitnami 确认想要安装的 MongoDB 图表版本： [aiops@aiops0113 player-stats]$ helm search repo mongodb --versions NAME CHART VERSION APP VERSION bitnami/mongodb 10.15.2 4.4.6 bitnami/mongodb 10.15.1 4.4.5 bitnami/mongodb 10.15.0 4.4.5 bitnami/mongodb 10.14.0 4.4.5 需要指定的是图表版本，而不是 APP 版本。APP 版本表示 MongoDB 的版本，而图表版本表示的是 Helm 图表的版本。 在依赖关系中可以指定具体的图表版本，或者通配符，比如 10.15.x。使用通配符可以轻松地将图表更新到通配符匹配的最新 MongoDB 版本(本例中通配符匹配的版本是 10.15.2)。在本例中，我们使用 10.15.x 版本。 将 dependencies 字段添加到 Chart.yaml 文件中，对于 player-stats 图表，我们仅做最基本的配置： name：依赖的图表名称 version：依赖的图表版本 repository：依赖的图表仓库的URL 具体配置如下： dependencies: - name: mongodb version: 10.15.x repository: https://charts.bitnami.com/bitnami 添加完依赖后，完整的 Chart.yaml 文件配置如下： apiVersion: v2 name: player-stats description: A Helm chart for Kubernetes type: application version: 0.1.0 appVersion: \"1.16.0\" dependencies: - name: mongodb version: 10.15.x repository: https://charts.bitnami.com/bitnami 也可以在 https://github.com/weiwendi/learn-helm/blob/main/helm-charts/charts/player-stats/Chart.yaml 页面查看该文件(请注意 version 和 appVersion 字段可能不同，因为我们会在本章的后面修改它们)。 既然依赖项已添加到图表定义中了，我们下载此依赖项以确保它已正确配置。 5.3.4 下载 MongoDB 图表依赖 第一次下载依赖时，应该使用 helm dependency update 命令，此命令会将依赖项下载到 charts/ 目录，并生成 Chart.lock 文件，该文件记录了已下载的图表元数据。 helm dependency update 以 Helm 图表位置作为参数，如下所示： [aiops@aiops0113 chart]$ helm dependency update player-stats Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"bitnami\" chart repository Update Complete. ⎈Happy Helming!⎈ Saving 1 charts Downloading mongodb from repo https://charts.bitnami.com/bitnami Deleting outdated charts 查看 charts/ 文件夹下是否出现了 MongoDB 图表以验证执行结果： [aiops@aiops0113 chart]$ ls player-stats/charts/ mongodb-10.15.2.tgz 现在已经包含了 MongoDB 依赖，接下来我们修改 values.yaml 值文件。 5.3.5 修改 values.yaml 值文件 Helm 图表中的 values.yaml 文件为图表模板提供了一组默认值，当然，用户在与 Helm 图表交互时，可以使用 --set 或 --values 标志覆盖这些值，但还是要尽量通过 values.yaml 文件来修改这些值，以便后续维护。除了提供默认值之外，还可以为每个值添加详细、直观的注释，方便使用者和维护者通过 values.yaml 文件快速了解图表的值。 helm create 命令生成的 values.yaml 文件，包含了许多模板中会使用到的值。我们需要在此文件中添加一些值，来配置 MongoDB 依赖。 5.3.5.1 配置 MongoDB 图表的值 使用 MongoDB 依赖项省去了很多麻烦，但让它更好的工作，我们还需覆盖它的一些值。首先我们看看 MongoDB 图表都包含了哪些值： [aiops@aiops0113 player-stats]$ helm show values charts/mongodb-10.15.2.tgz |more 输出的值列表太长，我们使用 more 分屏显示。我们来看看哪些值需要覆盖： 需要覆盖的第一个值是 fullnameOverride： ## String to fully override mongodb.fullname template ## # fullnameOverride: 这个值会被名称为 $CHART_NAME.fullname 的命名模板引用。当为 fullnameOverride 设置了值，命名模板会被赋予此值。否则，此模板将使用 .Release.Name 或者安装 Helm releas时提供的名称。 在 player-stats 应用程序中，连接 MongoDB 数据的地址是 “mongo”。MongoDB 的 Service 地址会根据此值设置，所以我们需要把此值设置为 “mongo”。 第二个需要覆盖的值是 auth.enabled auth: ## Enable authentication enabled: true Player-stats 应用程序被编写为无须身份验证访问 MongoDB 数据库，所以我们要把这个值设为 false。 我们将这些修改的值加到 values.yaml 文件的末尾： mongodb: fullnameOverride: \"mongo\" auth: enabled: false 在上一章中介绍了覆盖依赖项中的值，必须在其图表名称下的块中。这就是为什么将每个值都添加在 mongodb 块下的原因。 你可以参考 https://github.com/weiwendi/learn-helm/blob/main/helm-charts/charts/player-stats/values.yaml 的内容，来检验 MongoDB 部分的配置是否正确。 5.3.5.2 配置 player-stats 的值 使用 helm create 命令创建项目时，会在 templates/ 目录下自动创建一些模板文件，列表如下： deployment.yaml：部署 player-stats 应用程序到 Kubernetes 集群。 ingress.yaml：提供一种从 Kubernetes 集群外部访问 player-stats 应用程序的选项。 serviceaccount.yaml：为 player-stats 应用创建专用的 ServiceAccount service.yaml：用于在 player-stats 应用程序的多个实例间进行负载均衡。还可以提供从 Kubernetes 集群外部访问 player-stats 应用程序的选项 _helpers.tp：提供在整个 Helm 图表中使用的一组通用模板 NOTES.txt：提供在安装完成该应用后访问该应用的说明 每个模板都由图表的值配置，一些默认值不能满足使用需求，我们使用所需值对这些默认值进行替换，替换前，先来看看都需要替换哪些值。 在图表模板中有一个 deployment.yaml 文件，定义了部署时使用的镜像： image: '{% raw %}{{ .Values.image.repository }}{% endraw %}:{% raw %}{{ .Chart.AppVersion }}{% endraw %}' 从这行代码中可以看到，镜像由 image.repository 和 AppVersion 确定的。在 values.yaml 文件中可以看到 image.repository 配置的是一个 nginx 镜像： image: repository: nginx 在 Chart.yaml 文件中，可以看到 AppVersion 的值为 1.16.0： appVersion: 1.16.0 player-stats 应用程序的镜像已经被我上传到了镜像仓库，地址是 registry.cn-beijing.aliyuncs.com/sretech/player-stats:0.1.0。 因此为了正确的生成 image 字段，image.repository 的值须设置为 registry.cn-beijing.aliyuncs.com/sretech/player-stats，AppVersion 的值须设置为 0.1.0。 需要修改的第二个图表模板是 service.yaml，在这个文件中，定义了 Service 的类型： type: {{ .Values.service.type }} 在 values.yaml 中默认的 service.type 的值为： service: type: ClusterIP 对于 player-stats 图表，我们把 type 值修改为 NodePort，以创建一个 NodePort 类型的 Service。这样用户就可以通过 minikube 虚拟主机上的 IP 和端口，访问 player-stats 应用程序了。 helm create 默认生成的也有 ingress.yaml 模板，但在 minikube 环境中，推荐使用 NodePort 类型的 Service，因为不需要额外安装增强组件。 我们已经确定了需要修改的默认值，现在让我们在 values.yaml 文件中修改它们吧： 修改镜像，image 块配置如下： image: repository: registry.cn-beijing.aliyuncs.com/sretech/player-stats pullPolicy: IfNotPresent 替换 Service 的类型为 NodePort，service 块配置如下： service: type: NodePort port: 80 可以参考 https://github.com/weiwendi/learn-helm/blob/main/helm-charts/charts/player-stats/Chart.yaml 页面验证你配置的正确与否。 接着更新 Chart.yaml 文件： 替换 appVersion 字段的值为 v0.1.0： appVersion: \"0.1.0\" 可以参考 https://github.com/weiwendi/learn-helm/blob/main/helm-charts/charts/player-stats/Chart.yaml 页面验证你配置的正确与否。 我们已经正确配置了值，现在试着将其部署到 minikube 中。 5.3.6 安装 Player-stats 图表 在 player-stats/ 目录外部执行执行以下命令，安装 player-stats 图表： [aiops@aiops0113 helm-charts]$ helm install myplayer-stats player-stats -n website NAME: myplayer-stats LAST DEPLOYED: Wed Jun 9 03:41:42 2021 NAMESPACE: website STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace website -o jsonpath=\"{.spec.ports[0].nodePort}\" services myplayer-stats) export NODE_IP=$(kubectl get nodes --namespace website -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT 安装完成后，我们需要等待 player-stats、MongoDB 的 Pod 都处于 READY 状态，才能访问。查看 Pod 状态的命令： $ kubectl get pods -n website 在执行 helm install 时也可以加上 --wait 标识，以等待 Pod 处于 READY 状态时提示完成安装。--wait 可以与 --timeout 一起使用，以秒为单位设置 Helm 的等待时间，默认是 5 分钟，这个时间对于启动应用程序来说足够了。 一旦所有 Pod 处于 READY 状态，就可以执行安装完成时的 Notes 信息提供的命令，如果你没记住，可以使用这个命令再次显示这些信息： $ helm get notes myplayer-stats -n website 将 echo 输出的 IP:port 粘贴到浏览器里，就可以访问 player-stats 应用程序了。界面如下图所示： 5.1 player stats page 点击“添加信息”按钮，会跳转到添加信息页面。在对应的框中，输入球员姓名及球衣号码，再次点击“添加信息”按钮，数据就提交到了 MongoDB，页面自动返回到首页，并展示提交的信息。能够完成这些操作，说明 player-stats 部署成功了。 做完上面这些之后，我们就可以使用 helm uninstall 命令卸载它们了，这条命令会删除 player-stats 安装的所有相关资源以及 release 历史记录： [aiops@aiops0113 helm-charts]$ helm uninstall myplayer-stats -n website release \"myplayer-stats\" uninstalled uninstall 子命令还有另外三个别名，del、delete、un。 5.4 改进 player-stats 图表 上一节我们成功部署了 player-stats 应用，本节再增加两个功能： 添加备份和恢复数据的生命周期钩子 验证提供的值是否有效 5.4.1 创建生命周期钩子 在本节中，我们将创建两个生命周期钩子： 第一个钩子发生在 pre-upgrade 阶段，此阶段处于 helm upgrade 命令执行后、Kubernetes 资源被修改前。用于在执行升级前对 MongoDB 数据进行备份，以便在升级出错后能够恢复可能损坏的数据。 第二个钩子发生在 pre-rollback 阶段，此阶段处于 helm rollback 命令执行后、Kubernetes 资源被还原前。用于恢复 MongoDB 数据，并确保 Kubernetes 资源配置还原到与恢复数据匹配的点。 本节创建的钩子比较简单，仅用于教学，不适合在生产环境使用。不过在完成本节的学习后，你将更加熟悉生命周期钩子，并可以用它们执行一些强大的功能。 让我们来看看如何创建 pre-upgrade 生命周期钩子。 5.4.2 创建 pre-upgrade 钩子 我们创建一个 pre-upgrade 钩子，在更新应用程序前对 MongoDB 进行数据备份。MongoDB 的备份机制大体分为“延迟节点备份”和“全量备份 + oplog 增量备份”。我们的示例比较简单，仅是一个 MongoDB 的单节点部署，所以我们使用 mongodump 命令对数据库进行全量备份即可。在生产环境中，仅做全量备份是不足以保障数据安全的。 我们创建一个钩子，这个钩子首先会在 Kubernetes 的 website 命名空间中创建 PVC，然后创建 Job，Job 用于执行 mongodump 命令，我们把 dump 出的文件存储到新创建的 PersistentVolumeClaim 中。 helm create 命令虽然创建了许多模板，但它不会创建钩子，因此我们需要按照以下步骤创建 pre-upgrade 钩子： 创建一个文件夹，存放钩子模板。虽然这不是必须的，但新文件夹有助于将钩子模板与常规的图表模板分开，还可以按功能对钩子分组。 在 player-stats/ 文件结构中创建名为 template/backup 的文件夹： [aiops@aiops0113 helm-charts]$ mkdir player-stats/templates/backup 创建两个模板文件。persistentvolumeclaim.yaml 模板文件用于创建 PVC，存储 dump 出的数据；job.yaml 模板文件是用于执行 mongodump 命令的 Job 模板： [aiops@aiops0113 helm-charts]$ touch player-stats/templates/backup/persistentvolumeclaim.yaml [aiops@aiops0113 helm-charts]$ touch player-stats/templates/backup/job.yaml 编辑 persistentvolumeclaim 模板文件内容，也可以从 https://github.com/weiwendi/learn-helm/blob/main/helm-charts/charts/player-stats/templates/backup/persistentvolumeclaim.yaml 页面复制： NaN apiVersion: v1 kind: PersistentVolumeClaim metadata: name: '{{ .Values.mongodb.fullnameOverride }}'-data-backup-'{{ sub .Release.Revision 1 }}' labels: '{{- include \"player-stats.labels\" . | nindent 4 }}' annotations: \"helm.sh/hook\": pre-upgrade \"helm.sh/hook-weight\": \"0\" spec: accessModes: - ReadWriteOnce resources: requests: storage: '{{ .Values.mongodb.persistence.size }}' NaN 代码释义： 第一行和最后一行是一个 if 语句块，表示只有在 mongodb.persistence.enabled 的值为 true 时，才会创建此资源。该值在依赖的 MongoDB 图表中默认为 true，可以使用 helm show values 命令查看。 第五行定义了 PVC 的名称，这个名称基于 MongoDB 的 PVC 名称 mongo，在此名称后添加了 data-backup 及版本号，用于备份时命名再妥帖不过。由于钩子是在升级前执行的，因此它会引用要升级到的版本号，我们使用 sub 函数减去 1 个版本号。 第九行是注解行，声明钩子为 pre-upgrade。 第十行同样是注解行，指定与其它 pre-upgrade 的钩子相比，创建此资源的顺序。权重按升序运算，数值越小的优先被执行，因此该钩子将先于其它 pre-upgrade 资源被创建。 编辑 job.yaml 模板文件，也可以从 https://github.com/weiwendi/learn-helm/blob/main/helm-charts/charts/player-stats/templates/backup/job.yaml 页面复制： NaN apiVersion: batch/v1 kind: Job metadata: name: '{{ include \"player-stats.fullname\" . }}'-backup labels: '{{- include \"player-stats.labels\" . | nindent 4 }}' annotations: \"helm.sh/hook\": pre-upgrade \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded \"helm.sh/hook-weight\": \"1\" spec: template: spec: containers: - name: backup image: bitnami/mongodb:4.4.6-debian-10-r0 command: [\"/bin/sh\", \"-c\"] args: [\"cd /backup/ && mongodump -h '{{ .Values.mongodb.fullnameOverride }}':'{{ .Values.mongodb.service.port }}'\"] volumeMounts: - name: backup mountPath: /backup restartPolicy: Never volumes: - name: backup persistentVolumeClaim: claimName: '{{ .Values.mongodb.fullnameOverride }}'-data-backup-'{{ sub .Release.Revision 1 }}' NaN 代码释义： 第九行同样定义了是一个 pre-upgrade 的钩子，第十一行的权重设置为 1，表明其将在权重为 0 的 pre-upgrade 钩子创建之后创建。 第十行是新增注解，定义了何时删除此 Job。 默认情况下，Helm 在初始化创建图表后是不管理钩子的，这意味着执行 helm uninstall 命令时不会删除钩子。helm.sh/hook-delete-policy 注解用于定义删除资源的条件。before-hook-creation 删除策略指如果命名空间中存在此钩子，则在执行 helm upgrade 命令时将其删除，然后创建新 Job。hook-succeeded 表示 Job 执行成功后，删除该 Job。 第十九行执行 dump MongoDB 数据，通过 mongodump 客户端命令，连接到 MongoDB 服务，把数据 dump 到用于备份的 PVC。 第二十六行定义了 backup PVC，PVC 由 Job 挂载，存储 dump 出的文件。 遵循以上步骤，就能完成图表的 pre-upgrade 钩子了。 5.4.3 创建 pre-rollback 钩子 通过 pre-upgrade 钩子成功实现 MongoDB 数据备份后，我们再来编写一个 pre-rollback 钩子，将备份的数据还原到 MongoDB 数据库。 按照以下步骤创建 pre-rollback 钩子： 创建 template/restore 文件夹，存放 pre-rollback 钩子： [aiops@aiops0113 helm-charts]$ mkdir player-stats/templates/restore 创建 job.yaml 模板文件，用于恢复 MongoDB 数据： [aiops@aiops0113 helm-charts]$ touch player-stats/templates/restore/job.yaml 编辑 job.yaml 文件，也可以从 https://github.com/weiwendi/learn-helm/blob/main/helm-charts/charts/player-stats/templates/restore/job.yaml 页面复制内容： NaN apiVersion: batch/v1 kind: Job metadata: name: '{{ include \"player-stats.fullname\" . }}'-restore labels: '{{- include \"player-stats.labels\" . | nindent 4 }}' annotations: \"helm.sh/hook\": pre-rollback \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded spec: template: spec: containers: - name: restore image: redis:alpine3.11 command: [\"/bin/sh\", \"-c\"] args: [\"cd /backup/ && mongorestore -h '{{ .Values.mongodb.fullnameOverride }}':'{{ .Values.mongodb.service.port }}'\"] volumeMounts: - name: backup mountPath: /backup restartPolicy: Never volumes: - name: backup persistentVolumeClaim: claimName: '{{ .Values.mongodb.fullnameOverride }}'-data-backup-'{{ .Release.Revision }}' NaN 代码释义： 第九行声明了该资源为 pre-rollback 钩子。 执行数据恢复是在 18 和 19 行。18 行切换到存放备份文件的目录；第 19 执行 mongorestore 命令恢复数据。 第 27 行定义了备份卷，备份卷由回滚的版本号确定。 pre-upgrade 和 pre-rollback 钩子都创建完成了，让我们在 minikube 环境中运行它们，看看效果。 5.5 执行生命周期的钩子 为了运行创建好的钩子，需要通过 helm install 命令再次安装图表： [aiops@aiops0113 helm-charts]$ helm install myplayer-stats player-stats -n website 当每个 Pod 都处于 1/1 REDAY 状态时，我们就可以按照提示访问 player-stats 应用程序了。当然这次的端口可能会有变化。 我们在 player-stats 的页面上提交一条消息：球员姓名 Stephen Curry，球员号码 30，然后执行 helm upgrade 命令触发 pre-upgrade 钩子： [aiops@aiops0113 helm-charts]$ helm upgrade myplayer-stats player-stats -n website 命令执行成功后，可以看到两个 PVC： [aiops@aiops0113 helm-charts]$ kubectl get pvc -n website NAME STATUS VOLUME mongo Bound pvc-8451cb95-7272-4939-a85f-9f26bba4dd60 mongo-data-backup-1 Bound pvc-f6d41be0-c429-47e1-9e54-11dcf0813649 在执行 helm upgrade 命令后，我们登录 mongo 容器，进入 mongo 数据库命令行，手动删除从页面添加的数据，模拟数据丢失： [aiops@aiops0113 helm-charts]$ kubectl get pod -n website NAME READY STATUS RESTARTS AGE mongo-6b555fdbb9-zbqfx 1/1 Running 1 20h myplayer-stats-545dc849c-2sd5k 1/1 Running 19 20h [aiops@aiops0113 helm-charts]$ kubectl exec -it mongo-6b555fdbb9-zbqfx sh -n website # 进入容器后，执行 mongo 命令 $ mongo > use stats; > db.infor.deleteOne({name: \"Stephen Curry\"}) { \"acknowledged\" : true, \"deletedCount\" : 1 } 执行完成后，刷新 player-stats 页面，你会发现 Stephen Curry 相关的信息都消失了，页面回到了最初安装时的样子。 在执行 helm upgrade 命令后，我们通过手动删除数据，模拟了更新后数据损坏的情况。现在执行 helm rollback 命令，恢复数据： [aiops@aiops0113 helm-charts]$ helm rollback myplayer-stats 1 -n website Rollback was a success! Happy Helming! 命令执行完成后，再刷新页面，你会发现 Stephen Curry 的信息又回来了。 需要注意的是，在执行生命周期命令时(helm isntall 、helm upgrade、helm rollback、hell uninstall) ，可以使用 --no-hooks 标志可以跳过钩子。 5.6 添加输入验证 使用 Helm 创建 Kubernetes 资源时，Kubernetes API 服务会自动对输入进行验证。如果 Helm 创建了无效的资源，API 服务将返回错误信息，提示资源创建失败。尽管 Kubernetes 自身会执行输入验证，但仍有图表开发人员希望在资源创建请求到达 Kubernetes API 服务前完成验证。 我们可以使用 Helm 图表中的 fail 函数实现输入验证。 5.6.1 fail 函数 如果用户提供的值无效，fail 函数可以即刻呈现出模板中的异常。在本节中，我将演示一个限制用户输入的示例。 在 player-stats 图表的 values.yaml 文件中，有一个 service.type，用于配置 service 的类型： service: type: NodePort 我们将此值设置为 NodePort，但 service 还可以是其他类型。假设你想将 service 类型限制为仅 NodePort 和 ClusterIP 这两种，可以通过 fail 函数实现。 请参照以下步骤限制 player-stats 图表中的 service 类型： 在 template/service.yaml 文件中，有一行定义了根据 service.type 的值设置 service 类型，如下所示： type: '{{ .Values.service.type }}' 在设置 service 类型前，我们应首先检查 service.type 的值是否为 NodePort 或者 ClusterIP，这可以通过在列表中正确的设置变量来实现。然后判断 service.type 的值是否包含在列表中，如果是，则继续设置 service 类型；否则终止图表渲染，并向用户发送一条错误消息，通知用户输入有效的 service 类型。 在 service.yaml 文件中添加以下内容，你也可以从 https://github.com/weiwendi/helm-charts/player-stats/templates/service.yaml 页面复制这些内容到 service.yaml 文件中： apiVersion: v1 kind: Service metadata: name: '{{ include \"guestbook.fullname\" . }}' labels: '{{- include \"guestbook.labels\" . | nindent 4 }}' spec: ## 要添加的内容 '{{- $serviceTypes := list \"ClusterIP\" \"NodePort\" }}' '{{- if has .Values.service.type $serviceTypes }}' type: { { .Values.service.type } } NaN '{{- fail \"value 'service.type' must be either 'ClusterIP' or 'NodePort'\" }}' NaN ## 要添加的内容 ports: - port: { { .Values.service.port } } targetPort: http protocol: TCP name: http selector: '{{- include \"guestbook.selectorLabels\" . | nindent 4 }}' 代码释义： 第 8 行到第 13 行是输入验证。第 8 行创建了一个名为 serviceTypes 的变量，它是一个列表，包含了我们期望的 service 类型。第 9 行到第 13 行是 if 语句块。第 9 行中的 has 函数将检查 service.type 值是否包含在 serviceTypes 中。如果是，执行第 10 行，设置 service 类型；否则将执行第 12 行，使用 fail 函数停止模板渲染，并向用户显示期望的 service 类型。 现在，我们尝试在升级 my-guestbook 版本时提供无效的 service 类型： [aiops@aiops0113 helm-charts]$ helm upgrade myplayer-stats player-stats -n website --set service.type=LoadBalancer 如果之前的配置正确，现在应该看到类似以下的消息： Error: UPGRADE FAILED: template: player-stats/templates/service.yaml:14:6: executing \"player-stats/templates/service.yaml\" at : error calling fail: value 'service.type' must be either 'ClusterIP' or 'NodePort' fail 函数是验证用户输入是否符合某一组约束的好的方法，但在某些情况下，你还需要确保用户一开始提就供了某些值。这可以通过 required 函数实现。 5.6.2 required 函数 required 与 fail 有些相似，也用于中断模板渲染。不同之处在于，required 函数确保在渲染图表模板时，值不为空。 回想一下，图表包含了名为 image.repository 的值，如下所示： image: repository: registry.cn-beijing.aliyuncs.com/sretech/player-stats 这个值指定了我们要部署的镜像，考虑到这个值对图表的重要性，我们可以使用 required 函数，强制要求在安装图表时指定该值。尽管我们在图表中提供了一个默认值，但是如果你想确保用户始终使用自己的镜像，required 函数允许你删除这个默认值。 按照以下步骤，为 image.repository 设置 required 函数： template/deployment.yaml 文件包含了通过 image.repository 设置 image 的行，如下所示： image: '{{ .Values.image.repository }}:{{ .Chart.AppVersion }}' required 函数使用以下两个参数： 显示是否提示该值的错误消息 必须提供的值。 给定这两个参数，需要修改 deployment.yaml 文件中 image.repository 的值： containers: - name: {{ .Chart.Name }} securityContext: {{- toYaml .Values.securityContext | nindent 12 }} image: \"{{ required \"value 'image.repository' is required\" .Values.image.repository }}:{{ .Chart.AppVersion }}\" imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - name: http containerPort: 80 protocol: TCP 完整示例请参考 https://github.com/weiwendi/learn-helm/blob/main/helm-charts/charts/player-stats/templates/deployment.yaml 页面。 我们给 image.repository 提供一个空值，看看会显示什么结果： [aiops@aiops0113 helm-charts]$ helm upgrade myplayer-stats player-stats -n website --set image.repository='' 如果之前修改成功，你会看到类似以下的错误提示： Error: UPGRADE FAILED: execution error at (player-stats/templates/deployment.yaml:34:21): value 'image.repository' is required 至此，你已经成功开发了一个 Helm 图表，并完成了生命周期钩子和输入验证！ 在下一节中，我将介绍如何在 GitHub 页面创建一个简单的 Helm 图表存储库，用于向其他用户公开你的 player-stats 图表。 5.7 上传图表到图表存储库 你已完成了 player-stats 图表的开发，可以将图表发布到存储库中，以便其他用户使用它。我们先来创建一个图表存储库。 5.7.1 创建图表存储库 图表存储库包含两个组件： 图表的 tgz 归档文件 index.yaml 文件，包含了存储库中包含的图表元数据 基础的图表存储库需要维护人员生成 index.yaml 文件，而更复杂的解决方案(如 Helm 社区的 ChartMuseum 工具)则在推送新图表到存储库时动态生成 index.yaml 文件。在本示例中，我们将在 GitHub 页面创建一个简单的图表存储库，存储 player-stats 图表。 创建 GitHub 存储库，需要有 GitHub 账号，如果你已有账号，可以在 https://github.com/login 页面登录；如果没有，需要在 https://github.com/join 页面创建。 登录 GitHub 后，请按照以下步骤创建图表存储库： 在 https://github.com/new 页面创建新的存储库 存储库名称 charts 选中 “Add a README file” 前的复选框，这是必须的，因为 GitHub 不允许创建不包含任何内容的静态站点 我们把存储库设为 Public，这样其他用户就可以访问了 点击 “Create repository” 按钮，创建存储库 创建完成后，进入到存储库的页面，点击 “Code“ 按钮，会出现该存储库的 URL，通过此 URL 将存储库克隆到本地计算机： $ git clone $REPOSITORY_URL 克隆完成后，继续进行下一步，将 player-stats 图表发布到刚创建的图表存储库。 5.7.2 发布 player-stats 图表 Helm 提供了几个命令用于发布图表到存储库。但在执行这些命令前，你可能需要在 Chart.yaml 文件中添加 version 字段的值。版本控制是发布图表过程中一个很重要的环节，发布其它类型的软件也是如此。 修改 Chart.yaml 文件中 version 字段的值为 1.0.0： version: 1.0.0 版本修改后，我们把图表打包为一个 tgz 归档文件。在 player-stats 目录的上级目录中执行此命令： [aiops@aiops0113 helm-charts]$ helm package player-stats Successfully packaged chart and saved it to: /home/aiops/learn-helm/helm-charts/charts/player-stats-1.0.0.tgz 命令执行成功后，当前目录下会多出一个名为 player-stats-1.0.0.tgz 的归档文件。 当依赖有其他图表时，需要将这些依赖项下载到 charts/ 目录中，再执行 helm package 命令进行打包。如果 helm package 命令执行失败，请检查 MongoDB 依赖项是否已下载到 charts/ 目录中。如果没有下载，你可以使用 --dependency-update 标志，告知打包命令先下载依赖项，再打包： [aiops@aiops0113 helm-charts]$ helm package --dependency-update player-stats 图表打包后，使用 cp 命令将 tgz 文件拷贝到 GitHub 图表存储库的克隆目录中： $ cp guestbook-1.0.0.tgz $GITHUB_CHART_REPO_CLONE 复制完成后，可以使用 helm repo index 命令为 Helm 存储库生成 index.yaml 文件。此命令将 GitHub 图表存储库的克隆目录作为参数，命令如下： $ helm repo index $GITHUB_CHART_REPO_CLONE [aiops@aiops0113 charts]$ helm repo index ../charts 执行完成后，在 charts/ 目录中会生成 index.yaml 文件。index.yaml 文件包含了 player-stats 图表的元数据。如果 charts/ 目录中还有其它图表，则它们的元数据也将记录在 index.yaml 文件中。 有了 tgz 和 index.yaml 文件，我们就可以使用 git 命令将这些文件推送到 GitHub 了： $ git add --all $ git commit -m 'feat: adding the player-stats helm chart' $ git push origin main 执行 push 命令可能会要求你输入 GitHub 用户名密码，push 命令执行完成后，player-stats 图表就发布在了 GitHub 了。 接下来，我们将 Helm 图表存储库添加到本地 Helm 客户端。 5.7.3 添加图表存储库 在添加图表存储库之前，我们需要确定它的 URL，这个 URL 在 “Settings” 选项卡中的 “Pages” 页面可以创建或查看。 一旦知道了图表存储库的 URL，就可以使用 helm repo add 命令在本地添加存储库了： [aiops@aiops0113 charts]$ helm repo add aiops https://weiwendi.github.io/charts 执行完成后，本地 helm 客户端就可以与 “aiops” 存储库交互了。通过本地已经配置的 “repo” 搜索 player-stats 图表，验证图表是否发布完成： [aiops@aiops0113 charts]$ helm search repo player-stats NAME CHART VERSION APP VERSION DESCRIPTION aiops/player-stats 1.0.0 0.1.0 A Helm chart for Kubernetes 返回的结果中赫然显示着 aiops/player-stats 图表。 本章的所有试验都已经完成了，祝贺你又 get 了新技能。最后，让我们把试验环境清理一下。 5.8 清理环境 可以通过删除命名空间来清理环境： $ kubectl delete ns website 最后使用 minikube stop 命令停止 Minikube 集群。 By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-26 18:17:32 "},"helm/ceshicharts.html":{"url":"helm/ceshicharts.html","title":"测试 Helm 图表","keywords":"","body":"6. 测试 Helm 图表 测试是软件开发过程中必不可少的环节。对软件进行测试，既可以验证功能，又可以防止软件在运行过程中潜在的 Bug。实践证明，经过良好测试的软件更易于维护，开发人员也可以更自信地向用户提供新功能。 我们开发的 Helm 图表也应该经过适当的测试，以确保达到了预期。这也是本章的重点，我会在本章讨论测试 Helm 图表的方法。 6.1 环境准备 还是老套路，咱们先把部署 Helm 图表的环境准备好： 启动 Minikube： [aiops@aiops0113 ~]$ minikube start 创建 website 命名空间： [aiops@aiops0113 ~]$ kubectl create ns website Minikube 环境就绪后，我们开始讨论如何测试 Helm 图表。 6.2 验证 Helm 模板 在上一章中，我们从头构建了一个 Helm 图表。在开发完成后，最终产品还是相当复杂的：包括参数化、条件模板、生命周期钩子。由于 Helm 的主要目的之一是创建 Kubernetes 资源，因此在将资源模板应用到 Kubernetes 集群前，应该确保这些模板都已正确生成。可以通过多种方式验证模板，我会在下一节具体讨论。 6.2.1 使用 helm template 验证模板 验证图表模板的第一种方法是使用 helm template 命令，该命令可以在本地渲染图表模板，并在标准输出中显示模板渲染后的结果。 helm template 命令语法如下： $ helm template [NAME] [CHART] [flags] NAME 参数用来指定 release 名称，CHART 参数用来指定图表库。我们使用存储库中的 aiops/player-stats 作为示例，演示 helm template 命令。aiops/player-stats 就是咱们在上一章从 GitHub 添加到本地的图表存储库。 运行以下命令在本地渲染 player-stats 图表： [aiops@aiops0113 ~]$ helm template myplayer-stats aiops/player-stats -n website --- # Source: player-stats/charts/mongodb/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: mongo namespace: website labels: app.kubernetes.io/name: mongodb helm.sh/chart: mongodb-10.15.2 app.kubernetes.io/instance: myplayer-stats app.kubernetes.io/managed-by: Helm secrets: - name: mongo ... 该命令会打印出应用到 Kubernetes 集群的所有资源，上面的内容截取了 helm template 命令输出内容的开始部分，显示了一个渲染后的 ServiceAccount 资源类型。在本地渲染这些资源，开发人员可以清晰的看到创建的资源和规范。 在图表开发过程中，你可能会经常使用 helm template 命令验证是否正确生成了 Kubernetes 资源。 需要验证的常见内容如下： 参数化的字段是否被默认值或者覆盖值成功替换 流程控制语句，如 if、range 和 with 是否成功地根据提供的值生成了 YAML 文件 资源是否包含适当的间距和缩进 是否使用函数和管道正确格式化和操作 YAML 文件 诸如 required 和 fail 之类的函数能否根据用户输入正确地验证值 了解了在本地渲染图表模板后，我们再来看看在一些特定方面如何使用 helm template 命令进行测试和验证。 6.2.2 模板参数化测试 检查模板中的参数是否被正确的值填充，也是很重要的一项测试。因为图表可能包含多个值，如果值不合理、或者没有提供值，会导致图表渲染失败。 试想以下部署： apiVersion: apps/v1 kind: Deployment replicas: {{ .Values.replicas }} ports: - containerPort: {{ .Values.port }} replicas 和 port 的默认值在 values.yaml 文件中： replicas: 1 port: 8080 针对此模板资源，执行 helm template 命令，模板渲染结果如下，replaces 和 port 的值都被替换成了 values.yaml 文件中定义的默认值： apiVersion: apps/v1 kind: Deployment replicas: 1 ports: - containerPort: 8080 根据输出结果，我们可以验证参数是否被默认值正确替换了。还可以在执行 helm template 命令时通过 --values 或 --set 传递值，验证参数是否被成功覆盖： $ helm template my-chart $CHART_DIRECTORY --set replicas=2 执行结果中涵盖了我们提供的值： apiVersion: apps/v1 kind: Deployment replicas: 2 ports: - containerPort: 8080 使用 helm template 测试默认值很容易，难的是测试特定值，因为这种情况下，提供了无效值会导致图表安装失败。可以结合 required 和 fail 函数完成验证。 假设有这么一个 Deployment 模板： apiVersion: apps/v1 kind: Deployment replicas: {% raw %}{{ .Values.replicas }}{% endraw %} containers: - name: main image: {% raw %}{{ .Values.imageRegistry }}{% endraw %}/{% raw %}{{ .Values.imageName }}{% endraw %} ports: - containerPort: {{ .Values.port }} 如果此 deployment 和我们上面的代码块使用同一个 values.yaml 文件，并且在安装图表时，你希望用户提供 imageRegistry 和 imageName 的值。我们在执行 helm template 命令时不提供这些值，执行结果就和我们期望的不一样了，会输出如下内容： apiVersion: apps/v1 kind: Deployment replicas: 1 containers: - name: main image: / ports: - containerPort: 8080 通过 helm template 命令，我们发现因为没有传入值，deployment 模板被渲染后，image 的值是 /，这是一个无效值。为了避免所需值没有被正确定义的情况发生，我们可以使用 required 函数进行验证： apiVersion: apps/v1 kind: Deployment replicas: {% raw %}{{ .Values.replicas }}{% endraw %} containers: - name: main image: {% raw %}{{ required 'value 'imageRegistry' is required' .Values.imageRegistry }}{% endraw %}/{% raw %}{{ required 'value 'imageName' is required' .Values.imageName }}{% endraw %} ports: - containerPort: {% raw %}{{ .Values.port }}{% endraw %} 更新图表后，再次执行 helm template 命令，会返回一个错误信息，提示用户缺少了值： $ helm template my-chart $CHART_DIRECTORY Error: execution error at (player-stats/templates/deployment.yaml:34:21): value 'image.repository' is required 可以通过为 helm template 命令传递一个值文件，来进一步测试。对于此示例，我们假设值文件的内容如下： imageRegistry: my-registry.example.com imageName: learnhelm/my-image 将用户定义的值文件传递给 helm template 的命令如下： $ helm template my-chart $CHART_DIRECTORY --values my-values.yaml 这次模板渲染后的内容，image 的值就是值文件中提供的值了。 在进行参数化测试时，请确保验证了图表中使用的每个值，对于无法在 values.yaml 文件中提供的默认值，可以使用 required 函数验证。使用 helm template 命令确保正确渲染并生成了所需的 Kubernetes 资源配置。 顺便说一句，可以在 values.yaml 文件中为必填字段添加注释，这样用户在查看 values.yaml 文件时就能知道哪些字段是必须的： replicas: 1 port: 8080 ## REQUIRED imageRegistry: ## REQUIRED imageName: 注释内容是不会出现在 helm template 命令的输出中的。 接下来，我们探讨如何在本地生成图表模板来帮助我们测试图表的控制操作。 6.2.3 测试控制操作 除了测试参数化外，你还应该使用 helm template 命令验证控制操作(特别是 if 和 range)是否得到了正确的处理，以产生所需的结果。 假设有以下 deployment 模板： apiVersion: apps/v1 kind: Deployment {{- range .Values.env }} env: - name: {% raw %}{{ .name }}{% endraw %} value: {% raw %}{{ .value }}{% endraw %} {{- end }} {{- if .Values.enableLiveness }} livenessProbe: httpGet: path: / port: {% raw %}{{ .Values.port }}{% endraw %} initialDelaySeconds: 5 periodSeconds: 10 {{- end }} ports: containerPort: 8080 当 env 和 enableLiveness 的值都为 null 时，通过 helm template 命令来测试模板渲染是否成功： $ helm template my-chart $CHART_DIRECTORY --values my-values.yaml --- # Source: test-chart/templates/deployment.yaml apiVersion: apps/v1 kind: Deployment ports: - containerPort: 8080 可以看到 range 和 if 操作没有生成任何内容，这是因为 range 子句不会对 Null 或空值执行任何操作，即使提供给 if 执行，也会被当做 false。通过向 helm template 提供 env 和 enableLiveness 的值，可以验证已经编写好的模板能否正确生成 YAML。 可以将这些值添加到 values 文件中，如下所示： env: - name: weChatPublicId value: sretech enableLiveness: true 完成这些更改后，使用这些值验证 helm template 命令的执行结果，以证明模板是正确编写的： --- # Source: test-chart/templates/deployment.yaml apiVersion: apps/v1 kind: Deployment env: - name: weChatPublicId value: sretech livenessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 5 periodSeconds: 10 ports: - containerPort: 8080 在向图表中添加其他控制结构时，你应该确保使用 helm template 命令及时对模板渲染进行了测试，因为这些结构会增大图表开发的难度，特别是控制结构比较多或比较复杂时。 除了检查控制结构是否正确生成之外，你还应该检查用到的函数和管道是否按计划工作。 6.2.4 测试管道和函数 helm template 命令也可用于测试管道和函数的渲染，这些管道和函数通常用于生成格式化的 YAML。 以下面的模板为例： apiVersion: apps/v1 kind: Deployment resources: {% raw %}{{ .Values.resources | toYaml | indent 12 }}{% endraw %} 该模板使用管道格式化和参数化资源的值，以满足容器的需求。比较合理的做法是图表的 values.yaml 文件中设置一些合理的默认值，确保应用程序都有适当的资源限制，避免滥用集群资源造成资源浪费。 包含 resources 值的模板示例如下： resources: limits: cpu: 200m memory: 256Mi 执行 helm template 命令，查看这些值是否按照正确的缩进生成了有效的 YAML 文件。输出如下： apiVersion: apps/v1 kind: Deployment resources: limits: cpu: 200m memory: 256Mi 接下来，我们将讨论在使用 helm template 渲染资源时如何启用服务器端验证。 6.2.5 为图表渲染添加服务器端验证 虽然 helm template 命令对于开发图表很有帮助，并且在图表开发过程中你应该经常用它验证图表的渲染，但它也有一个很大的限制。helm template 主要用于客户端本地渲染，不与 Kubernetes API 进行交互验证资源。如果你想确保生产的资源是有效的，可以使用 --validate 标志指示 helm template 在资源生成后与 Kubernetes API 进行通信： $ helm template my-chart $CHART_DIRECTORY --validate 任何生成的 Kubernetes 模板，如果不是有效的 Kubernetes 资源，都会在错误信息中打印出来。假设，在一个 deployment 模板中 apiVersion 的值设为了 aipVersion: v1。我们知道，在 Kubernetes API 中，apiVersion 的值是 apps/v1。所以这是一个错误的资源，使用 --validate 标志，我们会看到下面的错误： Error: unable to build kubernetes objects from release manifest: unable to recognize '': no matches for kind 'Deployment' in version 'v1' --validate 用于打印生成的错误资源，使用此标志需要有访问 Kubernetes 的权限。 在执行 install/upgrade/rollback/uninstall 命令时，也可以使用 --dry-run 标志验证。 下面是使用 install 命令使用此标志的示例： $ helm install my-chart $CHART --dry-run 此标志会生成图表的模板，并执行验证，类似于 helm template --validate 命令。--dry-run 会把生成的每个资源打印到终端，而不会在 Kubernetes 环境中创建这些资源。主要用于在用户最终安装前，执行此命令以确保是按期望进行安装的。图表开发人员可以使用 --dry-run 标志测试图表的渲染，也可以使用 helm template --validate 命令进行验证。 验证了模板是否按期望生成，我们也需要确保模板是最优的，以降低开发和维护成本。Helm 提供了 helm lint 命令，可用于此，我们在下一节探讨。 6.2.6 helm lint 对图表进行整理，能有效防止图表格式或图表定义文件出错，并在使用 Helm 图表时提供最佳实践指导。helm lint 命令语法如下： $ helm lint PATH [flags] helm lint 命令需要指定图表目录，用来验证该图表的格式是否正确。 helm lint 命令不验证已渲染的 API 模式，也不检查 YAML 格式，而只是检查图表中是否包含有效的 Helm 图表应该包含的文件和设置。 可以针对 player-stats 图表执行 helm lint 命令，在 helm-charts 目录下执行以下命令： [aiops@aiops0113 helm-charts]$ helm lint player-stats ==> Linting player-stats [INFO] Chart.yaml: icon is recommended 1 chart(s) linted, 0 chart(s) failed 1 chart(s) linted, 0 chart(s) failed 说明图表是有效的。[INFO] 消息建议图表在 values.yaml 文件中包含一个 icon 字段，但这不是必需的。其它的消息类型有 [WARNING]，表示违反了图表惯例；[ERROR] 表示图表会安装失败。 让我们看几个例子。假设有一个结构如下的图表： player-stats/ templates/ values.yaml 这个图表结构是有问题的，缺少了定义图表元数据的 Chart.yaml 文件，对这样的图表执行 lint 命令，会有以下报错： ==> Linting . Error unable to check Chart.yaml file in chart: stat Chart.yaml: no such file or directory Error: 1 chart(s) linted, 1 chart(s) failed 错误信息显示在图表中找不到 Chart.yaml 文件。我们试着在图表中创建一个空的 Chart.yaml 文件，再次执行看看是什么情况： ==> Linting . [ERROR] Chart.yaml: name is required [ERROR] Chart.yaml: apiVersion is required. The value must be either 'v1' or 'v2' [ERROR] Chart.yaml: version is required [INFO] Chart.yaml: icon is recommended [ERROR] templates/: validation: chart.metadata.name is required Error: 1 chart(s) linted, 1 chart(s) failed 这段输出显示了 Chart.yaml 文件中缺少的必要的字段：name、apiVersion、version，这些字段是 Chart.yaml 文件中的必要字段，用来生成有效的 Helm 图表。helm lint 还反馈了额外的信息，apiVersion 的值必须是 v1 或 v2，version 字段的值是否为适当的 Semver 版本。lint 还会检查其他需要的文件，比如 values.yaml 和 template 目录。它还会确保 template 目录下的文件的扩展名是否为 .yaml/.yml/.tpl/.txt。helm lint 对于检查图表的内容很有用，但不会检查文件中具体的 YAML 格式。 要执行 YAML 检查，可以使用 yamllint 工具，该工具可以在 https://github.com/adrienverge/yamllint 上找到。可以通过 pip 包管理器安装： pip install yamllint --user 它也可以使用操作系统的包管理器安装，如 https://yamllint.readthedocs.io/en/stable/quickstart.html 上的 yamllint 快速入门所述。 rockyLinux 安装 yamllint： [root@aiops0113 ~]# dnf install -y epel-release [root@aiops0113 ~]# dnf install -y yamllint 为了在图表 YAML 资源上使用 yamllint，必须结合 helm template 命令渲染 Go 模板并生成 YAML 资源。因为图表的 YAML 资源中包含了 Go 模板，所以 yamllint 需要和 helm template 结合使用。下面是对 GitHub 包存储库中的 player-stats 图表运行 yamllint 命令的示例： [aiops@aiops0113 helm-charts]$ helm template myplayer-stats player-stats |yamllint - stdin 83:1 warning comment not indented like content (comments-indentation) 124:1 error trailing spaces (trailing-spaces) 128:1 error trailing spaces (trailing-spaces) 142:1 error trailing spaces (trailing-spaces) 191:81 error line too long (141 > 80 characters) (line-length) 203:23 error trailing spaces (trailing-spaces) 该命令将 templates/ 目录中的模板文件渲染成 Kubernetes 的资源文件，并将输出通过管道传递给 yamllint 命令。 这些是 yamllint 输出的 Helm 模板的内容，虽然有行号，但要和 YAML 资源文件中的行对应上，我们还需通过以下命令： [aiops@aiops0113 helm-charts]$ cat -n yamllint 可以根据以下规则进行 YAML 检查： 缩进 行长度 空格 空行 注释格式 可以通过创建以下文件之一设置规则，从而覆盖默认规则： .yamllint, .yamllint.yaml, and .yamllint.yml ，当前工作目录下 $XDB_CONFIG_HOME/yamllint/config ~/.config/yamllint/config 要重写针对 player-stats 图表的缩进规则报告，可以在当前目录下创建 .yamllint.yaml 文件，文件内容如下： rules: indentation: # Allow myList # - item1 # - item2 # Or # myList # - item1 # - item2 indent-sequences: whatever 这个配置会覆盖 yamllint，这样在添加列表条目时，就不会强制一个特定的缩进方法了。这由 indent-sequences: whatever 行定义。创建该文件后，再次执行 yamllint，就不会有缩进报错了： $ helm template myplayer-stats player-stats | yamllint - 在本节中，我们讨论了如何通过 helm template 和 helm lint 命令，验证图表的本地渲染。然而，这实际上并没有测试图表的功能或使用图表创建的应用程序资源的运行情况。 在下一节中，我们将学习如何在 Kubernetes 环境中创建测试，以测试 Helm 图表能否正常安装并运行。 6.2.7 在集群中进行测试 为图表创建测试，是维护和开发图表的重要组成部分。图表测试有助于验证图表是否按照预期运行，避免在添加功能或修复程序时回退。 测试包括两个步骤。首先需要在图表的 templates/ 目录下创建包含 helm.sh/hook: test 注解的 Pod 模板，这些 Pod 用于执行测试图表和应用程序功能的命令。第二步需要执行 helm test 命令，它会使用第一步中的注解创建测试钩子。 在这一节中，我们通过在 player-stats 图表中添加测试，来学习如何在 Kubernetes 集群中进行测试。参考样例在 https://github.com/weiwendi/learn-helm/tree/main/helm-charts/charts/player-stats 代码仓库中。 首先在 templates/test/ 目录中添加 frontend-connection.yaml 和 mongodb-connection.yaml 文件。图表测试虽然不必放在 test 子目录下，但将它们放在那里既能保持测试组织有序，又能与图表主模板分离： [aiops@aiops0113 helm-charts]$ mkdir player-stats/templates/test [aiops@aiops0113 helm-charts]$ touch player-stats/templates/test/frontend-connection.yaml [aiops@aiops0113 helm-charts]$ touch player-stats/templates/test/mongodb-connection.yaml 接下来，我们向这两个文件中添加内容，以验证其关联的应用程序组件。 6.2.7.1 创建图表测试 Player-stats 图表由 Go 前端和 MongoDB 后端组成，用户在前端对话框中输入信息，提交后被存储到后端。我们写几个测试，确保安装后前后端可用。先从 MongoDB 开始，编辑 templates/test/mongodb-connection.yaml 文件，添加以下内容，这些内容也可以在 https://github.com/weiwendi/learn-helm/blob/main/helm-charts/charts/player-stats/templates/test/mongodb-connection.yaml 页面查看： apiVersion: v1 kind: Pod metadata: name: {% raw %}{{ include \"player-stats.fullname\" . }}{% endraw %}-test-mongodb-connection labels: {% raw %}{{- include \"player-stats.labels\" . | nindent 4 }}{% endraw %} annotations: \"helm.sh/hook\": test \"helm.sh/hook-delete-policy\": before-hook-creation spec: containers: - name: test-mongodb-connection image: bitnami/mongodb:4.4.6-debian-10-r0 # image: registry.cn-beijing.aliyuncs.com/sretech/mongo:4.4.6 command: [\"/bin/sh\", \"-c\"] args: ['mongo --host {% raw %}{{ .Values.mongodb.fullnameOverride }}{% endraw %}:{% raw %}{{ .Values.mongodb.service.port }}{% endraw %} --quiet --eval \"db.infor.find()\" stats'] restartPolicy: Never 该模板定义了在 test 生命周期钩子期间创建 Pod，模板中还定义了一个删除钩子策略，用来指示何时删除以前的 test Pod。 containers 对象下的 args 字段是用于测试的命令，使用 mongo 客户端工具连接到 MongoDB，并执行 find() 命令。前端会把用户提交的信息存储为 MongoDB 的文档消息。这个简单的测试，旨在检查是否可以连接到 MongoDB 数据库，它返回的结果和用户通过页面查询到的内容是相同的。 再来配置前端的可用性测试，因为它是直面用户的。编辑 templates/test/frontend-connection.yaml 文件添加以下内容，这些内容也可以在 https://github.com/weiwendi/learn-helm/blob/main/helm-charts/charts/player-stats/templates/test/frontend-connection.yaml 页面查看： apiVersion: v1 kind: Pod metadata: name: {% raw %}{{ include \"player-stats.fullname\" . }}{% endraw %}-test-frontend-connection labels: {% raw %}{{- include \"player-stats.labels\" . | nindent 4 }}{% endraw %} annotations: \"helm.sh/hook\": test \"helm.sh/hook-delete-policy\": before-hook-creation spec: containers: - name: test-frontend-connection image: curlimages/curl:7.68.0 command: [\"/bin/sh\", \"-c\"] args: ['curl {% raw %}{{ include \"player-stats.fullname\" . }}{% endraw %}'] restartPolicy: Never 这是一个非常简单的测试，对 player-stats 服务发起 HTTP 请求，来检查前端是否可用。 到此，我们完成了图表测试所需的模板。当然，这些模板可以使用 helm template 命令在本地渲染，也可以使用 helm lint 和 yamllint 进行规范，就像本章之前介绍的。在开发图表时，这种更高级的测试，有时会非常有用。 测试已经写好了，我们在 Minikube 环境中运行它们。 6.2.7.2 执行图表测试 要执行图表测试，必须先执行 helm install 命令在 Kubernetes 集群中安装图表。因为我们编写的测试用例是在安装完成后运行的。可以在安装图表时使用 --wait 标志，方便确定 Pod 何时准备就绪： [aiops@aiops0113 helm-charts]$ helm install myplayer-stats player-stats -n website --wait 一旦图表安装完成，就可以使用 helm test 命令执行 test 生命周期钩子并创建测试资源。 helm test 命令的语法如下： helm test [RELEASE] [flags] 对已安装的 myplayer-stats release 进行测试： [aiops@aiops0113 helm-charts]$ helm test myplayer-stats -n website 如果测试成功，你将在输出中看到以下内容： NAME: myplayer-stats LAST DEPLOYED: Wed Jun 16 02:42:49 2021 NAMESPACE: website STATUS: deployed REVISION: 1 TEST SUITE: myplayer-stats-test-frontend-connection Last Started: Wed Jun 16 02:44:54 2021 Last Completed: Wed Jun 16 02:44:56 2021 Phase: Succeeded TEST SUITE: myplayer-stats-test-mongodb-connection Last Started: Wed Jun 16 02:44:56 2021 Last Completed: Wed Jun 16 02:44:58 2021 Phase: Succeeded 在运行测试时，可以使用 --logs 标志，将执行测试时的日志打印到命令行。使用 --logs 标志，再次运行测试： [aiops@aiops0113 helm-charts]$ helm test myplayer-stats -n website --logs 这次的输出中，除了与前面相同的输出内容外，还输出了进行测试的容器日志，这些额外的输出，和使用 kubectl logs 看到的是一样的。以下是测试前端连接的部分日志内容： POD LOGS: myplayer-stats-test-frontend-connection % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 球员统计 下面是测试后端连接的部分日志输出： POD LOGS: myplayer-stats-test-mongodb-connection 后端测试的日志内容是空的，因为我们尚未向 player-stats 提交任何信息。可以在前端页面提交信息后，再次执行此测试。 在本小节，我们编写了两个简单的测试用例，对安装的图表进行冒烟测试。这些测试帮助我们确认修改后的图表，能够正常运行。 在下一节中，我们来讨论如何使用 ct 工具改进测试过程。 6.3 Chart testing project 我们已经编写了测试 player-stats 应用程序是否被成功安装的测试用例，但这些测试用例在执行的过程中，是有一些限制的。 第一个限制是如何测试图表值可能出现的不同排列。helm test 命令无法修改 release 的值，不能测试安装或更新时未指定的值，因此在对不同的值执行 helm test 时，需要遵循以下流程： 使用一组初始值安装图表。 针对 release 执行 helm test。 删除执行完 helm test 的 release。 使用另一组值安装图表。 重复 2 - 4 步，直到对大部分值进行了测试。 除了值的不同排列外，还应确保对图表的修改不会发生回退。在测试图表的较新版本时，防止回退的最佳方式是按照以下流程执行： 安装之前的图表版本。 将 release 升级到较新的图表版本。 删除 release。 安装较新的图表版本。 针对排列组合中的每一组值重复此流程，确保对图表的修改不会发生回退或中断。 这些过程很乏味，尤其是在维护众多不同的 Helm 图表时，图表开发人员可能会承受额外的压力，在这些图表上应该进行仔细的测试。在维护多个 Helm 图表时，图表开发人员倾向于使用 git monorepo 设计，也就是把多个或所有的图表存储在一个 git 仓库中。 一个存储 Helm 图表的 monorepo 文件结构类似如下： helm-charts/ player-stats/ Chart.yaml templates/ README.md values.yaml mongodb/ wordpress/ README.md Monorepo 中的 Helm 图表在修改后应进行测试，以确保修改后的图表能正常运行。修改图表时，Chart.yaml 文件中的 version 字段也应该按照 SemVer 规范进行增加，以表示本次代码更改的程度。SemVer 规范遵循 MAJOR.MINOR.PATCH 版本编号格式。 以下是增加 SemVer 版本号的操作指南： 对图表进行突破性的更改需要增加 MAJOR 版本号。突破性的更改是指当前做的修改会使图表不向后兼容。 添加一个非突破性的功能，与之前的版本兼容，则应增加 MINOR 版本号。 对错误或安全漏洞进行修补，且更改后与之前的图表版本向后兼容，则应增加 PATCH 版本号。 如果没有完善的自动化工具，很难确保每次修改图表时都进行了测试，并且版本号也做了增加，尤其是在维护存储了多个 Helm 图表的 monorepo 时。这一挑战促使 Helm 社区创建了 ct 项目，用于在测试和维护图表时提供结构化和自动化的功能。 6.3.1 Chart testing project 介绍 图表测试项目(chart testing project，可以在 https://github.com/helm/chart-testing 页面找到)用来对 git monorepo 中的图表执行自动检测、验证和测试。自动化测试是通过 git 检测图表目标分支的更改来实现的。更改的图表应该经过测试，而没有更改的图表则不需要测试。 ct 提供了四个主要命令： lint：检查并验证更改的图表。 install：安装和测试更改的图表。 lint-and-install：梳理、安装和测试更改的图表。 list-changed：列出更改的图表。 list-changed 命令不执行任何验证或测试，lint-and-install 是 lint 命令和 install 命令的组合，它还会检查每个 Chart.yaml 文件的 version 字段是否增加了。内容更改了，但 version 没有增加的图表，将测试失败。这种验证有助于维护人员根据所做更改的类型严格地修改图表版本。 除了检查图表版本外，chart testing 还可以对多个值文件进行测试。在调用 lint、install、lint-and-install 命令时，chart testing 循环遍历每个需要测试的值文件，并根据提供的不同值排列执行验证和测试。为了区分这些值文件与图表的 values.yaml 文件，需要将测试的值文件存放在名为 ci/ 的文件夹下，文件结构如下： player-stats/ Chart.yaml ci/ nodeport-service-values.yaml ingress-values.yaml templates/ values.yaml Chart testing 会应用 ci/ 目录下的所有值文件，不论这些文件的名称是什么。根据被覆盖的值命名每个值文件是个很好的习惯，这样维护者和贡献者就能很直观的知道值文件的内容。 实际工作中，常用的 ct 命令是 lint-and-install。下面列出了该命令用于检测、安装和测试 git monorepo 中修改的图表的步骤： 检测被修改的图表。 使用 helm repo update 命令更新本地 Helm 缓存。 使用 helm dependency build 命令下载修改过的图表的依赖项。 检查每个修改过的图表 version 是否有增加。 对于在步骤 4 中计算为 true 的图表，检查 ci/ 目录中的每个值文件。 对于步骤 4 中计算为 true 的图表，执行以下步骤： 在自动创建的命名空间中安装图表。 运行 helm test 执行测试。 删除命名空间。 对 ci/ 目录下的每个值文件重复此操作。 lint-and-install 命令执行了多个操作，在命名空间中安装和测试每个修改过的图表，并对 ci/ 目录中的值文件重复此过程，以确保图表得到充分、正确的检查和测试。但是，默认情况下，lint-and-install 命令不会从图表的旧版本执行升级检查向后兼容性。可以通过 --upgrade 标志启用该特性： 如果未指定是突破性更改，则 --upgrade 标志会以以下步骤代替上面的第六步： 在自动创建的命名空间中安装旧版本的图表。 执行 helm test 对 release 进行测试。 将 release 升级到图表的修改版本，并再次执行测试。 删除命名空间。 在一个新的自动创建的命名空间中安装修改的图表版本。 执行 helm test 对 release 进行测试。 使用相同的图表版本再次升级 release，并重新执行测试。 删除命名空间。 对 ci/ 目录下的每个值文件重复上述操作。 建议在执行 chart testing 时使用 --upgrade 标志，以便对 Helm 的升级进行额外的测试，避免升级后可能出现的回退。 如果增加了 Helm 图表的 MAJOR 版本，--upgrade 标志将失效，因为这表明你做了一个突破性的更改，在当前版本上进行原地升级是不会成功的。 我们在本地安装 chart testing CLI 及其依赖的工具，以便观察测试过程。 6.3.2 安装 chart testing 工具 为了使用 chart testing CLI，你必须在本地机器上安装以下工具： helm git(version 2.17.0 or later) yamllint yamale kubectl Chart testing 在测试过程中用到了这些工具。helm & kubectl & yamllint & git 我们已经使用过了，所以现在只需要安装 Yamale。yamale 是一个 chart testing 工具，用来对照 Chart.yaml 模式文件验证图表中的 Chart.yaml 文件。 Yamale 可以通过 pip 包管理器安装，如下所示： $ pip install yamale --user 你也可以通过从 https://github.com/23andMe/Yamale/archive/master.zip 下载压缩文件手动安装 Yamale。 下载后，解压压缩文件并执行安装脚本： # 此命令需要使用 root 用户执行 $ python setup.py install 这些工具安装完成后，就可以下载 chart testing 工具了 ，GitHub 下载地址：https://github.com/helm/chart-testing/releases。在该页面下载和操作系统相符的最新版本。 解压下载的文件，会有以下内容： LICENSE README.md etc/chart_schema.yaml etc/lintconf.yaml ct LICENSE、README.md 文件不是必须的，可以删除。把 etc/chart_schema.yaml、etc/lintconf.yaml 两个文件移动到 /etc/ct/ 目录中。ct 文件移动到系统的 $PATH 变量路径中，我们把它放到 /usr/local/bin/ 目录下。 $ mkdir $HOME/.ct $ mv $HOME/Downloads/etc/* /etc/ct/ $ mv $HOME/Downloads/ct /usr/local/bin/ 所有的工具都已准备完成，接下来我们演示如何对本地的图表存储库进行修改，并使用 chart testing 整理和安装修改后的图表。 如果你还没有 clone 存储库到本地主机，可以使用以下命令进行 clone： $ git clone https://github.com/weiwendi/learn-helm.git clone 完成后，你会发现，该存储库中有一个 ct.yaml 的文件，内容如下： target-branch: main chart-dirs: - helm-charts/charts chart-repos: - bitnami=https://charts.bitnami.com/bitnami - mycharts=https://weiwendi.github.io/charts validate-maintainers: false chart-dirs 字段定义了 ct 的 helm-charts/charts 目录，相对于 ct.yaml 文件是图表库 monorepo 的根。chart-repos 字段提供了一个存储库列表，chart testing 会运行 helm repo add，以确保能够下载依赖。 还有其他配置可以添加到这个文件中，这里不做讨论，可以在 https://github.com/helm/chart-testing 的 chart testing 文档中查看。每次调用 ct 命令都会引用 ct.yaml 文件。 现在工具已经安装好了，包存储库也 clone 到了本地，我们可以执行 ct lint-and-install 命令来测试图表了。 6.3.3 执行 ct lint-and-install lint-and-install 命令作用于 learn-helm/helm-charts 目录下的 Helm 图表： player-stats：这是我们前一章创建的图表。 nginx：这是通过 helm create 命令创建的、用于演示的 Helm 图表。 要执行测试，首先切换到 learn-helm 存储库的顶层目录： [aiops@aiops0113 ~]$ cd learn-helm ct.yaml 文件通过 chart-dirs 字段定义 Helm 图表的 monorepo 的位置，因此，你可以从顶层路径运行 ct lint-and-install 命令。 [aiops@aiops0113 learn-helm]$ ct lint-and-install Linting and installing charts... ------------------------------------- No chart changes detected. ------------------------------------- All charts linted and installed successfully 运行此命令后，你会看到类似上面的输出内容。 因为没有任何图表被修改，所以 ct 不会对图表执行任何操作。我们修改图表，观察 ct 的执行过程。修改图表应该在非 main 分支进行，我们先创建一个名为 test 的新分支： [aiops@aiops0113 learn-helm]$ git checkout -b test Switched to a new branch 'test' 修改范围可大可小，我们简单地修改每个图表的 Chart.yaml 文件。修改 learn-helm/helm-charts/charts/player-stats/Chart.yaml 中的 description 字段为： description: Used to deploy the player-stats application. # 原来的值为：A Helm chart for Kubernetes. 修改 learn-helm/helm-charts/charts/nginx/Chart.yaml 文件中的 description 字段为： description: Deploys an NGINX instance to Kubernetes. # 原来的值为：A Helm chart for Kubernetes. 修改这些图表后，再次运行 lint-and-install 命令： [aiops@aiops0113 learn-helm]$ ct lint-and-install Linting and installing charts... ... ✖︎ nginx => (version: \"0.1.0\", path: \"helm-charts/nginx\") > Chart version not ok. Needs a version bump! ✖︎ player-stats => (version: \"1.0.0\", path: \"helm-charts/player-stats\") > Chart version not ok. Needs a version bump! ... Error: Error linting and installing charts: Error processing charts Error linting and installing charts: Error processing charts 执行报错，因为 ct 检测到 monorepo 中的两个图表发生了更改，而 version 并没有增加。 这个问题可以通过增加 player-stats 和 nginx 图表的版本来解决。由于此更改并没有引入新功能，因此我们将增加 PATCH 的版本号。将两个图表的 Chart.yaml 文件中的 version 字段分别修改为 1.0.1： version: 1.0.1 执行 git diff 命令，确认每个图表都进行了修改后，再次执行 lint-and-install 命令： [aiops@aiops0113 learn-helm]$ ct lint-and-install 在增加了图表版本后，lint-and-install 命令遵循了完整的 chart testing 工作流。从输出中可以看到，修改过的图表被 lint 并部署到了自动创建的命名空间中。一旦部署的应用程序的 Pod 就绪，ct 会根据 helm.sh/hook: test 注解自动运行图表的测试用例。Chart testing 输出了每个测试 Pod 的日志及命名空间事件。 从 lint-and-install 的输出中， 你可能注意到了，nginx 图表部署了两次，player-stats 图表只部署和测试了一次。这是因为 nginx 图表的 ci/ 目录中，包含了两个值文件。ci/ 目录中的值文件通过 chart testing 进行迭代，有几个值文件，就会部署几次，以确保每组值的组合都能成功安装。Player-stats 图表没有 ci/ 目录，因此只安装了一次。 这些可以在 lint-and-install 输出的下面几行中观察到： Linting chart with values file 'helm-charts/charts/nginx/ci/clusterip-values.yaml'... Linting chart with values file 'helm-charts/charts/nginx/ci/nodeport-values.yaml'... Installing chart with values file 'helm-charts/charts/nginx/ci/clusterip-values.yaml'... Installing chart with values file 'helm-charts/charts/nginx/ci/nodeport-values.yaml'... 此命令仅能测试图表的功能，不能验证能否对图表进行升级。为此，我们需要在执行 lint-and-install 命令时指定 --upgrade 标志。添加 --upgrade 标志再次执行该命令： [aiops@aiops0113 learn-helm]$ ct lint-and-install --upgrade 该命令使用 ci/ 目录下的值文件原地升级 release。这可以在输出中看到，如下所示： esting upgrades of chart 'player-stats => (version: \"1.0.1\", path: \"helm-charts/charts/player-stats\")' relative to previous revision 'player-stats => (version: \"1.0.0\", path: \"ct_previous_revision074583074/helm-charts/charts/player-stats\")'... 只有 MAJOR 版本相同时，才可以执行升级。在 MAJOR 版本不同时，执行 --upgrade，会收到如下错误提示： Skipping upgrade test of 'nginx => (version: \"1.0.1\", path: \"helm-charts/charts/nginx\")' because: 1 error occurred: * 1.0.1 does not have same major and minor version as 0.1.0 测试的相关知识，到这里就介绍完了。 By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-20 15:49:28 "},"helm/helmcicd.html":{"url":"helm/helmcicd.html","title":"使用 CI/CD 和 GitOps 实现 Helm 自动化","keywords":"","body":"使用 CI/CD 和 GitOps 实现 Helm 自动化 截止到本章，我们已经讨论了两个 Helm 的高级用法。第一个是将 Helm 作为包管理器，在 Kubernetes 集群中部署各种复杂的应用程序。第二个是开发和测试 Helm 图表，将 Kubernetes 的复杂性封装在 Helm 图表中，并对图表执行测试，确保成功交付了用户所需的功能。 这两种操作，都使用了多个不同的 Helm Cli 命令，这些命令虽然能有效地完成任务，但不够有效率，需要在命令行手动执行。在管理多个图表或应用程序时，手动执行会成为瓶颈，导致环境难以维护和扩展。因此，我们应该在 Helm 的基础上提供额外的自动化功能。本章我们讨论持续集成和持续交付(CI/CD)以及 GitOps 的相关概念，这些概念是自动调用 Helm Cli 和其他命令的方法论，实现对 Git 仓库执行自动化流程，这些流程包括在图表开发过程中构建、测试和打包图表，使用 Helm 自动部署应用程序。 本章的学习重点： 理解 CI/CD 和 GitOps 设置所需环境 创建 CI 管道构建 Helm 图表 创建 CD 管道部署 Helm 图表 CI/CD 和 GitOps 介绍 迄今为止，我们讨论了许多 Helm 开发、测试、部署的相关概念，但这些仅限于手动配置和调用 Helm Cli 实现。虽然在初学 Helm 时可以这样做，但当你准备将图表部署到生产环境，在生产环境一展身手时，就不得不考虑以下两个问题了： 如何才能确保图表开发和部署遵循了最佳实践？ 新成员加入时会对开发和部署过程产生什么影响？ 这两个问题适用于任何软件项目，而不仅是 Helm 图表开发。虽然到目前为止我们已经介绍了很多最佳实践，但在有新成员加入时，他们可能对最佳实践有不同的理解，甚至在执行关键步骤时缺乏纪律性。通过使用自动化和可重复的流程，建立诸如 CI/CD 之类的概念，可以解决其中的一些挑战。 CI/CD 每次软件变更时，都期望有自动化的软件开发过程，这就是 CI 产生的背景。CI 不仅确保了软件变更过程遵循了最佳实践，而且有助于避免许多开发人员面临的共同问题：“在我的机器上就能正常运行，怎么部署在服务器上就不行了呢？”。我们之前讨论过使用版本控制系统(如 git)存储代码，通常开发人员都有自己独立的源代码副本，这就导致维护代码库会很困难，因为其他开发人员也会提交代码。 CI 可以通过自动化工具实现，每次代码发生更改时，自动化工具都会检索代码并执行预定步骤。常用的自动化工具包括 Jenkins、TeamCity，以及各种基于软件即服务的解决方案。有了这些自动化工具的加持，开发人员可以将更多精力用于代码编写，并能够频繁地提交代码，进而更快的、更安全的交付软件。 这些工具都有一个关键特性，就是能够及时报告项目的当前状态。与在软件开发周期的最后阶段发现破坏性的变更不同，通过使用 CI，一旦在代码分支中合并了变更，就会执行流程，并将结果通知到相关人员。快速通知的机制，为引入变更的开发人员提供了解决问题的时机，让这些问题在代码的开发阶段就能被发现，而不是在交付阶段。 应用程序最终交付是要运行在生产环境的，借鉴软件交付生命周期中许多 CI 的概念，诞生了 CD。CD 是一组定义好的步骤，通过发布过程(通常是管道)发布软件。CI 和 CD 经常搭配在一起，因为 CI 的执行引擎也可以实现 CD。CD 已经得到了许多组织和公司的认可，并越来越流行，在这些公司中，为了使软件的发布过程更进一步，需要实施适当的变更控制或审批。由于 CI/CD 的许多概念都是以可重复的方式实现自动化的，因此一旦团队确信他们有了一个可靠的框架，就完全不需要手动审批了。 实现构建、测试、部署和发布过程的完全自动化，而不需要任何人为干预的过程称为持续部署(continuous deployment)。虽然许多软件项目从来没有完全实现持续部署，但只要实现 CI/CD 强调的概念，团队就能够更快地产生真正的业务价值。在下一节中，我们将介绍 GitOps，用于改进应用程序的管理及其配置。 GitOps 将 CI/CD 提升到新高度 Kubernetes 支持声明式配置，和其他编程语言编写的程序一样，Kubernetes 清单也可以通过 CI/CD 管道进行管理。清单应该存储在代码仓库中(如 git)，并且每次的构建、测试、部署步骤应该相同。在 Git 存储库中管理 Kubernetes 集群配置的生命周期，然后以自动化的方式应用这些资源，这一趋势促使了 GitOps 概念的流行。GitOps 于 2017 年首次由 WeaveWorks 软件公司提出，此后作为管理 Kubernetes 配置的一种方式，它的受欢迎程度不断提高。虽然 GitOps 在 Kubernetes 的上下文中广为人知，但它的原理可以应用于任何本地云原生环境。 和 CI/CD 类似，目前已经有了很多管理 GitOps 流程的工具，如 Intuit 的 ArgoCD、WeaveWorks 的 Flux。当然，你无需特意使用专门为 GitOps 设计的工具，因为任何 CI/CD 自动化流程管理工具都可以。传统的 CI/CD 工具和为 GitOps 设计的工具之间的关键区别在于，GitOps 工具能够持续观察 Kubernetes 集群的状态，并在当前状态与 Git 库中清单所定义的状态不符时，应用所需的配置。这些工具有效利用了 Kubernetes 原生的控制器。 由于 Helm 图表最终是作为 Kubernetes 资源呈现的，因此它们也适用于 GitOps 流程，并且很多 GitOps 工具原生支持 Helm。在本章的剩余部分，我将讨论如何使用 CI/CD 和 GitOps 部署 Helm 图表，并把 Jenkins 作为 CI 和 CD 的首选工具。 设置环境 在本章中，我们将配置两个管道，演示围绕 Helm 实现不同程度的自动化。 按照以下步骤设置本地环境： 删除原来的 Minikube，启动一个 4G 内存的 Minikube： [aiops@aiops0113 ~]$ minikube delete [aiops@aiops0113 ~]$ minikube start --memory=4g --image-mirror-country='cn' \\ --iso-url=https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/iso/minikube-v1.20.0.iso 在 Minikube 启动完成后，创建名为 cicd 的命名空间： $ kubectl create ns cicd 此外，你还需要 Fork Helm 包存储库，以便根据步骤操作： 包存储库地址：https://github.com/weiwendi/charts 单击右上角的 “Fork” 按钮创建包存储库的 Fork： 7.1 fork 创建包存储库的分支后，运行以下命令将该分支克隆到本地机器： $ git clone https://github.com/$GITHUB_USERNAME/charts 使用以下步骤从你的图表存储库中删除图表： 切换到本地图表存储库的目录，我们的示例中，目录名为 charts： [aiops@aiops0113 ~]$ cd charts/ [aiops@aiops0113 ~]$ ls ...... # CNAME 文件和图表无关，只因我为该仓库设置了自定义域名，GitHub 自动生成了 CNAME 文件。 从存储库中删除 README.md 之外的文件： [aiops@aiops0113 charts]$ rm -rf $(ls |egrep -v '(README.md)') [aiops@aiops0113 charts]$ ls README.md 把更改推送到远程 GitHub 仓库： [aiops@aiops0113 charts]$ git add --all [aiops@aiops0113 charts]$ git commit -m 'Preparing for ci/cd.' [aiops@aiops0113 charts]$ git push origin main 在 GitHub 上确认图表和索引文件已被删除，只剩 README.md 文件： 7.2 delete tar and index file 经过以上四步，你已经启动了 Minikube，Fork 了图表包存储库，并从 charts 中删除了 player-stats 图表及索引等文件。现在，我们开始学习创建 CI 管道发布 Helm 图表。 创建 CI 管道构建 Helm 图表 以图表开发者的角度，CI 的概念需要实现构建、测试、打包和发布 Helm 图表到图表存储库。在本节中，我们将讨论如何使用端到端的 CI 管道简化这一过程，并逐步引导你创建示例管道。首先，我们从设计示例管道所需的组件开始。 CI 管道设计 在之前的章节中，我们讨论了Helm 图表的开发，整个开发过程很大程度是手动完成的。虽然 Helm 可以在 Kubernetes 集群中创建自动化的测试钩子，但在代码更改后，仍需手动执行 helm lint、helm test 或者 ct lint-and-test 命令，以确保能够通过测试。一旦更改后的代码经过了测试，就可以执行 helm package 命令对图表打包，再执行 helm repo index 命令创建 index.yaml 文件。如果图表库是 GitHub，就可以把包文件和索引文件推送到 GitHub 仓库中了。 虽然这些步骤可以通过手动执行命令完成，但当你开发的图表越来越多，或者有更多的开发人员参与其中，这个工作流就会逐渐变得难以掌控。在手动执行时，很可能提交未经测试的代码，而且很难保证每位开发者都遵守了测试规范。幸运的是，可以通过创建一个自动发布流程的 CI 管道避免这些问题。 下面的步骤概述了一个 CI 工作流程，它使用了本书到目前为止介绍的命令和工具。在这个示例中，假设了图表被保存在 GitHub 仓库： 图表开发人员对 git monorepo 中对一个或多个图表代码进行了修改。 开发人员推送修改后的代码到远程仓库。 执行 ct lint 和 ct install 命令，在 Kubernetes 命名空间中自动检测和测试修改的图表。 如果经过测试，自动执行 helm package 命令将图表打包。 使用 helm repo index 命令生成 index.yaml 索引文件。 打包后的图表和更新后的 index.yaml 文件会自动推送到存储库。它们被推送到 job 对应的分支上。 在下一节中，我们将使用 Jenkins 执行此过程。首先我们先了解 Jenkins 是什么以及它是如何工作的。 Jenkins 介绍 Jenkins 是一个开源服务，用于执行自动化任务和工作流程。它通过 Jenkins 的管道即代码功能创建 CI/CD 管道，该功能通过名为 Jenkinsfile 的文件实现，Jenkinsfile 文件用于定义 Jenkins 管道。 Jenkins 管道是用 Groovy 特定领域语言(Domain-Specific Language DSL)编写的。Groovy 是一种类似于 Java 的语言，但与 Java 不同的是，它可以作为一种面向对象的脚本语言，适合编写易于阅读的自动化脚本。在本章中，我将带你学习两个已经编写好的 Jenkinsfile 文件：ciJenkinsfile 和 cdJenkinsfile。你不需要有任何编写 Jenkinsfile 文件的经验，因为对 Jenkins 的深入研究超出了本书的范围。也就是说，在本章结束时，你应该能够把学到的概念应用到你选择的自动化工具中。虽然本章以 Jenkins 为例，但其概念可以应用于任何其他自动化工具。 当 Jenkinsfile 被创建后，定义的工作流步骤集会在 Jenkins 主服务器或一台单独的 Agent 上执行。可以把 Agent 作为 Kubernetes 的一个 Pod 启动，从而简化 Agent 的创建和管理。在 Agent 执行完 Job 后，可以配置为自动终止。当有新的构建任务时，会在一个新创建的干净 Pod 中执行。在本章中，我们将使用 Jenkins Agent 运行示例管道。 Jenkins 可以扫描代码仓库中是否包含 Jenkinsfile 文件，这一功能使它很适合 GitOps 的概念。对于包含 Jenkinsfile 文件的分支，会自动创建一个新 Job，克隆所需分支的代码。这让测试新功能变得更加简单，因为新的 Job 可以与它们相应的分支一起自动创建。 对 Jenkins 有了基本的了解之后，现在让我们将 Jenkins 安装到 Minikube 环境中。 安装 Jenkins 与在 Kubernetes 上部署的其他应用程序一样，Jenkins 也可以使用 Helm Hub 上的图表部署。在本章中，我们使用 Jenkins 官方提供的 Jenkins Helm 图表。首先添加 jenkins 图表仓库： [aiops@aiops0113 charts]$ helm repo add jenkins https://charts.jenkins.io \"jenkins\" has been added to your repositories Jenkins 图表除了提供 Kubernetes 的相关值，如资源限制、service 类型等，还包含了与 Jenkins 相关的值，用于自动配置不同的 Jenkins 组件。在安装前，我们定义一个值文件，用来修改 Jenkins Master 的 service 类型，默认是 ClusterIP，我们把它改成 NodePort。在我们的示例中，用的是 Multibranch 类型的 Job，Jenkins 默认并没有安装此插件，所以也需要在值文件中，提供此插件的安装信息。最终的值文件内容如下： [aiops@aiops0113 jenkins]$ cat values.yaml controller: serviceType: NodePort installPlugins: - workflow-multibranch:2.26 执行 helm install 命令安装 Jenkins： [aiops@aiops0113 jenkins]$ helm install jenkins jenkins/jenkins --values ~/learn-helm/jenkins/values.yaml -n cicd NAME: jenkins LAST DEPLOYED: Thu Jul 8 02:24:10 2021 NAMESPACE: cicd STATUS: deployed REVISION: 1 NOTES: 1. Get your 'admin' user password by running: kubectl exec --namespace cicd -it svc/jenkins -c jenkins -- /bin/cat /run/secrets/chart-admin-password && echo 2. Get the Jenkins URL to visit by running these commands in the same shell: export NODE_PORT=$(kubectl get --namespace cicd -o jsonpath=\"{.spec.ports[0].nodePort}\" services jenkins) export NODE_IP=$(kubectl get nodes --namespace cicd -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT/login 3. Login with the password from step 1 and the username: admin 4. Configure security realm and authorization strategy 5. Use Jenkins Configuration as Code by specifying configScripts in your values.yaml file, see documentation: http:///configuration-as-code and examples: https://github.com/jenkinsci/configuration-as-cod e-plugin/tree/master/demos For more information on running Jenkins on Kubernetes, visit: https://cloud.google.com/solutions/jenkins-on-container-engine For more information about Jenkins Configuration as Code, visit: https://jenkins.io/projects/jcasc/ NOTE: Consider using a custom image with pre-installed plugins 命令执行完成后，会输出 NOTES 信息，这时 Jenkins Master Pod 可能还没启动完成，它需要一个初始化的阶段。通过以下命令监听 Jenkins Master Pod 的状态，当全部 READY 时，就可以通过 NOTES 提供的信息访问 Jenkins Master 了： [aiops@aiops0113 jenkins]$ kubectl get pod -w -n cicd # 获取 admin 账号的登录密码 [aiops@aiops0113 jenkins]$ kubectl exec --namespace cicd -it svc/jenkins -c jenkins -- /bin/cat /run/secrets/chart-admin-password && echo # 获取 node IP [aiops@aiops0113 jenkins]$ kubectl get nodes --namespace cicd -o jsonpath=\"{.items[0].status.addresses[0].address}\" # 获取 node port [aiops@aiops0113 jenkins]$kubectl get --namespace cicd -o jsonpath=\"{.spec.ports[0].nodePort}\" services jenkins 有了这三个信息，就可以在浏览器登录 Jenkins 控制台了。下一节，我们讨论如何配置 Jenkins。 配置 Jenkins 根据 NOTES 提供的信息登录 Jenkins 控制台，我们对 Jenkins 做一些必要的配置。 首先，我们添加两个凭据，一个用于向 GitHub 提交代码，一个用于访问 Kubernetes 集群。 凭据创建路径大致如此：“Manage Jenkins ” --> “Manage Credentials” --> “Jenkins” --> “全局凭据 (unrestricted)” --> “Add Credentials”。 创建 GitHub 凭据 在 CI 过程中，我们要把经过测试的 Helm 图表包推送到 Fork 出的 GitHub 包存储库，这个操作需要向 GitHub 提供凭据。在 2021 年 8 月 13 日之后，GitHub 将不再支持密码验证，需要使用 personal access token 替换。 我们先创建一个 token，登录 GitHub 页面，点击右上角头像旁的倒三角，会弹出一个窗口，点击其中的 “Settings” 按钮，在弹出的新页面中，点击左侧栏的 “Developer settings”，点击 “Personal access tokens”，再点击 “Generate new token”，填写 Note 信息，设置过期时间，勾选 repo 相关权限，最后点击底部的 “Generate token” 创建 token，然后把生产的 token 复制出来，我们会在创建 GitHub 凭据时用到。 7.3 create github token. 有了 GitHub token，我们到 Jenkins 控制台中创建凭据，配置信息请参考下图： 7.4 create jenkins github credentials Kind：使用何种方式创建该凭据，我们选择用户名、密码的方式。 Scope：凭据的使用范围。 Username：创建该凭据使用的用户名，这里需要填写你 GitHub 的用户名。 Password：创建该凭据使用的密码/token，这里需要把在 GitHub 上复制的 token 粘贴进去。 ID：凭据的标识符，可以填写一个有标示性的名称。在 Jenkinsfile 中会通过凭据 ID 引用凭据。 Description：凭据描述信息。 创建 kubeconfig 凭据 在 CI/CD 过程中，我们会通过 kubectl 和 helm 命令操作 Kubernetes 集群，这就需要在 Jenkins 创建一个访问 Kubernetes 集群的凭据，配置信息见下图： 7.5 create jenkins kubeconfig credentials Kind：因为我们上传的是 kubeconfig 文件，所以类型选择为 Secret file。 File：选择要上传的文件，在演示中我直接使用了 ~/.kube/config 文件。 创建 CI 管道 CI 是通过具体的 item 实现的，在安装 Jenkins 后，需要我们手动创建 CI item。 Item 的类型有多种，我们使用多分支流水线类型。在首页点击 “New Item”，输入 item 名称，我们 CI 示例的 item 名称为 “testAndRelease”，点击 ”多分支流水线“，然后点击 ”OK“ 创建。会弹出一个新的配置页面，我们只需配置 “Branch Sources” 和 “Build Configuration” 两个部分，其他默认即可。 在 “Branch Sources” 下点击 ”Add source“ --> “GitHub”，在 “Repository HTTPS URL” 框中输入源码仓库地址，其他地方保持默认即可，如下图： 7.6 add repo 在 “Build Configuration” 部分，“Mode” 选择默认的 “by Jenkinsfile”，“Script Path” 是 Jenkinsfile 文件的路径及名称，我把 CI 用到的 Jenkinsfile 放在了源码仓库中的根目录下，起名为 ciJenkinsfile，所以 “Script Path” 部分只需填写文件名 ciJenkinsfile 即可，如下图： 点击 Save，保存配置后，Jenkins 会自动对我们的配置做检查，输出以下内容就是配置完成了： Started [Tue Jul 13 07:06:29 UTC 2021] Starting branch indexing... 07:06:31 Connecting to https://api.github.com with no credentials, anonymous access Examining weiwendi/learn-helm Checking branches... Getting remote branches... Checking branch main Getting remote pull requests... ‘ciJenkinsfile’ found Met criteria Scheduled build for branch: main 1 branches were processed Checking pull-requests... 0 pull requests were processed Finished examining weiwendi/learn-helm [Tue Jul 13 07:06:34 UTC 2021] Finished branch indexing. Indexing took 5.3 sec Finished: SUCCESS 这时，如果你细心的话，会发现 Jenkins 控制台左下角 “Build Executor Status” 会有构建任务在执行，点击进度条，能够看到管道执行过程的日志信息。 7.8 scan repo 接下来，结合日志输出与 ciJenkinsfile 文件，我们讨论下 CI 管道的执行过程。 CI 管道介绍 通过 Jenkinsfile 创建 CI 管道，Jenkinsfile 文件位于 learn-helm 目录下，名称为 ciJenkinsfile，可以在 https://github.com/weiwendi/learn-helm/blob/master/ciJenkinsfile 查看文件。 ciJenkinsfile 内容如下： // 声明 CI Pipeline. pipeline { // environment 用于设置变量,变量作用域是该管道. environment { // 为 Helm Charts 源代码仓库及 Helm Charts Package 仓库设置变量. // gitSourceRepo 仓库包含了 Helm Charts 的源代码，也有本书其他相关代码，如 Jenkinsfile. // gitPackageRepo 是 Helm Package 存储库. // 这两个仓库地址，你都需要修改成 Fork 后的地址. gitSourceRepo = 'https://github.com/weiwendi/learn-helm.git' gitPackageRepo = 'https://github.com/weiwendi/charts.git' // 定义凭证变量，我们在 Jenkins 控制台创建了两个凭证，ID 分别为 github、kubeconfig. // 在向 gitPackageRepo 推送 Charts 包时会用到 GITAUTH 变量. // KUBECONFIG 会被挂在到 Pod 中，与 Kubernetes 集群进行交互. GITAUTH = credentials('github') KUBECONFIG = credentials('kubeconfig') } // 定义执行 Job 的 jenkins agent; // agent 是运行在 Kubernetes 集群中的一个 Pod. agent { kubernetes { yaml ''' apiVersion: v1 kind: Pod spec: serviceAccountName: jenkins containers: # 定义容器的名称 - name: test-and-release-ci # 指定容器的镜像,该镜像包含了 helm/ct/yamllint/yamale/git/kubectl. # 镜像 Dockerfile 可以在源码仓库的 docker 目录下查看. image: registry.cn-beijing.aliyuncs.com/sretech/cttools:latest command: - sleep args: - infinity resources: requests: memory: \"1024Mi\" cpu: \"1000m\" limits: memory: \"1024Mi\" cpu: \"1000m\" ''' // 设置执行各 stages 时默认使用 test-and-release-ci 容器 defaultContainer 'test-and-release-ci' } } // Jenkins 选项 options { // 显示执行的时间，需要 Timestamper 插件支持. timestamps() // 在 Stage View 中展示近 10 次构建信息 buildDiscarder(logRotator(numToKeepStr: '10')) } // 定义管道执行的各个阶段、每个阶段的具体步骤，建议包含至少一个 stage. stages { // stage 定义了 pipeline 完成的所有实际工作. // 首先，我们定义一个打印构建信息的 stage. stage('Build Messages') { // steps 定义了管道中具体执行的步骤. steps { // container 选择哪个容器执行该 steps，不是用 container 选项的话，会使用默认容器执行 steps. container(\"test-and-release-ci\") { echo \" workspace: ${WORKSPACE}\\n gitPackageRepo: ${gitPackageRepo}\\n gitSourceRepo: ${gitSourceRepo}\\n branch: ${env.BRANCH_NAME}\\n buildId: ${BUILD_ID}\" } } } stage(\"Copy Kubeconfig\") { steps { script { sh ''' mkdir -p ~/.kube cp ${KUBECONFIG} ~/.kube/config ''' } } } stage(\"List Changed Charts\") { steps { script { sh \"ct list-changed\" } } } stage(\"Lint\") { steps { container(\"test-and-release-ci\") { sh \"ct lint\" } } } stage(\"Install & Test\") { steps { script { sh \"ct install --upgrade\" } } } stage(\"Package Charts\") { steps { script { sh \"helm package --dependency-update helm-charts/charts/*\" sh \"ls -l\" } } } stage(\"Push Charts to Chart Repo\") { steps { script { def baseBranch = \"main\" // 克隆 Helm Chart Package 包存储库到 chart-repo 目录. sh \"git clone ${env.gitPackageRepo} chart-repo\" // 使用 if 语句，根据分支判断图表包应该推送到 stable 或是 staging 目录. def repoType if (env.BRANCH_NAME == baseBranch) { repoType = \"stable\" } else { repoType = \"staging\" } // 如果不存在，创建 stable 或 staging 目录. def files = sh(script: \"ls chart-repo\", returnStdout: true) if (!files.contains(repoType)) { sh \"mkdir chart-repo/${repoType}\" } // 移动图表包到 stable 或 staging 目录. sh \"mv *.tgz chart-repo/${repoType}/\" // 更新索引文件. sh \"helm repo index chart-repo/${repoType}\" // 更新 git 配置信息，需要填写你自己的 github 邮箱及用户名. sh \"\"\" git config --global user.email 'weiwendi@aiops.red' git config --global user.name 'weiwendi' \"\"\" dir(\"chart-repo\") { // Add and commit the changes sh \"git add --all\" sh \"git commit -m 'pushing charts from branch ${env.BRANCH_NAME}'\" // 推送图表包到 gitPackageRepo 仓库. script { // Inject GitHub auth and push to the repo where charts are being served def authRepo = env.gitPackageRepo.replace('://', '://${GITAUTH_USR}:${GITAUTH_PSW}@') sh \"git push ${authRepo} ${baseBranch}\" } } } } } } } 管道在执行时，首先在 Kubernetes 的 cicd 命名空间中创建一个 Pod，作为 jenkins agent，用来完成接下来的构建任务。Pod 的命名规范是 “itemname-branchname-buildnumber-xx-xx”。 Pod 中包含了两个容器，一个是我在 ciJenkinsfile 文件中定义的，名称为 “test-and-release-ci”，使用指定的 cttools 镜像；另一个容器是 jnlp，与 jenkins master 通信。 cttools 镜像是我构建的，可以在 https://github.com/weiwendi/learn-helm/blob/master/docker/cttools/Dockerfile 中查看 Dockerfile 文件内容。镜像地址是 registry.cn-beijing.aliyuncs.com/sretech/cttools，包含了第六章中介绍的一些工具： helm ct yamllint yamale git kubectl 由于这个镜像包含了测试 Helm 图表所需的工具，因此可以将其作为执行 Helm 图表的 CI 的镜像。 当 Jenkins agent 运行时，会自动克隆指定的代码仓库。这是由 Jenkins 执行的，不需要在 Jenkinsfile 中配置。可以在日志中看到这一过程 ，在 Pod 创建完成后，有一个叫 SCM 的 stage 被执行，拉取我们在 item 中配置的 github 仓库代码。 在 Jenkins agent 完成克隆后，就开始执行 Jenkinsfile 文件中定义的阶段(stages)。阶段是管道中的逻辑分组，能可视化步骤。执行的第一个和 lint 相关的阶段是 “lint stage”，包含以下命令： sh 'ct lint' sh 用于运行 bash shell 命令或者脚本，本处用于执行 ct lint 子命令，用来检测所有针对 main 分支修改的图表中的 Chart.yaml、values.yaml 文件，我们在第六章讨论过。 如果检测成功，管道将执行下一个阶段，也就是 “test stage”，该阶段执行的命令如下： sh 'ct install --upgrade' 这个命令看起来也很熟悉。它安装主分支上的每个修改过的图表版本，并执行定义好的测试步骤。它还能确保从上一版本的任何升级都是成功的，这有助于向后兼容。 虽然前面两个阶段可以通过运行 ct lind-and-install --upgrade 命令实现，也就是说两个阶段可以合并成一个，但为了更好的可视化所执行的操作，我们还是把它分解成了不同的阶段。 如果 “test stage” 执行成功，管道将继续执行 “package charts stage” 阶段，命令如下： sh 'helm package --dependency-update helm-charts/charts/*' 这个命令将 helm-charts/charts 目录下的图表打包，它还将更新和下载声明的依赖项。 如果打包成功，管道将进入最后一个阶段 “Push Charts to gitPackageRepo”。这是最复杂的阶段，所以我们将它拆分成更小的步骤，第一步如下： // 克隆 Helm Chart Package 包存储库到 chart-repo 目录. sh \"git clone ${env.gitPackageRepo} chart-repo\" // 使用 if 语句，根据分支判断图表包应该推送到 stable 或是 staging 目录. def repoType if (env.BRANCH_NAME == baseBranch) { repoType = \"stable\" } else { repoType = \"staging\" } // 如果不存在，创建 stable 或 staging 目录. def files = sh(script: \"ls chart-repo\", returnStdout: true) if (!files.contains(repoType)) { sh \"mkdir chart-repo/${repoType}\" } 由于我们要推送到的 Helm 图表存储库是单独的 GitHub 页面，因此必须先将存储库克隆到 Jenkins agent 本地，以便添加新图表并推送修改。克隆 GitHub 存储库后，将根据 CI/CD 管道运行的分支设置名为 repoType 的变量。此变量用于确定将前一阶段打包的图表推送到 stable(稳定) 或 staging(临时) 图表存储库中。 stable 和 staging 作为两个单独的图表存储库，可以通过在 GitHub 页面创建两个单独的目录来实现： charts/ stable/ staging/ stable 和 staging 目录中会包含自己的 index.yaml 文件，用于把它们区分为不同的图表存储库。 如果基于分支的管道执行依赖于这两个目录，为方便起见，可以在上一个阶段自动创建它们。 现在已经确定了图表应推送到的存储库，继续执行下一个阶段： sh \"\"\" // 将图表包移动到 chart-repo 的 stable 或 staging 目录中. mv *.tgz chart-repo/${repoType}/ // 更新 index.html. helm repo index chart-repo/${repoType} // 更新 git 配置信息，需要把邮箱、用户名改为你自己的. git config --global user.email 'weiwendi@aiops.red' git config --global user.name 'weiwendi' \"\"\" 第一个命令将上一个阶段中打的图表包复制到 stable/ 和 staging/ 目录中。接下来，使用 helm repo index 命令更新 stable/ 或 staging/ 目录中的 index.yaml 文件，以反映更改或增加的图表。 需要记住的一点是，如果我们使用不同的图表存储库解决方案，比如 ChartMuseum(一个由Helm社区维护的图表库解决方案)，那么 helm repo index 命令就不需要了。当 ChartMuseum 收到一个新打包的 Helm 图表时， index.yaml 文件会自动更新。对于不自动更新 index.yaml 文件的，例如 GitHub 页面，helm repo index 命令是必须的，正如我们在这个管道中看到的。 该阶段中的最后两条命令用来设置 git 邮箱和用户名，这是提交内容到 git 存储库必须的步骤。在本例中，我们将用户名设置为 weiwendi，邮箱设置为 weiwendi@aiops.red，这些都是示例，在操作过程中，你需要把它们改成你的真实邮箱和用户名。 最后一步是 push 操作： dir(\"chart-repo\") { // Add and commit the changes. sh \"\"\" git add --all git commit -m 'pushing charts from branch ${env.BRANCH_NAME}' \"\"\" // 推送图表包到 gitPackageRepo 仓库. script { // Inject GitHub auth and push to the repo where charts are being served def authRepo = env.gitPackageRepo.replace(\"://\", \"://${GITAUTH_USR}:${GITAUTH_PSW}@\") sh \"git push ${authRepo} ${baseBranch}\" } } 首先使用 git add 和 git commit 命令添加和提交图表包，接下来使用 git push 命令推送到存储库，推送时使用一个名为 GITAUTH 的凭据变量。这个凭证是安装期间通过 Jenkins 控制台创建的凭据。GITAUTH 凭据可以安全的引用密码，从而避免在管道代码中以明文的形式显示密码。 Helm 社区发布了一个名为 Chart Releaser 的工具，它通过调用 helm repo index 命令生成 index.yaml 文件，并使用 git push 将其推送到 GitHub。Chart Releaser 工具旨在通过管理 GitHub 页面中包含的 Helm 图表，来抽象这些额外的复杂性。不过，我们决定不在本章中使用这个工具来实现管道，因为 Chart Releaser 不支持 Helm 3(在撰写本文时)。 在构建日志中，Stage 的 steps 前都会显示执行时间，这是由 options.timestamps() 定义的。所有 stage 执行完成，最后输出 Finished：SUCCESS 字段，整个 CI 管道也就成功执行完成了。 现在我们已经熟悉了 CI 管道的概述，让我们来执行一个示例。 运行 CI 管道 我们前面已经介绍过，在保存 item 配置后，会触发 “Scan Repositor“ 的操作，针对配置的分支自动运行 CI 管道，可以通过 Jenkins 页面的 ”testAndRelease“ 连接查看。你会看到一个成功执行的 Job，运行在 main 分支上。 Jenkins 中的每个 pipeline 构建都包含执行输出的相关日志。你可以通过点击左侧蓝色圆圈旁边的链接，然后在下一个屏幕上选择 Console Output 查看构建日志。这个构建日志显示，第一个 stage Lint 成功地显示了以下信息： All charts linted successfully ---------------------------------- No chart changes detected. This is what we would expect because no charts were changed from the perspective of the master branch. A similar output can be seen under the install stage as well: All charts installed successfully ----------------------------------- No chart changes detected. Because both the Lint and Install stages completed without error, the pipeline continued to the Package Charts stage. Here, you can view the output:: + helm package --dependency-update helm-charts/charts/player-stats helm-charts/charts/nginx Successfully packaged chart and saved it to: /home/jenkins/agent/workspace/t_and_Release_Helm_Charts_master/player-stats-1.0.0.tgz Successfully packaged chart and saved it to: /home/jenkins/agent/workspace/t_and_Release_Helm_Charts_master/nginx-1.0.0.tgz Finally, the pipeline concludes by cloning your GitHub Pages repository, creating a stable folder within it, copying the packaged charts over to the stable folder, committing the changes to the GitHub Pages repository locally, and pushing the changes to GitHub. We can observe that each file that was added to our repository is outputted in the following lines: + git commit -m 'pushing charts from branch master' [master 9769f5a] pushing charts from branch master 3 files changed, 32 insertions(+) create mode 100644 stable/player-stats-1.0.0.tgz create mode 100644 stable/index.yaml create mode 100644 stable/nginx-1.0.0.tgz GitHub 存储库在自动推送后，页面内容如下，请忽略 CNAME 文件： 7.9 charts repo list 在 stable 文件夹中，你应该能够看到三个不同的文件：两个独立的图表和一个 index.yaml 文件： 7.10 stable list 第一个 pipeline 构建成功地创建了初始的 stable 图表集，但它没有演示如何在新图表被认为稳定，并准备好供最终用户使用前对其进行检查和测试。为了演示这一点，我们需要从 main 分支上检出一个 feature 分支，用来修改一个或多个图表，并将更改推送到 feature 分支，然后在 Jenkins 中开始一个新的构建。 首先，在 main 分支上创建一个名为 chapter7 的新分支： $ cd charts $ git checkout main $ git checkout -b chapter7 在这个分支上，我们简单的修改 nginx 图表的版本，以触发图表的 linting 和 testing。Nginx 是一个 WEB 服务器和反向代理，它比 player-stats 应用程序要轻量的多，因此，在这个例子中，我们使用包存储库中的 nginx 图表，以避免 Minikube 中运行 Jenkins 时可能出现的资源限制。 将 helm-charts/charts/nginx/Chart.yaml 文件中 version 的值从 1.0.0 改为 1.0.1： version: 1.0.1 执行 git status 命令查看更改： $ git status On branch chapter7 Changes not staged for commit: (use 'git add ...' to update what will be committed) (use 'git checkout -- ...' to discard changes in working directory) modified: helm-charts/charts/nginx/Chart.yaml no changes added to commit (use 'git add' and/or 'git commit -a') nginx 图表的 Chart.yaml 文件已经被修改，add 修改后的文件，然后执行 commit，最后将其推送到你的 Fork： $ git add helm-charts $ git commit -m 'bumping NGINX chart version to demonstrate chart testing pipeline' $ git push origin chapter7 我们需要在 Jenkins 中触发扫描存储库，以便 Jenkins 能够检测并针对这个分支启动一个新的构建。点击 “testAndRelease” 按钮，点击左侧栏菜单中的 “Scan Repository Now” 按钮，触发 Jenkins 检测新分支，并自动启动一个新的构建。扫描大约10秒内完成。刷新页面，新的 chapter7 分支将出现在页面上。 chapter7 job 会比 main job 执行更长的时间，因为 chapter7 job 包含一个修改的 Helm 图表，该图表会使用 testing 工具进行测试。可以在 chapter7 job 的输出中查看 pipeline 的操作。pipeline 执行结束，会显示以下信息： Finished: SUCCESS 在控制台输出的日志开头部分，请注意 ct lint 和 ct install 命令是如何针对 nginx chart 运行的，因为这是唯一发生修改的 chart： Charts to be processed: --------------------------------------------------------------- nginx => (version: '1.0.1', path: 'helm-charts/charts/nginx') 这些命令的输出你应该已经很熟悉了，因为在第六章中，我们已经接触过了。 在 GitHub 页面，你应该会看到 staging 文件夹中新版本的 nginx chart，因为它不是基于 main 分支构建的。 要发布 nginx-1.0.1.tgz 图表，需要将 chapter7 分支合并到 main 分支，并推送到远程库中，命令如下： $ git checkout main $ git merge chapter7 $ git push origin main 返回 “testAndRelease” Jenkins 页面，点击 main job，点击该页面左侧栏的 “Build Now” 按钮。从输出日志中看到，图表测试被跳过了，因为 chart testing tool 将 clone 与 main 分支进行了比较，由于内容相同，tool 决定不需要进行测试。当构建完成后，可以在 GitHub 页面看到 nginx-1.0.1.tgz 图表在 stable 存储库中。 使用 helm repo add 命令在本地添加存储库，验证这些图表是否已正确发布到 GitHub stable 存储库中： $ helm repo add aiops $gitPackageRepoPage/stable $gitPackageRepoPage 的值是 GitHub 站点的地址，而不是实际的 git 存储库。地址格式类似于 https://$GITHUB_USERNAME.github.io/charts/stable，该链接可以在 GitHub 页面的 “Settings” 标签中找到，因为我在 “Settings” 的 “Page” 中配置了 CNAME，所以我的地址可以是 https://charts.aiops.red/stable。 添加了 stable 存储库后，运行以下命令查看在两次 main 构建过程中构建和推送的存储库中的图表： $ helm search repo aiops --versions 在本节中，我们讨论了如何通过 CI pipeline 管理 Helm 图表的生命周期。根据示例，通过遵循自动化工作流，你可以在向用户发布图表之前轻松地执行例行检查和测试了。 下一节，我们介绍使用 CD pipeline，将 helm 图表部署到不同的环境中。 创建 CD 管道部署 Helm 图表 CD 管道是一组可重复的步骤，可以在单个或多个环境中，以自动化的方式部署应用程序。在本节中，我们将创建一个 CD 管道来部署 Nginx 图表，在上一节中，我们已经把它推送到了 GitHub 存储库。GitOps 还可以应用保存到 git 存储库中的值文件。 我们来梳理一下此管道中有哪些步骤： 添加包含 Nginx 图表 release 的 GitHub stable 库。 将 Nginx 图表部署到开发环境中。 将 Nginx 图表部署到 QA(Quality Assurance ) 环境中。 等待用户批准管道进行生产环境部署。 将 Nginx 图表部署到生产环境中。 CD 工作流包含在一个单独的 Jenkinsfile 文件中，该文件的名称是 cdJenkinsfile。与 ciJenkinsfile 不同，在创建 cdJenkinsfile 前，我们先更新 Minikube 和 Jenkins 环境，以便执行 CD 流程。 更新环境 在 Minikube 中创建三个命名空间，对应开发、QA和生产环境。我们这里只是为了演示 CD 流程，在实际使用中，不建议把非生产环境(开发和QA)和生产环境部署在一个集群中。 创建 dev、qa、prod 命名空间，部署不同的环境： $ kubectl create ns dev $ kubectl create ns qa $ kubectl create ns prod 删除 chapter7 分支，因为在创建新的 CD 管道时，Jenkins 会尝试在每个分支上执行它，为了简单和避免资源受限，我们仅使用 main 分支。 使用以下命令从存储库中移除 chapter7 分支： $ git push -d origin chapter7 $ git branch -D chapter7 最后，在 Jenkins 控制台创建一个名为 “deployNginxChart” 的 item，步骤和创建 “testAndRelease” 一样，唯一不同的是，需要把 Jenkinsfile 的名字从 “ciJenkinsfile” 改为 “cdJenkinsfile”。 保存配置后，同样会执行代码库扫描，并自动执行 CD 管道。 CD 管道介绍 在本节中，我们只介绍管道的主要部分，完整的 CD 管道可以参考 https://github.com/weiwendi/learn-helm/blob/master/cdJenkinsfile。 与 CI 管道一样，为了测试和发布 Helm 图表，CD 管道首先在 Kubernetes 中动态的创建一个 Jenkins Agent Pod。 Agent 创建完成后，Jenkins 就会隐式地 clone 你 fork 的源代码仓库了，就像之前在 CI pipeline 中所作的那样。 Pipeline 定义的第三个 stage 是 Add Chart，它将 GitHub stable 存储库中的图表添加到 Jenkins Agent 的本地 Helm 客户端： sh \"helm repo add aiops ${env.gitPackageRepoPage}/stable\" 添加存储库之后，管道就可以将 Nginx 部署到不同的环境中了。下一步，部署 Nginx 图表到 dev 命名空间中，并打印出环境变量，方便我们查看： dir(\"nginx-cd\") { sh \"\"\" helm upgrade --install nginx-${env.BRANCH_NAME} aiops/nginx --values common-values.yaml --values dev/values.yaml -n dev --wait kubectl -n dev exec deploy/nginx-${env.BRANCH_NAME} -- env | grep ENVIRONMENT \"\"\" } dir('nginx-cd') 是 Jenkins 的语法，用于设置执行命令的工作目录。在这个 stage 中，我们给 helm upgrade 命令提供了 --install 标志。当 release 存在时，执行 helm upgrade 会对 release 进行升级，如果 release 不存在，则会执行失败，所以我们提供了 --install 标志，当 release 不存在时，会安装图表，这大大增加了自动化的程度。 在这个命令中，我们传递了两个值文件：commmon-values.yaml 和 dev/values.yaml。这两个文件都在 nginx-cd 目录下，目录内容如下： nginx-cd/ ├── common-values.yaml ├── dev │ └── values.yaml ├── prod │ └── values.yaml └── qa └── values.yaml 将应用程序部署到不同的环境时，我们需要稍微修改应用程序的配置，以便与环境中的其它服务集成，比如数据库地址。dev、qa 和 prod 文件夹下的值文件，都包含了一个环境变量，该变量是在 Nginx 部署时设置的，具体取决于要部署到的环境。例如 dev/values.yaml 文件内容： envVars: - name: ENVIRONMENT value: dev 同样的，qa/values.yaml 文件内容如下： envVars: - name: ENVIRONMENT value: qa prod/values.yaml 文件内容如下： envVars: - name: ENVIRONMENT value: prod 虽然本示例中部署的 Nginx 图表很简单，并不严格要求指定这些值，但可能你已经发现了，使用这种在单独的值文件中分离特定于某个环境配置的方法，有助于处理实际环境中更复杂的用例，比如不同环境，有不同的中间件地址。相应的值文件可以通过变量的形式传递，如 helm upgrade --install command with --values ${env}/values.yaml，其中 ${env} 表示 dev、qa、prod。 common-values.yaml 文件从文件名可以看出，用于配置各环境的通用值。common-values.yaml 文件内容如下： service: type: NodePort 这个文件定义了在图表安装过程中，创建的 Nginx service 类型为 NodePort，在 values.yaml 文件中设置的其它值，也会应用于每个环境，因为它们没有被 common-values.yaml 或独立的 values.yaml 文件覆盖。 需要注意的是，应用程序在每个环境中应该以相同的方式部署。任何改变运行中的 Pod 或容器的属性值，都应该在 common-values.yaml 文件中定义，这些配置包括但不限于： replica 数量 资源的 requests 和 limits 值 service 类型 image 名称 image tag imagePullPolicy values 挂载 可以针对特定环境独立修改的值如下： 度量或监控服务的地址 数据库或后端服务地址 应用对外暴露的 ingress URL 通知服务 继续介绍 “Deploy to Dev” stage 中的 helm 命令， 它使用了 common-values.yaml、dev/values.yaml 两个值文件，这两个值文件被用来在 dev 中安装 nginx 图表。这个命令还使用了 -n dev 标志，用来指定在 dev 命名空间中部署 Nginx 图表。--wait 标志用于暂缓输出创建信息，直到 Pod 处于 reday 状态。 在 CD 管道中，把应用部署到 dev 之后的下一个 stage 是冒烟测试，执行以下命令： sh \"helm test nginx-${env.BRANCH_NAME} -n dev\" Nginx 图表包含一个检查 Nginx Pod 连接的 test 钩子。如果 test 钩子能够连接到 Pod，则 test 返回成功。虽然 helm test 命令常用于图表测试，但它也可以用于 CD 过程中执行基本冒烟测试。冒烟测试主要用于确保程序部署后关键功能是按照设计工作的。由于 Nginx 图表测试不会以任何方式干扰正在运行的应用程序或部署环境的其它应用，因此 helm test 命令是确保 Nginx 图表成功部署的适当方法。 冒烟测试完成后，CD 管道将执行下一个 stage——“Deploy to QA”。此 stage 包含一个条件，用于评估管道正在执行的当前分支是否为 main 分支，如下所示： when { expression { return env.BRANCH_NAME == 'main' } } 此条件可以使用 feature 分支来测试 values.yaml 中的值，而不是将其放在生产环境中测试，main 分支中包含的 helm 值应该是生产就绪的。 Deploy to QA 使用的命令如下： dir(\"nginx-cd\") { sh \"\"\" helm upgrade --install nginx-${env.BRANCH_NAME} aiops/nginx --values common-values.yaml --values qa/values.yaml -n qa --wait kubectl -n qa exec deploy/nginx-${env.BRANCH_NAME} -- env | grep ENVIRONMENT \"\"\" } 这条命令和 “Deploy to DEV” stage 里的命令相似，这里就不再详述了。 在部署到 QA 或类似的测试环境之后，可以再次运行冒烟测试，以确保 QA 部署的基本功能都能正常工作。在这个 stage，你还可以执行其他的自动化测试，在将应用程序部署到 prod 之前验证其功能是非常必要的。我在这个示例管道中省略了这些测试细节。 管道的下一个 stage 是 Wait for Input： stage(\"Wait for Input\") { when { expression { return env.BRANCH_NAME == \"main\" } } steps { input \"Deploy to Prod?\" } } 这一步会暂停执行 CD Pipeline，并提示用户是否继续 “Deploy to Prod”？在运行 job 的控制台日志中，给用户两个选项——“Proceed” 和 “Abort”。虽然生产环境的部署可以自动执行，但许多开发人员可能更倾向在非生产部署和生产部署之间设置一道闸。这个 input 命令为用户提供了一个机会，来决定是继续部署还是在部署 QA 之后终止 CD 管道。 如果用户决定继续，则执行最后一个 stage：Deploy to Prod： dir(\"nginx-cd\") { sh \"\"\" helm upgrade --install nginx-${env.BRANCH_NAME} aiops/nginx --values common-values.yaml --values prod/values.yaml -n prod --wait kubectl -n prod exec deploy/nginx-${env.BRANCH_NAME} -- env | grep ENVIRONMENT \"\"\" } 这个 stage 与 “Deploy to Dev” 和 “Deploy to QA” 两个 stage 几乎相同，除了值和命名空间。 整个 CD 管道介绍完了，我们看一下它的执行。 运行 CD 管道 要查看 CD pipeline 的实际操作，在 Jenkins 页面点击 “deployNginxChart” job 的 main 分支，单击 #1 查看日志输出。当导航到日志时，你会看到一个提示，显示 “Deploy to Prod”？我们将很快解决这个问题。首先，让我们回顾日志的开头部分，查看管道到目前为止的执行情况。 你已看到第一个部署的是 dev 环境： 19:14:51 + helm upgrade --install nginx-main aiops/nginx --values common-values.yaml --values dev/values.yaml -n dev --wait 19:14:56 Release \"nginx-main\" has been upgraded. Happy Helming! 19:14:56 NAME: nginx-main 19:14:56 LAST DEPLOYED: Fri Aug 13 11:14:53 2021 19:14:56 NAMESPACE: dev 19:14:56 STATUS: deployed 19:14:56 REVISION: 34 19:14:56 NOTES: 19:14:56 1. Get the application URL by running these commands weiwendi: 19:14:56 export NODE_PORT=$(kubectl get --namespace dev -o jsonpath=\"{.spec.ports[0].nodePort}\" services nginx-main) 19:14:56 export NODE_IP=$(kubectl get nodes --namespace dev -o jsonpath=\"{.items[0].status.addresses[0].address}\") 19:14:56 echo http://$NODE_IP:$NODE_PORT 19:14:56 + kubectl -n dev exec deploy/nginx-main -- env 19:14:56 + grep ENVIRONMENT 19:14:56 ENVIRONMENT=dev 然后，你应该看到冒烟测试，这是由 helm test 命令运行的： + helm test nginx-master -n dev Pod nginx-master-test-connection pending Pod nginx-master-test-connection pending Pod nginx-master-test-connection succeeded NAME: nginx-master LAST DEPLOYED: Thu Apr 30 02:07:55 2020 NAMESPACE: dev STATUS: deployed REVISION: 1 TEST SUITE: nginx-master-test-connection Last Started: Thu Apr 30 02:08:03 2020 Last Completed: Thu Apr 30 02:08:05 2020 Phase: Succeeded19:14:57 + helm test nginx-main -n dev 19:15:05 NAME: nginx-main 19:15:05 LAST DEPLOYED: Fri Aug 13 11:14:53 2021 19:15:05 NAMESPACE: dev 19:15:05 STATUS: deployed 19:15:05 REVISION: 34 19:15:05 TEST SUITE: nginx-main-test-connection 19:15:05 Last Started: Fri Aug 13 11:14:57 2021 19:15:05 Last Completed: Fri Aug 13 11:15:04 2021 19:15:05 Phase: Succeeded 冒烟测试之后，是 qa 环境的部署： 19:15:06 + helm upgrade --install nginx-main aiops/nginx --values common-values.yaml --values qa/values.yaml -n qa --wait 19:15:32 Release \"nginx-main\" has been upgraded. Happy Helming! 19:15:32 NAME: nginx-main 19:15:32 LAST DEPLOYED: Fri Aug 13 11:15:28 2021 19:15:32 NAMESPACE: qa 19:15:32 STATUS: deployed 19:15:32 REVISION: 17 19:15:32 NOTES: 19:15:32 1. Get the application URL by running these commands weiwendi: 19:15:32 export NODE_PORT=$(kubectl get --namespace qa -o jsonpath=\"{.spec.ports[0].nodePort}\" services nginx-main) 19:15:32 export NODE_IP=$(kubectl get nodes --namespace qa -o jsonpath=\"{.items[0].status.addresses[0].address}\") 19:15:32 echo http://$NODE_IP:$NODE_PORT 19:15:32 + kubectl -n qa exec deploy/nginx-main -- env 19:15:32 + grep ENVIRONMENT 19:15:32 ENVIRONMENT=qa 然后是 input stage，就是最初我们看到的内容： 7.11 Proceed or Abort 在 “Stage View” 页面单击 “Proceed” 继续 pipeline 的执行（单击 “Abort” 将导致 pipeline 终止，并阻止生产环境的部署）。然后，你将看到 prod 部署的提示如下： 19:19:31 + helm upgrade --install nginx-main aiops/nginx --values common-values.yaml --values prod/values.yaml -n prod --wait 19:19:35 Release \"nginx-main\" has been upgraded. Happy Helming! 19:19:35 NAME: nginx-main 19:19:35 LAST DEPLOYED: Fri Aug 13 11:19:33 2021 19:19:35 NAMESPACE: prod 19:19:35 STATUS: deployed 19:19:35 REVISION: 1 最后，如果生产部署成功，您将在管道的末尾看到以下消息： [Pipeline] End of Pipeline Finished: SUCCESS 可以从命令行手动验证部署是否成功。使用 helm list 命令查看 nginx-master 版本： $ helm list -n dev $ helm list -n qa $ helm list -n prod 每个命令都应该在对应的命名空间中列出 nginx release： NAME NAMESPACE REVISION nginx-master dev 1 你还可以使用 kubectl 列出每个命名空间中的 Pods，并验证是否部署了 Nginx： $ kubectl get Pods -n dev $ kubectl get Pods -n qa $ kubectl get Pods -n prod 每个命名空间的结果如下（dev 还将有一个冒烟测试阶段的 test pod）： NAME READY STATUS RESTARTS AGE nginx-fcb5d6b64-rmc2j 1/1 Running 0 46m 在本节中，我们讨论了如何在 CD 管道中使用 Helm 在 Kubernetes 中的多个环境中部署应用程序。管道依赖于 GitOps 存储配置(values.yaml 文件)的实践。并引用这些文件来正确配置 Nginx。了解了 Helm 如何在 CD 环境中使用后，现在就可以清理 Minikube 集群了。 环境清理 要清理本章练习中的 Minikube 集群，请删除 chapter7、dev、qa 和 prod namespaces： $ kubectl delete ns chapter7 $ kubectl delete ns dev $ kubectl delete ns qa $ kubectl delete ns prod 关闭 Minikube VM： $ minikube stop By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-24 18:52:04 "},"helm/operator.html":{"url":"helm/operator.html","title":"Helm 与 Operator","keywords":"","body":"8 Helm 与 Operator Helm 的特性之一是能够同步本地状态和运行时状态。本地状态由 Helm 值文件管理，在执行 install 或 upgrade 命令时提供这些值，这些值就会被应用到 Kubernetes 集群中的程序运行时状态。在前面的章节中，程序需要更改时，我们就调用了这些命令。 同步这些更改，还有另外一种方法，那就是在集群中创建一个应用程序，定期检查所需的状态是否与当前环境中配置的状态相同。如果状态不匹配，应用程序可以自动修改环境中配置的状态，以匹配所需的状态。这个应用程序被称为 Kubernetes Operator。在本章中，我们将创建一个基于 Helm 的 Operator，它确保本地定义的状态始终与集群的运行时状态相同。如果不相同，Operator 将执行相应的 Helm 命令更新运行时状态。 8.1 技术要求 本章使用到的命令如下： minikube helm kubectl 除了这些工具外，你还可以在 GitHub 的 https://github.com/weiwendi/learn-helm 上找到包含相关示例资源的资源库。这个存储库将在本章中被引用。 8.2 Kubernetes Operators 自动化是 Kubernetes 平台的核心，正如我在第一章中介绍的，Kubernetes 资源可以通过 kubectl 隐式管理，也可以通过 YAML 格式的文件进行声明式管理。一旦使用 Kubernetes Command-Line Interface (CLI) 应用了资源，Kubernetes 就会将集群内资源的当前状态与所需状态匹配，这一过程称为控制循环。这种持续的、无终止的集群状态监控模式是通过使用控制器实现的。Kubernetes 包含了许多原生控制器，示例包括从拦截到 Kubernetes API(应用程序编程接口)请求的准入控制器，以及管理正在运行的 Pod 副本数量的副本控制器(ReplicationController)。 为了提高用户的使用热情，Kubernetes 除了提供扩展基础平台功能的能力，以及提供管理应用程序生命周期的更多智能的方法外，还提出了几个重要的发展趋势，这些趋势引导了 Kubernetes 开发的二次浪潮。首先，定制资源定义(CRDs)的引入使用户能够扩展默认的 Kubernetes API(与Kubernetes平台交互的机制)，以便创建和注册新的资源类型。注册一个新的 CRD 会在 Kubernetes API 上创建一个新的标示性状态传输(RESTful)资源路径。因此，与使用 Kubernetes CLI 执行 kubectl get pods 查看所有 Pod 对象类似，假如为名称是 player-stats 的对象类型注册一个新的 CRD， 就可以通过 kubectl get player-stats 来查看以前创建的所有 player-stats 对象。随着这一新功能的实现，开发人员可以创建自己的控制器来监视这些类型的 CRs，从而管理可以通过使用 CRDs 描述的应用程序生命周期。 第二个主要趋势是，复杂和有状态的应用程序被更频繁地部署在 Kubernetes 上。和小型、简单的应用程序不同，这类应用程序管理和维护起来通常更麻烦，例如部署多个组件，以及围绕 “day2” 的备份与恢复。这些超出了 Kubernetes 原生控制器的处理能力，因为实现这些需要对该应用程序、或该组服务有深入的了解。使用 CR 来管理应用程序及其组件的模式称为 Operator 模式。该项目由 CoreOS 软件公司于 2016 年首创，目的是获取运维人员管理应用程序生命周期所需的知识。Operators 被打包为容器化的应用程序，部署在 Pod 中，这些应用程序会根据 CRs 对 API 的更改做出对应的反应。 可以使用 Operator Framework 工具箱，基于以下三种技术中的一种编写 Operators： Ansible Helm Go 基于 Go 的 Operators 是利用 Go 语言来实现控制循环逻辑。基于 Ansible 的 Operators 利用 Ansible CLI 工具和 Ansible playbooks。Ansible 是一个自动化工具，我在《配置管理工具》部分有过详细介绍，它的逻辑是在 PlayBooks 的 YAML 文件中编写的。 在本章中，我们将重点关注基于 Helm 的 Operators。Helm Operators 基于 Helm 图表和 Helm CLI 提供的功能子集，来建立它们的循环逻辑，简化了 Helm 用户实现 Operators 的复杂度。 在理解了 Operators 之后，让我们使用 Helm 创建一个属于我们自己的 Operator。 8.3 创建Helm Operator 在本节中，我们编写一个基于 Helm 的 Operator，用于安装 player-stats Helm 图表。这个图表可以在图表存储库中找到，页面地址：https://github.com/weiwendi/learn-helm/tree/main/helm-charts/charts/player-stats。 Operator 被构建为用于维护应用程序的控制循环逻辑的容器镜像。下图演示了 Player-stats Operator 在部署后是如何工作的： 8.1 cr install helm chart Player-stats Operator 会不断观察 Player-stats CRs 的变化。当一个 Player-stats CR 被创建，Player-stats Operator 将安装 player-stats 图表。如果删除了 Player-stats CR，则 Player-stats Operator 将删除 player-stats Helm 图表。 8.4 设置环境 Operator 需要部署到 Kubernetes 中，因此我们应先通过以下命令来启动 Minikube 环境： $ minikube start 启动 Minikube 后，创建一个名为 operators 的命名空间： $ kubectl create ns operators Player-stats Operator 是作为容器镜像构建的，我们需要有一个存放容器镜像的仓库，以便使用镜像。可以在 Quay (quay.io) 中创建一个镜像存储库，这是一个公共的容器注册中心。如果你有其他存储库的账号，也是可以的。我们还需要准备一个包含构建 Operator 镜像所需工具的本地开发环境。 让我们首先在 Quay 中创建一个新的镜像存储库。 8.4.1 创建 QUAY 存储库 在 Quay 中创建存储库，首先你需要有 Redhat 帐户。按照以下步骤创建 Redhat 帐户： 在浏览器中访问 https://quay.io/signin/ 页面，也可以直接使用 Redhat 账号登录。 根据提示进行 Redhat 账号创建。 创建完成后，使用新账号登录 Quay。 创建了 Quay 账号后，就可以继续操作镜像创建新的镜像仓库了。 要创建一个新的镜像存储库，选择 Quay 页面右上角的 + 图标，然后选择 “New Repository”，如下图所示： 8.2 create quay image repo 接着会进入Create New Repository 页面，输入以下详细信息创建存储库： 选择 Container Image Repository 输入 Repository 名称：playerstats-operator。 选择 Public 按钮，表示访问存储库时不需身份验证，简化 Kubernetes 访问镜像的方式。 其余选项可以保持默认值。一旦完成，Create Public Repository 页面应该出现如下图所示的内容： 8.3 create public image repository 点击 Create Public Repository 按钮，创建 Quay 镜像存储库。 现在已经创建了存储 Player-stats Operator 镜像的存储库，让我们准备一个环境，其中包含构建 Helm Operator 所需的工具。 8.4.2 准备本地开发环境 为了创建 Helm Operator，你至少需要以下 CLI 工具： operator-sdk docker/podman/buildah operator-sdk CLI 是一个工具包，用于帮助开发 Kubernetes Operators。它包含了简化 Operator 开发过程的内在逻辑。在本质上，operator-sdk 需要一个容器管理工具，用来构建 Operator 的镜像。operator-sdk CLI支持docker、podman、build等底层容器管理工具。 要安装 operator-sdk CLI，只需从它的 GitHub 仓库 https://github.com/operator-framework/operator-sdk/releases 下载软件包安装即可。安装 docker、podman或者 buildah 的过程就会有所不同，这取决于你使用的操作系统。operator-sdk 没有Windows版的。 幸运的是，Minikube VM 可以作为许多不同操作系统开发人员的工作环境，因为它是一个 Linux 虚拟机，并且包含了 Docker CLI。在本节中，我们将把 operator-sdk 安装到 Minikube VM 中，并使用这个环境创建 Operator。虽然我们是在 VM 中进行的，但这些步骤适用于所有 Linux 和 Mac 主机。 跟着以下步骤，将 operator-sdk 安装到 Minikube VM 上： 使用 minikube ssh 命令登录虚拟机： $ minikube ssh 登录 VM 后，下载 operator-sdk CLI。编写此文档时最新版本为 1.11.0，我们使用 curl 命令下载该版本： $ curl -o operator-sdk -L https://github.com/operator-framework/operator-sdk/releases/download/v1.11.0/operator-sdk_linux_amd64 下载完后后，修改为可执行文件，并移动到PATH路径中： $ chmod u+x operator-sdk $ sudo mv operator-sdk /usr/bin 最后，通过运行 operator-sdk version 命令验证 operator-sdk 安装是否成功： $ operator-sdk version operator-sdk version: \"v1.11.0\", commit: \"28dcd12a776d8a8ff597e1d8527b08792e7312fd\", kubernetes version: \"1.20.2\", go version: \"go1.16.7\", GOOS: \"linux\", GOARCH: \"amd64\" 如果执行该命令没有报错，说明 operator-sdk CLI 安装成功。 作为附件步骤，在 Minikube VM 中克隆图表存储库，因为我们稍后将使用 player-stats Helm 图表构建 Helm Operator： $ git clone https://github.com/weiwendi/learn-helm.git 现在，Quay 镜像仓库和 Minikube VM 本地开发环境都准备好了，让我们开始编写 Player-stats Operator。这里有一个 Operator 的参考示例：https://github.com/weiwendi/learn-helm/tree/main/playerstats-operator。 8.4.3 Operator 文件结构 与 Helm 图表类似，operator-sdk CLI 构建的 Helm Operators 也有一个必须遵守的文件结构，如下表所示： 文件或目录 定义 config 存放在集群上启动项目的配置文件 Dockerfile Operator 项目的 Dockerfile，用于构建镜像 helm-charts/ 包含 Operator 负责安装的 Helm 图表 Makefile 使用 make 时会引用此文件辅助构建项目 PROJECT 包含了项目的配置信息 watches.yaml 定义 Operator 负责监视的自定义资源的文件，包括 Group、Version、Kind、Helm 图表位置 创建 playerstats-operator： $ midir playerstats-operator $ cd playerstats-operator 使用 operator-sdk init 命令轻松创建 Operator 文件结构 这创建了Nginx -operator项目，专门用于使用APIVersion查看Nginx资源 使用 operator-sdk init 命令创建基于 Helm 的 playerstats-operator 项目。playerstats-operator 项目专门用于 APIVersion 查看 Player-stats 资源： $ operator-sdk init --plugins helm --domain quay.io --helm-chart ~/learn-helm/helm-charts/charts/player-stats --kind Player-stats Writing kustomize manifests for you to edit... Creating the API: $ operator-sdk create api --kind Player-stats --helm-chart /home/aiops/learn-helm/helm-charts/charts/player-stats Writing kustomize manifests for you to edit... Created helm-charts/player-stats Generating RBAC rules WARN[0010] The RBAC rules generated in config/rbac/role.yaml are based on the chart's default manifest. Some rules may be missing for resources that are only enabled with custom values, and some existing rules may be overly broad. Double check the rules generated in config/rbac/role.yaml to ensure they meet the operator's permission requirements. --plugins helm 指定创建基于 Helm 的 operator；--kind 指定 CR 的名称为 Player-stats，该值必须以大写字母开头；--domain quay.io 指定 operator 的镜像仓库地址为 quay.io；--helm-chart 指示 operator-sdk CLI 将 player-stats 图表拷贝到 playerstats-operator 目录。 成功创建了 player-stats operator 后，我们构建 Operator 镜像并将其推到 Quay 注册中心。 8.4.4 构建 Operator 镜像并推送到 Quay Makefile 文件中 IMAGE_TAG_BASE 的值，定义了镜像的注册中心地址、镜像仓库名称以及镜像的 tag。该值是在执行 operator-sdk init 命令时生成的，注册中心是 --domain 提供的，仓库名称就是所在目录的名称：playerstats-operator，tag 是该 operator 的版本号。当然，如果你想修改这些值，可以编辑 Makefile 文件进行修改，也可以通过命令行修改。 在本地构建完 operator 镜像，需要把它推送到镜像注册中心，以便 Kubernetes 拉取。使用 Docker 将镜像推送到注册中心，需要通过身份验证，使用 docker login 命令登录到 Quay： $ docker login quay.io --username $YOUR_QUAY_USERNAME --password $YOUR_QUAY_PASSWORD 构建 Operator 镜像并推送到注册中心，别忘记把 IMG 中的 sretech 修改成你的 quay 账号： $ make docker-build docker-push IMG=\"quay.io/sretech/playerstats-operator:v0.0.1\" docker build -t quay.io/sretech/playerstats-operator:v0.0.1 . Sending build context to Docker daemon 149kB Step 1/5 : FROM quay.io/operator-framework/helm-operator:v1.11.0 ---> e8bc8fc22395 Step 2/5 : ENV HOME=/opt/helm ---> Using cache ---> 939f3e320dc2 Step 3/5 : COPY watches.yaml ${HOME}/watches.yaml ---> Using cache ---> dd1aaf3731cc Step 4/5 : COPY helm-charts ${HOME}/helm-charts ---> Using cache ---> 11c08abe328a Step 5/5 : WORKDIR ${HOME} ---> Using cache ---> d50c86585eb0 Successfully built d50c86585eb0 Successfully tagged quay.io/sretech/playerstats-operator:v0.0.1 docker push quay.io/sretech/playerstats-operator:v0.0.1 The push refers to repository [quay.io/sretech/playerstats-operator] 49108043bfd4: Pushed 1fd26746c559: Pushed 81c1dccd8779: Mounted from operator-framework/helm-operator 337cb014e36c: Mounted from operator-framework/helm-operator 83ab849ae91d: Mounted from operator-framework/helm-operator e7ed17121dee: Mounted from operator-framework/helm-operator 785573c4b945: Mounted from operator-framework/helm-operator v0.0.1: digest: sha256:753559ae39712cdf5173f60d7afbd485e6f435dc1f05f709ebb4b65414e87f76 size: 1779 再次查看 quay.io 的 playerstats-operator 仓库页面，点击 tag 标签，可以看到我们上传的镜像，如图： 8.4 player stats operator image 现在 operator 镜像已经被推送到了容器注册中心，接下来我们将其部署到 Kubernetes 中。 8.4.5 部署 playerstats operator 在创建 playerstats operator 时，operator-sdk CLI 还创建了一个名为 config/ 的文件夹，并在该文件下生成了部署 operator 所需的文件。 下面是 config/ 文件夹的结构： config/ ├── crd │ ├── bases │ │ └── charts.quay.io_player-stats.yaml │ └── kustomization.yaml ├── default │ ├── kustomization.yaml │ ├── manager_auth_proxy_patch.yaml │ └── manager_config_patch.yaml ├── manager │ ├── controller_manager_config.yaml │ ├── kustomization.yaml │ └── manager.yaml ├── manifests │ └── kustomization.yaml ├── prometheus │ ├── kustomization.yaml │ └── monitor.yaml ├── rbac │ ├── auth_proxy_client_clusterrole.yaml │ ├── auth_proxy_role_binding.yaml │ ├── auth_proxy_role.yaml │ ├── auth_proxy_service.yaml │ ├── kustomization.yaml │ ├── leader_election_role_binding.yaml │ ├── leader_election_role.yaml │ ├── player-stats_editor_role.yaml │ ├── player-stats_viewer_role.yaml │ ├── role_binding.yaml │ ├── role.yaml │ └── service_account.yaml ├── samples │ ├── charts_v1alpha1_player-stats.yaml │ └── kustomization.yaml └── scorecard ├── bases │ └── config.yaml ├── kustomization.yaml └── patches ├── basic.config.yaml └── olm.config.yaml 11 directories, 29 files crd/ 文件夹包含了创建 player-stats CRD 所需的 YAML 资源(charts.quay.io_player-stats.yaml)。该文件用于向 Kubernetes 注册新的 Player-stats API 端点。此外 samples/ 文件夹还包含一个 Player-stats CR 应用程序示例(charts_v1alpha1_player-stats.yaml)。创建此文件将触发 operator 安装 player-stats Helm 图表。 查看 Player-stats CR 的内容，以便熟悉定义的属性类型： $ cat playerstats-operator/config/samples/charts_v1alpha1_player-stats.yaml apiVersion: charts.quay.io/v1alpha1 kind: Player-stats metadata: name: player-stats-sample spec: # Default values copied from /helm-charts/player-stats/values.yaml affinity: {} autoscaling: enabled: false maxReplicas: 100 minReplicas: 1 targetCPUUtilizationPercentage: 80 fullnameOverride: \"\" image: pullPolicy: IfNotPresent repository: registry.cn-beijing.aliyuncs.com/sretech/player-stats tag: \"\" imagePullSecrets: [] ingress: annotations: {} enabled: false ...... spec 下的条目引用了 player-stats 图表的 values.yaml 文件中的值。operator-sdk 工具自动使用该文件中的每个默认值创建这个示例 CR。可以在应用此 CR 前添加或修改条目，以覆盖 player-stats 图表的其他值。operator 在运行时使用这些值部署 player-stats 应用程序。 rbac/ 目录下的 role_binding.yaml、role.yaml、service_account.yaml 文件的创建是为了向 operator 提供必要的权限，以便监视 Player-stats CRs 和安装 player-stats Helm 图表到 Kubernetes。它通过 service_account.yaml 中定义的 service account，使用 Kubernetes API 进行身份验证来执行这些操作。一旦身份验证通过，将根据 role.yaml、role_binding.yaml 资源向 operator 提供授权。role.yaml 文件定义了细粒度的权限，这些权限允许 operator 执行确切的资源和操作。role_binding.yaml 文件将角色绑定到 operator 的 service account 上。 了解了 rbac/ 文件夹中的资源后，就可以按照以下步骤部署 player-stats operator 了： Minikube VM中没有kubectl命令，如果你还登录在VM中，需要使用以下命令退出连接： $ exit 前面用 operator-sdk 创建的资源位于源码存储库的 playerstats-operator/ 文件夹下。如果你没有在之前的章节中克隆这个存储库，现在使用以下命令克隆它： $ git clone https://github.com/weiwendi/learn-helm.git operator-sdk CLI 基于 player-stats Helm 图表中的模板文件生成了一个简单的 role.yaml 文件，但如果你还记得的话，player-stats 图表包含一些资源，这些资源只能基于条件值创建，如 Job 钩子资源，只有启用了持久性存储才会包含这些资源。下面的代码片段显示了 Job 模板的一个例子： NaN apiVersion: batch/v1 kind: Job operator-sdk CLI 没有自动为 Jobs 创建基于角色的访问控制(RBAC)规则，因为它不确定要部署的图表中是否包含了这个模板。 因此，我将这些规则添加到了role.yaml 文件，位于https://github.com/weiwendi/learn-helm/blob/main/playerstats-operator/config/rbac/role.yaml#L62-L67. - verbs: - \"*\" apiGroups: - \"batch\" resources: - \"jobs\" 有三种方式运行 Operator，我们采用在集群中运行 Deployment 的方式，其他的方式可以参考官网介绍 https://sdk.operatorframework.io/docs/building-operators/helm/tutorial/。但在运行 Operator 前，我们再做一些辅助性的修改，帮助我们更顺利的运行 Operator。 修改 Makefile 中 kustomize 工具的获取方式。在 Makefile 原文件中，使用 curl 命令从 https://github.com/kubernetes-sigs/kustomize/releases 下载 kustomize 的 tar 包，并解压到 Makefile 同级目录的 bin/ 目录中，这个下载过程耗时比较长，网络不稳定的环境中容易出现异常，我们可以提前准备好该文件，并把 curl 命令换成 cp 命令： # 原命令类似如下： curl -sSLo - https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize/v3.8.7/kustomize_v3.8.7_$(OS)_$(ARCH).tar.gz | \\tar xzf - -C bin/ ;\\ # 下载对应的 tar.gz 包并解压到某个目录，从该目录进行拷贝 # 注意 kustomize 文件要有执行权限 cp /root/software/kustomize $(dir $(KUSTOMIZE)) ;\\ 修改 kube-rbac-proxy 镜像。运行 Operator 需要 kube-rbac-proxy 镜像，默认是 gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0 的镜像仓库，我们把镜像改成 registry.cn-beijing.aliyuncs.com/sretech/kube-rbac-proxy:v0.8.0： [aiops@aiops0113 playerstats-operator]$ sed -i 's/gcr.io\\/kubebuilder\\/kube-rbac-proxy:v0.8.0/registry.cn-beijing.aliyuncs.com\\/sretech\\/kube-rbac-proxy:v0.8.0/g' config/default/manager_auth_proxy_patch.yaml 需要注意的是，以后镜像 tag 可能会发生变化，不再是 v0.8.0，这时再使用上面的命令，会替换失败。 接下来执行 make deploy 命令，安装 config/rbac 中的清单： make deploy IMG=\"quay.io/sretech/playerstats-operator:v0.0.1\" 验证 playerstats-operator 是否启动： kubectl get deployment -n playerstats-operator-system NAME READY UP-TO-DATE AVAILABLE AGE playerstats-operator-controller-manager 1/1 1 1 8m 现在，Player-stats Operator 已经部署，让我们使用它来安装 Player-stats Helm 图表。 8.4.6 部署 player-stats 应用程序 将 Helm 用作独立工具时，可以使用 helm install 命令安装 Helm 图表。使用 Helm operator，可以通过创建 CR 安装 Helm 图表： $ kubectl apply -f playerstats-operator/config/samples/charts_v1alpha1_player-stats.yaml -n operators 观察 operators 命名空间中的 Pod 启动过程： $ kubectl get pods -n operators -w NAME READY STATUS RESTARTS AGE mongo-fc6f6b868-s4lfg 0/1 ContainerCreating 0 10s player-stats-sample-5599464bfd-6bzzn 0/1 Running 0 10s player-stats-sample-5599464bfd-6bzzn 0/1 Running 1 42s player-stats-sample-5599464bfd-6bzzn 0/1 Running 2 74s player-stats-sample-5599464bfd-6bzzn 0/1 Running 3 112s mongo-fc6f6b868-s4lfg 0/1 Running 0 118s mongo-fc6f6b868-s4lfg 1/1 Running 0 2m3s player-stats-sample-5599464bfd-6bzzn 1/1 Running 3 2m13s 当创建 Player-stats CR 时，operator 会执行 helm install 命令安装 player-stats 图表。可以通过 helm list 确认release： $ helm list -n operators NAME NAMESPACE REVISION UPDATED player-stats-sample operators 1 2021-08-26 07:33:24 可以通过修改 player-stats-sample CR 来执行版本升级，将 playerstats-operator/config/samples/charts_v1alpha1_player-stats.yaml 文件中 replicas 的值从 1 改为 2： replicaCount: 2 应用更改： $ kubectl apply -f playerstats-operator/config/samples/charts_v1alpha1_player-stats.yaml -n operators 这时将触发 helm upgrade 命令，player-stats Helm 图表的升级又会触发 upgrade hook 备份 MongoDB 数据。如果你在修改 CR 后对 operators 命名空间中的 Pod 进行监控，能看到备份 Job 开始执行。还可以使用 helm list 命令查看 player-stats-sample 的版本变成了 2： $ helm list -n operators NAME NAMESPACE REVISION player-stats-sample operators 2 尽管修订号变成了 2，但截至编写此文时，基于 helm operator 的一个限制是，不能像使用 helm cli 那样对以前版本进行回滚。如果你对 player-stats-sample 执行 helm history，查看发布历史，只能看到版本 2： $ helm history player-stats-sample -n operators REVISION UPDATED STATUS 2 Thu Aug 26 08:18:27 2021 deployed 这是 helm cli 和 helm operator 之间的一个重要区别。因为不保留发布历史，所以基于 Helm operator 不允许执行显示回滚。但是，在升级失败时，将运行 helm rollback 命令。在这种情况下，将执行回滚 hook，尝试回滚到升级前版本。 尽管基于 Helm 的 operator 不保留发布历史，但它有一个优势就是同步 application 的期望状态和运行时状态。这是因为 operator 能够不断监视 Kubernetes 环境的状态，并确保 application 的配置始终与 CR 上指定的配置相同。换句话说，如果修改了 Player stats application 的一个资源，operator 会立即恢复更改，使其与 CR 的定义相匹配。你可以通过修改 Player stats 的某个资源来验证这一点。 例如，我们将直接将 Player stats 部署的副本数从 2 更改为 3，并观察 operator 将其自动恢复为 2 个副本，以重新同步为 CR 中定义的所需状态。 执行 kubectl patch 命令将 Player stats 部署的副本计数从 2 修改为 3： $ kubectl patch deployment player-stats-sample -p '{\"spec\":{\"replicas\":3}}' -n operators 通常，这会增加一个 player-stats application 副本，但是由于 Player-stats CR 定义的副本数为 2，Operator 会迅速将副本数修改为 2，并终止新增 Pod 的创建。如果你需要把副本数调整为 3，必须更新 Player-stats CR上的 replicaCount 值。这个特性确保了集群运行时状态与配置状态保持一致。 8.4.7 删除 Operator 先删除 player-stats-sample CR，并卸载 Helm release： $ kubectl delete -f playerstats-operator/config/samples/charts_v1alpha1_player-stats.yaml -n operators 该命令会删除 player-stats-sample 和所有依赖的资源。 在资源删除后，就可以删除 Operator 了： $ make undeploy /tmp/test/playerstats-operator/bin/kustomize build config/default | kubectl delete -f - namespace \"playerstats-operator-system\" deleted customresourcedefinition.apiextensions.k8s.io \"player-stats.charts.quay.io\" deleted serviceaccount \"playerstats-operator-controller-manager\" deleted role.rbac.authorization.k8s.io \"playerstats-operator-leader-election-role\" deleted clusterrole.rbac.authorization.k8s.io \"playerstats-operator-manager-role\" deleted clusterrole.rbac.authorization.k8s.io \"playerstats-operator-metrics-reader\" deleted clusterrole.rbac.authorization.k8s.io \"playerstats-operator-proxy-role\" deleted rolebinding.rbac.authorization.k8s.io \"playerstats-operator-leader-election-rolebinding\" deleted clusterrolebinding.rbac.authorization.k8s.io \"playerstats-operator-manager-rolebinding\" deleted clusterrolebinding.rbac.authorization.k8s.io \"playerstats-operator-proxy-rolebinding\" deleted configmap \"playerstats-operator-manager-config\" deleted service \"playerstats-operator-controller-manager-metrics-service\" deleted deployment.apps \"playerstats-operator-controller-manager\" deleted 应该确保在删除 Operator 之前先删除 CR。当你删除 CR 时，Operator 会执行 helm uninstall 命令。如果你不小心先删除了 Operator，需要手动运行 helm uninstall，删除相关资源。 在本节中，你创建了一个 Helm operator ，并学习了如何使用基于 operator 的方法部署应用程序。在下一节中，我们将继续讨论如何使用 Helm 来管理 operators。 Operator 的相关知识就讨论到这里，接下来清理实验环境。 8.5 清理 Kubernetes 环境 删除 operators 命名空间： $ kubectl delete ns operators 最后，停止运行Minikube： $ minikube stop By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-31 14:21:05 "},"helm/plugins.html":{"url":"helm/plugins.html","title":"Helm 插件","keywords":"","body":"9 Helm 插件 如我们所见，Helm 有很多特性和方法，帮助使用者在 Kubernetes 上部署应用程序。当然，我们也可以自定义和扩展 Helm 的功能。 在本章中，我们讨论如何通过插件增强 Helm 功能。 插件可以向 Helm 添加额外的功能，与 Helm CLI 无缝集成，帮助用户完成特殊的工作流。网上有很多常用的第三方插件。此外，使用插件可以非常方便的部署一次性的任务。 通过插件，我们可以基于 Helm 现有的特性构建新功能，以简化和自动化日常工作流程。 9.1 插件介绍 Helm 插件是可以直接从 Helm CLI 访问的外部工具，支持自定义的子命令，而无需修改 Helm 源码。在设计上与 kubectl(Kubernetes CLI) 等工具实现插件系统的方法类似。 下载插件可以指定自定义协议与图表存储库通信，如果你有一些自定义身份验证方法，或者你需要以某种方式修改 Helm 从存储库获取图表的方法，那么这将非常有用。 9.2 安装第三方插件 大多第三方插件都是开源的，公开在 GitHub 上。这些插件中有很多使用了 helm-plugin tag/topic，方便查找。https://github.com/topics/helm-plugin。 一些可以在 GitHub 上找到的 Helm 插件示例： helm/helm-2to3：把 helm2 转换为 helm3 jkroepke/helm-secrets：管理 YAML 格式的 secrets 的插件 maorfr/helm-backup：备份 Helm releases 到文本文件，或从文本文件恢复 Helm releases karuppiah7890/helm-schema-gen：基于 values.yaml 生成 values.schema.json 的插件 hickeyma/helm-mapkubeapis：更新 Helm releases 元数据，包含过时的 Kubernetes APIs 找到要安装的插件，在其版本控制 URL 中可以获取 plugin.yaml 文件及其源代码。 例如，一个管理 Helm starters 的插件，位于 https://github.com/salesforce/helm-starter。这个插件的版本控制 URL 是 https://github.com/salesforce/helm-starter.git。 要安装这个插件，执行 helm plugin install，将版本控制 URL 作为第一个参数： $ helm plugin install https://github.com/salesforce/helm-starter.git Installed plugin: starter 安装成功后，便可以使用插件了： $ helm starter --help Fetch, list, and delete helm starters from github. Available Commands: helm starter fetch GITURL Install a bare Helm starter from Github (e.g., git clone) helm starter list List installed Helm starters helm starter delete NAME Delete an installed Helm starter --help Display this text 要列出已安装的插件： $ helm plugin list NAME VERSION DESCRIPTION starter 1.0.0 This plugin fetches, lists, and deletes helm starters from github. 也可以在插件目录中查看已安装的插件，首先检查插件存储根目录的配置路径： $ helm env |grep HELM_PLUGINS HELM_PLUGINS=/home/aiops/.local/share/helm/plugins 如果想使用自定义的插件根目录，可以设置当前环境中的 HELM_PLUGINS 环境变量，以提供自定义的插件根目录。 更新插件： $ helm plugin update starter Updated plugin: starter 卸载已安装的插件： $ helm plugin remove starter Uninstalled plugin: starter 默认情况下，Helm 在安装插件时，将使用 plugin.yaml 和 Git repo 默认分支上的源码。如果要安装指定的 Git tag，需要使用 --version 标志： $ helm plugin install https://github.com/databus23/helm-diff.git --version v3.1.0 也可以使用 tarball URL 安装插件。Helm 将下载 tarball 并解压到 plugins 目录中： $ helm plugin install https://example.com/archives/myplugin-0.6.0.tar.gz 从本地目录安装插件： $ helm plugin install /path/to/myplugin # Helm将创建到原始文件的符号链接，而不是复制文件 $ ls -la \"$(helm env HELM_PLUGINS)\" total 8 drwxrwxr-x 2 myuser myuser 4096 Jul 3 21:49 . drwxrwxr-x 4 myuser myuser 4096 Jul 1 21:38 .. lrwxrwxrwx 1 myuser myuser 21 Jul 3 21:49 myplugin -> /path/to/myplugin 这样的好处是，当你在频繁开发、修改一个插件时，符号链接会和源文件保持一致。 9.3 自定义子命令 插件有许多特性，可以实现与现有 Helm 用户体验的无缝集成。Helm 插件最显著的特性可能是每个插件都为 Helm 提供了一个自定义的顶级子命令。这些子命令甚至可以使用 shell 补全。 一旦插件安装完成，就会新增一个与插件同名的子命令。这个新命令直接与 Helm 集成，甚至可以出现在 helm help 中。 假设我们安装了一个名为 inspect-templates 的插件，它可以在有关图表中查询 YAML 模板的类型信息。此插件将会提供一个新的子命令： $ helm inspect-templates [args] 该命令会执行 inspect-templates 插件，传递给调用插件时执行的底层工具的任何参数或标志。插件的作者定义了一些命令，Helm 应该在每次调用插件时作为子进程运行。 插件提供了一个很好的替代方案来扩充 Helm 现有的特性集，而不需要对 Helm 本身进行任何修改。 9.4 构建插件 构建 Helm 插件是一个相当简单的过程，根据需求和插件的整体复杂性，可能依赖一些编程知识；然而许多插件仅仅运行一个基本的 shell 命令即可实现。 9.4.1 底层实现 下面的 Bash 脚本 inspect-templates.sh，它是我们示例 inspect-templates 插件的底层实现： #!/usr/bin/env bash set -e # First argument on the command line, a relative path to a chart directory CHART_DIRECTORY=\"${1}\" # Second argument on the command line, is the depth of the directory to look for MAXDEPTH=\"${2}\" # Fail if no chart directory provided or is invalid if [[ \"${CHART_DIRECTORY}\" == \"\" ]]; then echo \"Usage: helm inspect-templates \" exit 1 elif [[ ! -d \"${CHART_DIRECTORY}\" ]]; then echo \"Invalid chart directory provided: ${CHART_DIRECTORY}\" exit 1 elif [[ \"${MAXDEPTH}\" == \"\" ]]; then MAXDEPTH=1 fi # Print a summary of the chart's templates cd \"${CHART_DIRECTORY}\" cd templates/ echo \"----------------------\" echo \"Chart template summary\" echo \"----------------------\" echo \"\" total=\"$(find . -maxdepth ${MAXDEPTH} -type f -name '*.yaml' | wc -l | tr -d '[:space:]')\" echo \" Total number: ${total}\" echo \"\" echo \" List of templates:\" for filename in $(find . -maxdepth ${MAXDEPTH} -type f -name '*.yaml' | sed 's|^\\./||'); do kind=$(cat \"${filename}\" | grep kind: | head -1 | awk '{print $2}') echo \" - ${filename} (${kind})\" done echo \"\" 当运行 helm inspect-templates 时，Helm 将在后台执行此脚本。 底层插件实现不局限于 Bash、Go 或任何特定的编程语言编写。对于这个插件的最终用户来说，它应该只是 Helm CLI 的一部分。 9.4.2 插件清单(manifest) 每个插件都由一个名为 plugin.yaml 的 YAML 文件定义。此文件包含插件元数据和有关调用插件时要运行的命令的信息。 下面是 inspect-templates 插件 plugin.yaml 的基本示例： name: inspect-templates #1 version: 0.1.0 #2 description: get a summary of a chart's templates #3 command: \"${HELM_PLUGIN_DIR}/inspect-templates.sh\" #4 1 插件的名称 2 插件的版本 3 插件的基本描述 4 调用插件时会执行的命令 9.6 自动补全 从 Helm 3.2 版本开始，可以为插件提供对 shell 的自动补全。 如果插件提供了自己的参数或者子命令，可以通过位于插件根目录下的 completion.yaml 文件，定义补全。completion.yaml 格式如下： name: flags: - - validArgs: - - commands: name: flags: - - validArgs: - - commands: 需要注意的是： 所有部分都是可选的，应该在适用时提供。 参数不该包含- 或 --前缀。 应该指定短和长参数。短参数不需要与其对应的长格式关联，但是都应被列出。 参数不需要以任何方式排序，但是需要列举在文件子命令层次结构的正确位置。 Helm 现有的全局参数已经由 Helm 的自动补全机制处理，因此插件不需要指定以下参数：--debug，--namespace 或 -n， --kube-context，以及 --kubeconfig，或者其他全局参数。 validArgs 列表提供了一个以下子命令的第一个参数可能补全的静态列表。 By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-08-31 14:18:48 "},"helm/security.html":{"url":"helm/security.html","title":"Helm 安全","keywords":"","body":"10 Helm 安全 经过这么多章的学习，你可能体会到 Helm 是一个功能强大的工具了，它为用户提供了部署各种应用程序的可能性。但如果未能意识到某些安全规范，或者未能严格遵守这些规范，可能会导致这种强大的功能被滥用，最终导致灾难性的结果。幸运的是，从下载 Helm CLI 那一刻起，到在 Kubernetes 集群中安装 Helm 图表，Helm 都提供了将安全性融入其中的便捷方法。 本章主要介绍的知识点如下： 数据来源和完整性 Helm 图表安全 关于 RBAC、值和图表仓库的注意事项 10.1 技术要求 本章会用到以下技术： minikube kubectl helm GNU Privacy Guard(GPG) 我们还会用到 https://github.com/weiwendi/learn-helm 中的 player-stats 图表，作为演示示例，如果你还没有将其 clone 到本地，需要确保主机能执行以下命令： $ git clone https://github.com/weiwendi/learn-helm.git 10.2 数据来源及完整性 在处理任何类型的数据时，都应该考虑两个经常被忽视的问题： 数据来源是可靠的吗，或者数据是来自你期望的数据源吗？ 数据是完整的吗？ 第一个问题涉及数据源，用来确定数据的来源。 第二个问题涉及数据完整性。数据完整性是为了确定远程获取的数据是否是你期望的数据，以及数据在传输过程中是否被篡改。可以使用数字签名的概念来验证数据来源及数据完整性。作者可以创建一个基于密码学的唯一签名对数据进行签名，数据的使用者可以使用密码工具来验证签名的真实性。 如果验证了签名的真实性，说明数据来自预期的数据源，并且在传输过程中没有被篡改。 作者可以通过创建一个 Pretty Good Privacy 密钥对创建数据签名。这里的 PGP 指的是 OpenPGP，它是一种基于密钥的加密方式。PGP 侧重于建立非对称加密，使用两种不同的密钥——私钥和公钥。 私钥是保密的，而公钥是被设计为共享的。对于数字签名，私钥用于加密数据，公钥用于解密数据。PGP 密钥对通常使用一种称为 GPG 的工具创建，它是一种实现 OpenPGP 标准的开源工具。 一旦创建了 PGP 密钥对，作者就可以使用 GPG 对数据进行签名。在数据签名完成后，GPG 在后台执行以下步骤： 根据内容计算出一个 hash 值，输出为一个固定的长度的字符串，称为消息摘要。 消息摘要使用作者的私钥进行加密。输出的是数字签名。 要验证签名，使用者必须使用作者的公钥对其解密。这种验证也可以使用 GPG 执行。 数字签名在 Helm 中起着两种作用： 首先，每个 Helm 下载都有一个来自维护人员的数字签名，可以用来验证二进制文件的真实性。签名可用于验证下载的来源以及完整性。 其次，也可以对 Helm 图表进行数字签名，这样就可以验证它的来源及完整性了。图表作者在打包过程中对图表进行签名，图表用户使用作者的公钥验证图表的有效性。 了解了数据来源、完整性与数字签名的关系后，我们在本地创建一个 GPG 密钥对(它将用于详细说明前面描述的许多概念。)。 10.3 创建密钥对 创建密钥对，需要先安装 GPG(Linux系统中可能已经默认安装了)： Windows： > choco install gnupg 也可以从 https://gpg4win.org/download.html 页面下载安装。 macOS： $ brew install gpg 也可以从 https://sourceforge.net/p/gpgosx/docu/Download/ 页面下载安装。 Debian-based Linux发行版： $ sudo apt install gnupg RPM-based Linux发行版： $ sudo dnf install gpg 安装完成后，就可以创建 GPG 密钥对了，步骤如下： 运行以下命令创建新的密钥对(该命令可以在任何目录下运行)： $ gpg --generate-key 安装提示输入姓名及邮箱。这两个信息将你标识为密钥对的所有者，接收公钥的人可以看到这两个信息。 按 O 键继续。 然后，系统将提示您输入私钥密码。输入并确认用于加密和解密操作的密码。 一旦创建了GPG密钥对，你会看到类似如下的输出： 9.1 gpg outputs pub：公钥信息，第二行是公钥的指纹。指纹是唯一的标识符，用于将你标识为该秘钥的所有者。 uid：显示生成 GPG 秘钥对时输入的名称及邮箱。 sub：私钥信息。 秘钥对已经创建完了，我们继续下一节的学习，如何验证 helm 下载。 10.4 验证 Helm 下载 在第二章中，我们讨论了 Helm 的部署，可以从 https://github.com/helm/helm/releases 页面下载 Helm 的不同版本，页面内容如下： 9.2 helm versions 在 Installation and Upgrading 底部，有一段内容说明了该版本已签名。每个 Helm 发行版都有维护人员签名，并可以与下载的 Helm 版本对应的数字签名进行验证。每个数字签名都位于 Assets 部分下。并可根据与下载的 Helm 发行版对应的数字签名进行验证。 每个数字签名都位于 Assets 下，截图显示了它们是如何表示的： 9.3 digital signature 要验证下载的 Helm 的来源和完整性，还应该下载相应的 .asc 文件。.sha256.asc 文件仅用于验证完整性。在本例中，我们下载相应的 .asc 文件，以验证数据来源及完整性。 按照以下步骤开始验证 Helm 发行版： 下载与操作系统对应的 Helm 压缩文件。如果你第二章安装后保留了该文件，则无需重新下载。 下载与操作系统对应的 .asc 文件，例如，你的操作系统是基于 amd64 的 Linux 系统，则需要下载 helm-v3.0.0-linux-amd64.tar.gz.asc 文件(文件名中包含的版本对应于你下载的 Helm 版本)。 下载完成后，应该在下载目录中看到以下两个文件： [aiops@aiops0113 software]$ ll -h total 12M -rw-rw-r--. 1 aiops aiops 12M Apr 14 17:20 helm-v3.5.4-linux-amd64.tar.gz -rw-rw-r--. 1 aiops aiops 833 Apr 14 17:29 helm-v3.5.4-linux-amd64.tar.gz.asc 下一步是将 Helm 维护者的公钥导入到本地秘钥环，解密 .asc 文件中包含的数字签名，以验证你下载的来源及完整性。可以通过链接到维护人员的 keybase account 来检索维护人员的公钥。将光标悬停在 keybase account (图9-2)上会出现该链接 https://keybase.io/mattfarina，在连接后面添加 /pgp_keys.asc 下载公钥 https://keybase.io/mattfarina/pgp_keys.asc。 因为有不同的 Helm 维护者，如果你验证的是其它版本，链接可能会和示例中不同。请确保下载的公钥与签署发行版的秘钥对应。 继续进行验证： 使用命令行下载 Helm 发行版签名对应的公钥： [aiops@aiops0113 software]$ curl -o release_key.asc https://keybase.io/mattfarina/pgp_keys.asc 下载之后，需要将公钥导入到 gpg 密钥环中： [aiops@aiops0113 software]$ gpg --import release_key.asc gpg: key 461449C25E36B98E: 3 signatures not checked due to missing keys gpg: key 461449C25E36B98E: public key \"Matthew Farina \" imported gpg: Total number processed: 1 gpg: imported: 1 gpg: marginals needed: 3 completes needed: 1 trust model: pgp gpg: depth: 0 valid: 1 signed: 0 trust: 0-, 0q, 0n, 0m, 0f, 1u gpg: next trustdb check due at 2023-09-01 导入数字签名的公钥后，就可以使用 GPG 的 --verify 子命令验证Helm的发行版了。针对 helm*.asc 文件执行以下命令： [aiops@aiops0113 software]$ gpg --verify helm-v3.5.4-linux-amd64.tar.gz.asc gpg: assuming signed data in 'helm-v3.5.4-linux-amd64.tar.gz' gpg: Signature made Wed 14 Apr 2021 05:25:20 PM EDT gpg: using RSA key 711F28D510E1E0BCBD5F6BFE9436E80BFBA46909 gpg: Good signature from \"Matthew Farina \" [unknown] gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: 672C 657B E06B 4B30 969C 4A57 4614 49C2 5E36 B98E Subkey fingerprint: 711F 28D5 10E1 E0BC BD5F 6BFE 9436 E80B FBA4 6909 该命令会尝试解密 .asc 文件中包含的数字签名，如果成功，意味着下载的 Helm 发行版(.tar.gz)源是我们期望的，且数据没有被篡改。 在上面的输出中，有一条 WARNING 消息，This key is not certified with a trusted signature!，此密钥没有通过可信签名认证！这条消息并不影响结果，验证是成功的，只是我们执行时，没有确认维护者的公钥是属于该维护者的。 可以通过以下步骤执行可信签名验证： 检查输出内容中的 Primary key fingerprint 一行最后64位(8个字符)，与 Helm releases 页面中显示的 64 位指纹是否匹配。指纹显示如下： This release was signed with 672C 657B E06B 4B30 969C 4A57 4614 49C2 5E36 B98E and can be found at @mattfarina keybase account. 指纹匹配，确认了是属于我们期望的人，因此我们可以安全地认证维护者的公钥。通过使用自己的 gpg 秘钥对公钥进行签名来实现： $ gpg --sign-key 461449C25E36B98E # Last 64 bits of fingerprint 在 Really sign? 提示中输入 y。 签署了维护者公钥，现在密钥已经被认证。我们再次执行 --verify 子命令验证 Helm 发行版： $ gpg --verify helm-v3.5.4-linux-amd64.tar.gz.asc gpg: assuming signed data in 'helm-v3.5.4-linux-amd64.tar.gz' gpg: Signature made Wed 14 Apr 2021 05:25:20 PM EDT gpg: using RSA key 711F28D510E1E0BCBD5F6BFE9436E80BFBA46909 gpg: checking the trustdb gpg: marginals needed: 3 completes needed: 1 trust model: pgp gpg: depth: 0 valid: 1 signed: 1 trust: 0-, 0q, 0n, 0m, 0f, 1u gpg: depth: 1 valid: 1 signed: 0 trust: 1-, 0q, 0n, 0m, 0f, 0u gpg: next trustdb check due at 2023-09-01 gpg: Good signature from \"Matthew Farina \" [full] 数字签名在验证 Helm 图表的来源及完整性方面同样发挥着重要作用，我们在下一节讨论。 10.5 签名和验证 Helm 图表 与 Helm 维护人员签署发行版类似，你可以签署自己的 Helm 图表，以便用户可以验证他们安装图表是来自于你的。要对图表签名，首先需要有 gpg 密钥对。在先前的部分，我们已经创建了 gpg 密钥对，可以直接使用。 接下来，可以使用 helm package 命令的某些标识，指定密钥为图表签名。 我们使用包存储库中的 player-stats 图表作为示例，图表位于 learn-helm/helm-charts/charts/player-stats 目录。 在签名 player-stats 图表之前需要注意一点，如果你使用了GPG v2 及更高的版本，需要将 public 和 secret 钥匙环导出为老版本的格式。较新版本的 GPG 以 .kbx 文件格式存储钥匙环，而老版本以 .gpg 文件格式存储钥匙环，目前Helm仅支持 .gpg 格式。 将 GPG public 和 secret 钥匙环转换为 .gpg 文件格式： 查看 gpg 版本： $ gpg --version gpg (GnuPG) 2.2.20 libgcrypt 1.8.5 Copyright (C) 2020 Free Software Foundation, Inc. 如果你的 gpg 版本是 2 或更大，使用以下命令导出 public 和 secret 钥匙环： $ gpg --export > ~/.gnupg/pubring.gpg $ gpg --export-secret-keys > ~/.gnupg/secring.gpg 导出完成，就可以对 Helm 图表执行签名和打包操作了。helm package 命令提供了三个标志，用于签名和打包图表： --sign: 允许你使用 GPG 私钥对图表签名 --key: 签名时使用的密钥名称 --keyring: 指定包含 PGP 私钥的密匙环的路径 在下一步中，这些标志将与 helm package 命令一起用于签名和打包 player-stats Helm 图表。 执行 helm package 命令： $ helm package --sign --key '$KEY_NAME' --keyring ~/.gnupg/secring.gpg player-stats $KEY_NAME 变量可以是所需密钥关联的邮箱、名称或指纹。可以通过 gpg --list-keys 命令查看。 当使用不带签名标识的 helm package 命令时，会生成一个包含 helm 图表的 tgz 归档文件，使用带签名标识的打包命令时，会产生两个文件： player-stats-1.0.0.tgz player-stats-1.0.0.tgz.prov player-stats-1.0.0.tgz.prov 文件称为出处文件。出处文件中包含出处记录，内容如下： 来自 Chart.yaml 文件的图表元数据 player-stats-1.0.0.tgz sha256 hash值 player-stats-1.0.0.tgz 文件的 PGP 数字签名 Helm 图表的用户可以使用出处文件验证图表的数据来源和完整性。当把图表推送到图表存储库时，应确保同时推送了 .tgz 归档文件和 .tgz.prov 出处文件。 一旦 Helm 图表签名并打包完成，你需要导出用于加密数字签名的私钥对应的公钥。用户下载此公钥进行签名验证。 使用以下命令将公钥导出为 ascii-armor 格式： $ gpg --armor --export $KEY_NAME > pubkey.asc 如果你的 player-stats 图表发行版是公开的，那么这个公钥是可以被图表用户下载的，比如 Keybase。用户可以使用 gpg --import 命令导入这个公钥。 在安装图表前，图表用户可以使用 helm verify 命令验证图表的数据来源及完整性。此命令针对下载的 .tgz 归档文件和 .tgz.prov 出处文件运行。 下面的命令演示了在 player-stats Helm 图表上运行此过程，并假设你的公钥已经导入到名为 ~/.gnupg/pubring.gpg 的密匙环中： $ helm verify --keyring ~/.gnupg/pubring.gpg player-stats-1.0.0.tgz Signed by: weiwendi Using Key With Fingerprint: DC06795BA6B4C0526F085753519584CA7F2EFA86 Chart Hash Verified: sha256:d47cac011e7d85ad21df6c2b00844de74f3954bdd857087cd257fc370b824da0 如果将返回错误信息，验证失败的原因有多种，包括： .tgz 和 `.tgz.prov 文件不在同一个目录 .tgz.prov 文件被损坏 文件 hash 不匹配，表示文件完整性有问题 用于解密签名的公钥与最初用于加密签名的私钥不匹配。 helm verify 命令用于验证本地下载的图表，因此有一个更简便的方式，使用 helm install --verify 命令，该命令同时执行了验证和安装。假设 .tgz 和 .tgz.prov 文件已经下载到本地。 下面是 helm install --verify 命令的示例： $ helm install my-playerstats $CHART_REPO/player-stats --verify --keyring ~/.gnupg/pubring.gpg 通过使用本节中描述的方法来签署和验证 Helm 图表，你和用户都可以确保正在安装的图表是可靠的，并且没有被篡改。 在了解了数据来源和完整性在 Helm 安全中的作用之后，我们继续讨论 Helm 的其它安全事项，我们的下一个主题是 Helm 图表开发相关的安全性。 10.6 开发安全的 Helm 图表 虽然来源和完整性在 Helm 的安全中扮演着重要的角色，但它们仅是其中的一环。图表开发人员应该确保在开发过程中，遵守有关安全性的最佳实践，以防止用户在 Kubernetes 集群中安装图表时引入漏洞。在本节中，我们将讨论与 Helm 图表开发有关的主要安全问题，以及作为开发人员，该如何以安全为最高优先级开发 Helm 图表。 我们先从 Helm 图表使用的容器镜像的安全性开始讨论。 10.6.1 使用安全的镜像 由于 Helm(和Kubernetes) 的目标是部署容器镜像，镜像本身就是一个主要的安全问题。首先，图表开发人员应该了解镜像 tags 和镜像摘要的区别。 tag 简单的描述了镜像内容，方便开发人员和使用者能快速获知镜像提供了什么功能。但 tag 也可能带来安全隐患，因为不能保证 tag 和镜像内容始终一致。例如，镜像维护人员可能使用相同的 tag，提交解决了某个安全漏洞的新镜像，这会导致容器在运行时，使用了不同的镜像，从而导致一些问题。除了 tag，镜像也可以使用 digest 获取。镜像 digest 是镜像的 SHA-256 值，不可变，容器运行时可以根据该值验证镜像是否包含预期的内容。这消除了使用 tag 导致拉取错镜像的隐患，还可以消除中间人攻击的风险。 例如，图表模板中配置镜像时，可以把使用 tag 的 quay.io/bitnami/mongodb:4.2.15 替换为 digest 的 quay.io/bitnami/mongodbsha256:bb084ff5c04c6c6b86e39fd7ebefe9af52e9e4663594a2f76fdea707bf5bcf31，以确保镜像内容不会被修改。 随着时间推移，一个安全的镜像可能变得不安全，如该镜像里的操作系统版本被发现有漏洞。有很多方法能够检测镜像是否包含漏洞，利用镜像注册中心的漏洞扫描功能就是常用的一种方式。 Quay 容器注册表（Quay container registry）可以按照指定的时间间隔扫描镜像，以确定镜像包含的漏洞数量。Nexus 和 Artifactory 容器注册表也有这种功能，当然还有 CNCF 中毕业的 Harbor。除了容器注册中心提供的扫描功能外，还可以利用其它工具，如 Clair（Quay、Harbor都使用它进行漏洞扫描）、anchor、Vuls 和 OpenSCAP。当你的镜像 registry 或独立扫描工具扫描出镜像容易被攻击时，就该对镜像进行更新了，以防止将漏洞引入用户的 Kubernetes 群集。 作为镜像维护人员，你需要定期对镜像进行扫描，以防止镜像中存在漏洞。当然，你还需确保镜像是来自于可靠的组织和镜像注册中心，以减少镜像包含漏洞的可能性。此设置是在容器运行时上配置的，不同运行时的配置也不相同。 除了镜像漏洞扫描和镜像来源，你还应该避免部署需要提升权限或功能的镜像。功能是为进程提供根权限的子集。例如 NET_ADMIN，它允许进程执行与网络相关的操作，SYS_TIME 允许进程修改系统时钟。使用 root 身份运行容器可以访问所有功能，所以要尽量避免使用 root 用户。 授予容器功能或以 root 身份运行容器，恶意进程会更有可能破坏底层主机。这不仅会影响包含漏洞的容器，还会影响在该主机上运行的其它容器，甚至可能影响整个 Kubernetes 集群。如果一个容器确实有漏洞，但没有授予它任何功能，则攻击范围要小很多，并且有可能完全被阻止。在开发 Helm 图表时，必须考虑镜像的漏洞和权限要求，以确保你及 Kubernetes 集群中其他租户的安全。 除了镜像外，图表开发人员还应关注分配给应用程序的资源。我们在下一节进行讨论。 10.6.2 设置资源限制 Pod 使用底层主机的资源，如果没有设置适当的默认值，Pod 可能会耗尽节点的资源，从而导致 CPU throttling、内存 OOM 或者 Pod 被驱逐等问题，同时也会影响该节点上的其他 Pod。由于资源限制不受控制时可能会出现问题，所以图表开发人员应关注在 Helm 图表或 Kubernetes 集群中设置合理的默认值。 可以在图表的 values.yaml 文件中定义 deployment resources 字段，为图表运行设置一个合理的默认值： resources: limits: cpu: 500m memory: 2Gi 在这个默认值中，CPU 限制为 500m，内存限制为 2Gi。这样做，既防止了节点资源被耗尽，又给出了合理的建议值。用户可以根据实际使用情况修改这些值。开发人员也可以提供 requests 的默认值，但它不能避免节点资源被耗尽的风险。 你还应该考虑，在 values.yaml 文件中，对图表要部署到的命名空间设置 limit range(限制范围) 和 resource quotas(资源配额)。这些资源通常不包含在 Helm 图表中，而是由集群管理员在应用程序部署前创建的。limit range 用于定义允许命名空间使用的资源量，还用于部署到尚未定义 limit range 的命名空间的每个容器设置默认资源限制。以下是 LimitRange 对象的定义示例： apiVersion: v1 kind: LimitRange metadata: name: limits-per-container spec: limits: - max: cpu: 1 memory: 4Gi default: cpu: 500m memory: 2Gi type: Container LimitRange 在创建 LimitRange 对象的命名空间中强制执行指定的限制。它定义了最大的 CPU 资源设置为 1 核，最大的内存限制为 4Gi。如果没有定义资源限制，它会自动将资源限制设置为 500m 和 2Gi。type 字段也可以设置为 Pod，这样限制级别就是 Pod了，Pod 中所有容器的资源总和应低于指定的值。除了 CPU 和内存的限制外，还可以将 type 字段设置为 PersistentVolumeClaim，将 LimitRange 对象设置为 PersistentVolumeClaim 对象声明的默认存储。 以下是对 PVC 存储的限制配置： apiVersion: v1 kind: LimitRange metadata: name: limits-per-pvc spec: - max: storage: 4Gi type: PersistentVolumeClaim 当然，在图表的 values.yaml 文件中，也可以设置存储的默认值，LimitRange 对象会强制覆盖用户指定的最大值。 除了限制范围外，你还可以设置资源配额，以对命名空间中的资源使用添加额外的限制。限制范围在每个容器、Pod 或 PVC 级别强制使用资源，而资源配额在每个命名空间级别强制使用资源。它们用于定义命名空间可以利用的最大资源数量。资源配额示例如下: apiVersion: v1 kind: ResourceQuota metadata: name: pod-and-pvc-quota spec: hard: limits.cpu: '4' limits.memory: 8Gi requests.storage: 20Gi ResourceQuota 对象会把整个命名空间的资源使用量限制在 4 核 CPU 和 8Gi 内存，以及 20Gi 存储。资源配额还可用于 secrets、ConfigMap 和其他 Kubernetes 资源。通过使用资源配额，可以防止单个命名空间过度利用集群资源。 通过在 Helm 图表中设置合理的默认资源限制，以及 LimitRange 和 ResourceQuota 的存在，你可以确保部署该Helm 图表不会耗尽集群资源并导致中断。了解了如何强制执行资源限制后，让我们继续讨论下一个主题，Helm 图表安全——如何处理 Helm 图表中的 secrets。 10.6.3 处理 Helm 图表中的 secrets 在 Helm 图表中，如何处理 secrets 是一个常见问题。在第三章中安装 WordPress 时，我们为 admin 用户设置了密码。默认情况下，values.yaml 文件中是不应提供密码的，因为如果你忘记修改此密码，会导致程序更易被攻击。图表开发人员应该养成不为密码提供默认值的习惯，而应要求由最终用户显式提供值。这可以通过 required 函数实现。Helm 还可以使用 randAlphaNum 函数生成随机字符串。 但 randAlphaNum 函数会在每次执行图表升级时，都会生成一个新的随机字符串。因此开发人员在设计图表时，应该要求用户提供密码，并使用 required 函数确保用户提供了该值。 用户在安装图表时用到的 secrets，应该保存在 Kubernetes 的 secret 资源中，它以 Base64 编码对内容加密，而不是纯文本的方式保存在 ConfigMap 资源中。Secret 还可以作为 tmpfs 挂载到 Pod 中，Pod 随着应用的生命周期结束而销毁，要比把 secrets 放在磁盘上更安全。作为图表开发人员，应该确保你的 Helm 图表管理的所有凭证和机密配置都是使用 Kubernetes Secrets 创建的。 图表开发人员应该使用 Kubernetes secrets 和 required 函数恰当的处理 secrets，而图表用户应该安全地向Helm 图表提供凭证和私密数据。Helm 图表的值通常以 --values 标志提供，附加值或重写值通常在 values 文件中声明，并在安装期间传递给 Helm CLI。在处理常规值时，这是一种适当的方式，但在使用这种方式处理 secrets 值时应该足够谨慎，不能把包含 secrets 的值文件检入到 git 存储库或其他可能暴露 secrets 的公共空间。 避免暴露 secrets 的一种方法是使用 --set 标志，从本地命令行传递 secrets。这降低了凭证暴露的风险，但是用户应该意识到，凭证会暴露在 bash 历史命令中。 避免暴露 secrets 的另一种方法是使用加密工具对包含 secrets 的值文件加密。这样就可以把加密后的值文件推送到远程 git 仓库了。值文件只能由手握密钥的用户解密，只允许受信任的成员访问。用户可以使用 GPG 对值文件加密，也可以使用 sops 等工具。sops 用于加密 YAML 或 JSON 的值，但不加密键。以下是 sops 加密文件的键值对： password:ENC[AES256GCM,data:xhdUx7DVUG8bitGnqjGvPMygpw==,iv:3LR9KcttchCvZNpRKqE5LcXRyWD1I00v2kEAIl1ttco=,tag:9HEwxhT9s1pxo9lg19wyNg==,type:str] password 键并没有被加密，它的值是被加密的。这样既可以方便查看文件包含了哪些键，又不会暴露其值。 还有其它工具可以加密包含 secrets 的值文件，如 git-crypt(https://github.com/AGWA/git-crypt)、blackbox(https://github.com/StackExchange/blackbox)。此外，HashiCorp 的 Vault、CyberArk Conjur 之类的工具可用于 key/value 存储的形式加密。然后，可以通过使用秘钥管理系统进行身份验证，通过 --set 传递它们，在 Helm 中使用它们来检索加密的值。 了解了 Helm 图表开发过程中的安全注意事项后，现在我们开始讨论如何在 Kubernetes 中使用基于角色的访问控制来为用户提供更大的安全性。 10.7 配置 RBAC 规则 Kubernetes 中经过身份验证的用户，要操作集群，还需要一组 RBAC 策略。在第二章我们介绍过策略，也就是角色，可以与 user 或者 service accounts 相关联，Kubernetes 包含几个可以关联的默认角色。从 1.6 版本开始，Kubernetes 中默认启用了 RBAC。在为 Helm 配置 RBAC 时，需要考虑两个因素： 安装 Helm 图表的使用的 user 与运行的 Pod 关联的 service account 在大多数情况下，负责安装 Helm 图表的个人与 Kubernetes user 有关。不过，Helm 图表可以通过其他方式安装，例如由 Kubernetes operator 使用关联的 service account 安装。 默认情况下，users 和 service account 在 Kubernetes 集群中的权限是最小的。通过使用作用域为单个命名空间的角色或在集群级别授予访问权的集群角色，可以授予其他权限。然后使用角色绑定或集群角色绑定将它们与 users 或 service account 关联，具体取决于目标策略的类型。虽然 Kubernetes 包含了许多可以应用的角色，但是应该尽可能使用最小特权访问的概念。最小特权访问是指仅授予用户或应用程序正常工作所需的最小权限集。以我们之前开发的 player-stats 图表为例，假设我们想要添加新的功能，可以查询 player-stats 应用程序命名空间中 Pods 的元数据。 虽然 Kubernetes 有一个名为 view 的内置角色，提供查询指定命名空间中 Pod 清单所需的权限，但它也提供对其它资源的访问，比如 ConfigMap 和 Deployment。为了最小化授权应用程序的访问级别，可以创建一个角色或集群角色形式的自定义策略，该策略仅提供应用程序所需的必要权限。由于 Kubernetes 集群的大多数用户没有在集群级别上创建资源的权限，所以让我们创建一个应用于 Helm 图表所在命名空间的角色。 kubectl create role 命令用于创建角色，基本角色包含两个因素： 针对 Kubernetes API 的动作(动词)类型 Kubernetes 的目标资源列表 我们为经过身份验证的 user 添加查看命名空间中 Pod 的权限，以此来演示如何创建 RBAC。 在此之前，确保已经使用 minikube start 命令启动了 Minikube。 创建一个新的命名空间，名称为 testrbac： $ kubectl create ns testrbac 使用 Kubectl CLI 创建一个名称为 playerstats-pod-viewer 的角色： $ kubectl create role playerstats-pod-viewer --resource=pods --verb=get,list -n testrbac 创建新角色后，需要将其与 user 或者 service account 关联。因为我们要把它与 Kubernetes 中运行的应用程序关联，所以我们把这个角色应用到一个 service account 上。在创建 Pod 时，它使用名为 default 的 service account。在尝试遵循最小权限访问时，建议使用一个单独的 service account。这是为了确保没有其它应用部署在与 player-stats 相同的命名空间中，因为它也将继承相同的权限。 创建名为 player-stats 的 service account： $ kubectl create sa player-stats -n testrbac 接下来，创建一个名为 playerstats-pod-viewer 的角色绑定(role binding)，将 playerstats-pod-viewer 角色与 player-stats ServiceAccount 关联起来： $ kubectl create rolebinding playerstats-pod-viewer --role=playerstats-pod-viewer --serviceaccount=testrbac:player-stats -n testrbac 要使用新创建的 player-stats ServiceAccount 运行 player-stats 应用程序，需要将 service account 的名称应用于 Deployment。 下面显示了 Deployment YAML中的 serviceAccount 配置： serviceAccountName: player-stats 配置完这些，就可以安装 player-stats 应用程序了。 https://github.com/weiwendi/learn-helm/tree/main/helm-charts/charts/player-stats 这个player-stats 图表设置了 deployment 的 service account 的值。 运行以下命令安装 player-stats 图表： $ helm install my-playerstats learn-helm/helm-charts/charts/player-stats \\ --set serviceAccount.name=player-stats \\ --set serviceAccount.create=false \\ -n testrbac 因为我们在之前的步骤中创建了 serviceAccount，所以 --set serviceAccount.create 的值设置为了 false。 此时 player-stats 应用程序已拥有了获取和列出所有 Pod 的权限，为了验证这个假设，我们可以使用 kubectl 命令查询 user 或 service account 是否有权执行某个操作。执行以下命令验证 ServiceAccount player-stats 是否有权限查询 player-stats 命名空间中所有的 Pod： $ kubectl auth can-i list pods --as=system:serviceaccount:testrbac:player-stats -n testrbac --as 标志利用 Kubernetes 中的用户模拟特性来允许调试授权策略。 以上命令的输出应该为 yes。要验证 service account 无法访问其不该访问的资源，例如列出 deployment，可以执行以下命令： $ kubectl auth can-i list deployments --as=system:serviceaccount:player-stats:player-stats -n testrbac # 这次会输出 no. 测试完毕，可以删除我们的部署了： $ helm uninstall my-playerstats -n testrbac 也可以停在 Minikube 了，它在本章中剩余部分已经不再需要了： $ minikube stop 正确使用 Kubernetes RBAC，可以帮助 Helm 开发人员总是以最小权限访问所需的工具，从而保护用户和应用程序免受潜在的意外或恶意行为的影响。 接下来我们讨论如何以增强 Helm 整体安全性的方式，保护和访问图表存储库。 10.8 图表存储库的安全设置 可以通过图表存储库搜索 Helm 图表，并将它们安装到 Kubernetes 集群上。第一章中讨论了 Helm 图表存储库，它是一个 HTTTP 服务，包含 index.yaml 文件，记录了存储库中的图表元数据。在前面的章节中，我们使用了来自不同存储库的图表，还使用 GitHub 页面实现了自己的存储库。这些存储库都可以免费供人使用。但是，Helm支持额外的安全措施来保护存储库中存储的内容，包括： Authentication SSL/TLS 虽然大多数公共存储库不需要任何形式的身份验证，但 Helm 支持用户对安全的图表存储库执行基于证书和基本的身份验证功能。对于基本身份验证，当使用 helm repo add 添加存储库时，可以使用 --username 和 --password 标志提供用户名和密码。例如，如果要访问使用基本身份验证保护的存储库，需要使用以下方式添加存储库： $ helm repo add $REPO_URL --username= --password= 接着，就可以与存储库交互，而无需每次提供凭证。 对于基于证书的身份认证，helm repo add 命令提供了 --ca-file, --cert-file 和 --key-file 标志。--ca-file 标志用于验证图表存储库的权威证书，--cert-file 和 --key-file 标志分别用于指定客户端证书和密钥。 在图表存储库上启用基本身份认证和证书认证的方式，取决于存储库本身。主流的 ChartMuseum 存储库使用 --basic-auth-user 和 --basic-auth-pass 标志，可以在启动时配置基本身份验证的用户名和密码。它还提供了 --tls-ca-cert 标志配置证书颁发结构(CA)以进行证书验证。其他图表存储库实现可能会提供其他标志或要求你提供配置文件。 开启了身份验证，也要保证 HTTP 服务器与 Helm 之间的传输是可靠的，可以使用 SSL 或 TLS 对传输进行加密，以确保 Helm 客户端和 Helm 图表存储库之间的通信安全。这样使用基本身份验证的存储库(以及未使用身份验证的存储库)都可以从加密网络中受益，因为这会尝试保护身份验证和存储库的内容。与身份验证一样，在图表存储库上配置 TLS 取决于所使用的存储库实现。ChartMuseum 提供 --tls-cert 和 --tls-key 标志来提供证书链和密钥文件。更常用的 Nginx 等 Web 服务器，通过一个配置文件指定证书及密钥文件的位置。GitHub 页面已经配置了TLS。 到目前为止，我们使用的每个 Helm 存储库都使用了由公共可用 CA 颁发的证书，这些证书存储在 Web 浏览器和底层操作系统中。许多大型组织都有自己的 CA，可用于生成在图表存储库中配置的证书。由于证书可能不是来自公开可用的 CA，因此 Helm CLI 可能不信任该证书，添加存储库会导致以下错误： Error: looks like '$REPO_URL' is not a valid chart repository or cannot be reached: Get $REPO_URL/index.yaml: x509: certificate signed by unknown authority 要允许 Helm CLI 信任图表存储库的证书，可以将 CA 证书或包含多个证书的 CA 包添加到操作系统的信任存储中，也可以使用 helm repo add 命令的 --ca-file 标志显式指定，这样就可以无误地执行命令。 最后，根据图表存储库的配置方式，还可以获得额外的 Metrics 来执行请求审计和日志记录，以确定谁试图访问存储库。 通过使用身份验证和管理控制传输层的证书，实现了增强 Helm 存储库安全性的额外功能。 By 魏文弟，使用知识共享 署名-相同方式共享 4.0 国际协议发布            updated 2021-09-01 17:21:13 "}}